[
  {
    "id": 2,
    "question": "Which of the following components are essential for the functioning of the Cloudera Data Platform\n(CDP) on-premises Data Lake, focusing on data storage and metadata management?",
    "options": {
      "A": "HDFS (Hadoop Distributed File System) for data storage.",
      "B": "Hive Metastore for metadata management.",
      "C": "YARN (Yet Another Resource Negotiator) for resource management.",
      "D": "ZooKeeper for coordinating distributed processes.",
      "E": "Kafka for real-time data streaming."
    },
    "correctAnswers": [
      "A",
      "B",
      "C",
      "D"
    ],
    "explanation": "HDFS provides distributed storage, Hive Metastore manages metadata, YARN manages resources and ZooKeeper coordinates distributed processes are all essential for a functioning Data Lake. Kafka is more related to streaming ingest rather than core Data Lake functionality, though it may be part of the overall architecture. The question specifically asks about Data Lake functioning."
  },
  {
    "id": 3,
    "question": "You are configuring resource pools in YARN for different departments in your organization. The\nmarketing department requires a guaranteed minimum of 40% of the cluster's resources, while the\nengineering department requires at least 50%. What is the most appropriate configuration for the\n'yarn.scheduler.capacity.maximum-am-resource-percent' property in 'yarn-site.xmP , considering\nefficient resource utilization and potentially variable workloads?",
    "options": {
      "A": "Set 'yarn.scheduler.capacity.maximum-am-resource-percent’ to 0.1 (10%).",
      "B": "Set 'yarn.scheduler.capacity.maximum-am-resource-percent’ to 0.4 (40%).",
      "C": "Set 'yarn.scheduler.capacity.maximum-am-resource-percent’ to 0.5 (50%).",
      "D": "Set ‘yarn.scheduler.capacity.maximum-am-resource-percent’ to 0.9 (90%).",
      "E": "Set 'yarn.scheduler.capacity.maximum-am-resource-percent’ to 1.0 (100%)."
    },
    "correctAnswers": [
      "D"
    ],
    "explanation": "‘yarn.scheduler.capacity.maximum-am-resource-percent' defines the maximum percentage of resources in the cluster that can be used for ApplicationMasters (AMs). While marketing and engineering require 40% and 50% respectively of overall cluster resources, the AMs need resources too. The maximum usage percentage should allow enough space for the Application Master processes to start for a resource intensive job. Setting this to a lower value might lead to resource starvation for the application masters."
  },
  {
    "id": 4,
    "question": "A critical Hive query that performs complex aggregations on a large dataset is consistently slow. You\nsuspect the issue is related to suboptimal data partitioning and bucketing. How can you re-design the\nHive table definition to improve query performance?",
    "options": {
      "A": "Remove partitioning and bucketing altogether to simplify the table structure.",
      "B": "Partition the table by a low-cardinality column and bucket it by a high-cardinality column frequently used in 'JOIN’ or ‘GROUP BY operations.",
      "C": "Partition the table by a high-cardinality column and bucket it by a low-cardinality column.",
      "D": "Use dynamic partitioning with a very large number of partitions.",
      "E": "Bucket the table only, with a very small number of buckets."
    },
    "correctAnswers": [
      "B"
    ],
    "explanation": "Partitioning by a low-cardinality column reduces the amount of data scanned for queries filtering on that column. Bucketing by a high- cardinality column frequently used in ‘JOIN’ or ‘GROUP BY operations distributes the data evenly across buckets, allowing for parallel processing and improved join performance (bucket map joins). Using high cardinality column as partition would result in large number of small files and low cardinality column as bucket will result in data skewness. Dynamic partitioning is useful but can be inefficient if not managed properly."
  },
  {
    "id": 5,
    "question": "Which of the following statements accurately describe the role of the Cloudera Manager Agent in a CDP\non-premises cluster?",
    "options": {
      "A": "The Cloudera Manager Agent is responsible for distributing the Cloudera Manager Server software across the cluster.",
      "B": "The Cloudera Manager Agent collects metrics and status information from the services running on its host and reports them to the Cloudera Manager Server.",
      "C": "The Cloudera Manager Agent executes commands issued by the Cloudera Manager Server, such as starting, stopping, and reconfiguring services.",
      "D": "The Cloudera Manager Agent is only required on the Cloudera Manager Server host.",
      "E": "The Cloudera Manager Agent automatically optimizes query performance without direct intervention."
    },
    "correctAnswers": [
      "B",
      "C"
    ],
    "explanation": "Cloudera Manager Agents monitor the services on each host, reporting back to the Cloudera Manager Server, and execute commands issued by the Server. They don't distribute the Server software, and are needed on all cluster hosts, not just the Server host. They don't directly optimize query performance without configuration."
  },
  {
    "id": 6,
    "question": "You are configuring Kerberos authentication for your CDP on-premises cluster After enabling Kerberos,\nyou notice that some of your existing Spark applications are failing with authentication errors. What are\nthe likely causes and how do you resolve them?",
    "options": {
      "A": "The Spark application is not configured to use Kerberos credentials. You need to configure the ‘spark.yarn.principar and ‘spark.yarn.keytab' properties in the Spark configuration.",
      "B": "The Kerberos ticket-granting ticket (TGT) has expired on the client machine. You need to run 'kinit' to",
      "C": "The necessary Java Cryptography Extension (JCE) Unlimited Strength Jurisdiction Policy Files are not installed on the Spark driver and executor nodes.",
      "D": "The clock skew between the Kerberos Key Distribution Center (KDC) and the Spark nodes is too large. You need to synchronize the clocks using NTP (Network Time Protocol).",
      "E": "Spark applications automatically inherit Kerberos authentication settings from the operating system; no specific configuration is required."
    },
    "correctAnswers": [
      "A",
      "B",
      "C",
      "D"
    ],
    "explanation": "Spark applications need to be explicitly configured to use Kerberos with correct principal and keytab. Expired TGTs prevent authentication. JCE policy files are required for strong encryption used by Kerberos. Clock skew can cause authentication failures. So all options except E are correct."
  },
  {
    "id": 7,
    "question": "Your Cloudera Data Platform (CDP) on-premises cluster is experiencing frequent HDFS NameNode\nfailures, leading to data unavailability. You want to implement High Availability (HA) for the NameNode.\nWhat steps are essential to configure NameNode HA using Quorum Journal Manager (QJM)?",
    "options": {
      "A": "Install and configure a single ZooKeeper server.",
      "B": "Configure two NameNode hosts and share a single edit log directory between them.",
      "C": "Install and configure a quorum of JournalNodes (typically 3 or more) to store the edit log.",
      "D": "Configure the 'dfs.nameservices’ and 'dfs.ha.namenodes.[nameserviceld]' properties in 'hdfs-site.xmr.",
      "E": "Format the NameNode using the '-format' option on both NameNode hosts."
    },
    "correctAnswers": [
      "C",
      "D"
    ],
    "explanation": "QJM requires a quorum of JournalNodes to store the edit log redundantly. You also need to configure 'dfs.nameserviceS and dfs.ha.namenodes.[nameserviceld]' to enable HA. A single ZooKeeper server is not enough for HA. NameNodes should not share a single edit log directory. Formatting with -format should be done only on the initial NameNode setup and not on both after configuring HA."
  },
  {
    "id": 8,
    "question": "A user reports that they are unable to access a specific directory in HDFS, even though they believe they\nhave the correct permissions. You need to troubleshoot the access issue. Which of the following actions\nwould be most helpful in diagnosing the problem?",
    "options": {
      "A": "Check the HDFS audit logs to see if the user's access attempts are being logged and if any errors are reported.",
      "B": "Use the ‘hdfs dfs -Is -R command to recursively list the directory and its contents, paying attention to the permissions of each file and directory.",
      "C": "Verify the user's group membership and check if any ACLs (Access Control Lists) are configured on the directory or its parent directories.",
      "D": "Restart the NameNode to refresh the permissions cache.",
      "E": "Increase the HDFS replication factor to improve data availability."
    },
    "correctAnswers": [
      "A",
      "B",
      "C"
    ],
    "explanation": "Checking audit logs provides insights into access attempts and errors. Listing the directory recursively shows the permissions of each file and directory. Verifying group membership and ACLs confirms if the user has the necessary permissions. Restarting the NameNode is unlikely to resolve a permission issue, and increasing the replication factor does not affect access control."
  },
  {
    "id": 9,
    "question": "You are planning a CDP on-premises deployment and need to choose the appropriate hardware for your\nHDFS DataNodes. Your workload consists of large sequential reads and writes. Which storage\nconfiguration would provide the best performance and cost- effectiveness?",
    "options": {
      "A": "Solid-state drives (SSDs) in a RAID 0 configuration.",
      "B": "High-capacity SATA hard disk drives (HDDs) in a RAID 10 configuration.",
      "C": "Small-capacity SAS hard disk drives (HDDs) in a RAID 5 configuration.",
      "D": "A mix of SSDs for frequently accessed data and HDDs for less frequently accessed data, leveraging HDFS tiering.",
      "E": "NVMe drives in JBOD (Just a Bunch of Disks) configuration."
    },
    "correctAnswers": [
      "B"
    ],
    "explanation": "For large sequential reads and writes, high-capacity HDDs in RAID 10 offer a good balance of performance and cost-effectiveness. SSDs are faster but more expensive and may not be necessary for sequential workloads. RAID 5 has write performance limitations. HDFS tiering is a good option for mixed workloads, but the question specifies a sequential workload. NVMe drives in JBOD are very fast, but lack redundancy unless HDFS replication covers it. So RAIDIO configuration of HDDs is the most appropriate answer here."
  },
  {
    "id": 10,
    "question": "You observe high disk I/O utilization on your HDFS DataNodes, impacting overall cluster performance.\nWhich of the following techniques can help reduce disk I/O and improve performance?",
    "options": {
      "A": "Increase the HDFS replication factor.",
      "B": "Enable HDFS data compression (e.g., using Snappy or LZO).",
      "C": "Adjust the 'dfs.datanode.handler.count' property to increase the number of DataNode worker threads.",
      "D": "Implement HDFS caching to store frequently accessed data in memory.",
      "E": "Reduce the block size of HDFS files."
    },
    "correctAnswers": [
      "B",
      "C",
      "D"
    ],
    "explanation": "Enabling HDFS data compression reduces the amount of data written to disk. Adjusting 'dfs.datanode.handler.count can increase the number of DataNode worker threads that handle I/O requests. Implementing HDFS caching stores frequently accessed data in memory, reducing disk I/O. Increasing the replication factor increases disk I/O, and reducing the block size can lead to more metadata overhead."
  },
  {
    "id": 11,
    "question": "You are tasked with setting up a new CDP on-premises cluster. You want to ensure that data is\nprotected against node failures. What is the minimum recommended replication factor for HDFS data\nblocks to provide fault tolerance?",
    "options": {
      "A": "1",
      "B": "2",
      "C": "3",
      "D": "4",
      "E": "5"
    },
    "correctAnswers": [
      "C"
    ],
    "explanation": "The minimum recommended replication factor for HDFS is 3. This ensures that data is stored on at least three different DataNodes, providing fault tolerance against node failures."
  },
  {
    "id": 12,
    "question": "You've configured Ranger for centralized access control in your CDP cluster. You need to grant a specific\nuser, 'data_scientist', read access to all tables within a particular Hive database named 'analytics db'.\nWhat is the most efficient and recommended way to achieve this using Ranger?",
    "options": {
      "A": "Grant the user 'data_scientist' direct access to each table in 'analytics_db' individually using Hive's 'GRANT statement.",
      "B": "Create a Ranger policy that grants read access to the 'analytics db' database to the user 'data_scientist'. This will automatically apply to all tables within the database.",
      "C": "Modify the HDFS permissions of the underlying data directories for the 'analytics db' database to grant read access to the user 'data_scientist'.",
      "D": "Create a new Linux group, add the user 'data_scientist' to the group, and then grant the group read access to the 'analytics_db' database using Ranger.",
      "E": "Grant the user 'data_scientist' the ‘ALL' privilege on the 'analytics_db' database in Hive."
    },
    "correctAnswers": [
      "B"
    ],
    "explanation": "Ranger policies can be defined at the database level, granting access to all tables within that database. This is the most efficient and recommended approach. Granting individual table access via Hive is tedious. Modifying HDFS permissions bypasses Ranger's centralized control. A Linux group can be used, but specifying the user directly is simpler if only one user needs access. Granting privilege would give more access than requested."
  },
  {
    "id": 13,
    "question": "Your organization needs to ingest streaming data from multiple sources (e.g., web server logs, sensor\ndata) into your CDP on-premises cluster for real-time analytics. Which of the following components is\nbest suited for this purpose, considering scalability, fault tolerance, and integration with other CDP\nservices?",
    "options": {
      "A": "Flume",
      "B": "Sqoop",
      "C": "Kafka",
      "D": "Oozie",
      "E": "HBase"
    },
    "correctAnswers": [
      "C"
    ],
    "explanation": "Kafka is a distributed, fault-tolerant streaming platform that can handle high volumes of data from multiple sources. Flume is another option but is generally less scalable than Kafka. Sqoop is for batch data import. Oozie is a workflow scheduler. HBase is a NoSQL database. Therefore, Kafka fits this scenario best."
  },
  {
    "id": 14,
    "question": "Which of the following components are essential for a highly available Cloudera Manager deployment?",
    "options": {
      "A": "A single Cloudera Manager Server instance.",
      "B": "An external database (e.g., PostgreSQL, MySQL) for storing Cloudera Manager metadata.",
      "C": "A load balancer to distribute traffic between multiple Cloudera Manager Server instances.",
      "D": "A shared file system (e.g., NFS) for storing Cloudera Manager agent data.",
      "E": "ZooKeeper for leader election and coordination among Cloudera Manager Server instances."
    },
    "correctAnswers": [
      "B",
      "C",
      "E"
    ],
    "explanation": "A highly available Cloudera Manager deployment requires an external database to store metadata, a load balancer to distribute traffic, and ZooKeeper for coordination and leader election between Cloudera Manager Server instances. A single instance (A) defeats the purpose of HA. While a shared file system can be used, it's not strictly essential for basic HA of the CM server itself. Agent data is managed differently."
  },
  {
    "id": 15,
    "question": "You're deploying Cloudera Manager and want to configure the database. Which database types are\nofficially supported for production environments in the latest CDP release?",
    "options": {
      "A": "Embedded Derby database.",
      "B": "MySQL/MariaDB.",
      "C": "PostgreSQL.",
      "D": "Oracle.",
      "E": "SQLite"
    },
    "correctAnswers": [
      "B",
      "C",
      "D"
    ],
    "explanation": "Embedded Derby (A) is not supported for production. SQLite (E) also not supported for production, while MySQL/MariaDB, PostgreSQL, and Oracle are officially supported database options for production Cloudera Manager deployments."
  },
  {
    "id": 16,
    "question": "What is the primary function of the Cloudera Manager Agent in the CDP on-premises architecture?",
    "options": {
      "A": "To provide a web-based UIfor cluster management.",
      "B": "To monitor and manage services on the host it is installed on, as instructed by the Cloudera Manager Server.",
      "C": "To act as a central repository for all cluster logs.",
      "D": "To perform automatic software updates on all cluster nodes.",
      "E": "To authenticate users accessing the cluster."
    },
    "correctAnswers": [
      "B"
    ],
    "explanation": "The Cloudera Manager Agent is responsible for executing commands, monitoring services, and reporting status back to the Cloudera Manager Server. It doesn't provide the UI(A), centrally store logs (C), automatically update software (D) or handle authentication directly (E)."
  },
  {
    "id": 17,
    "question": "You observe high CPU utilization on the Cloudera Manager Server Which of the following actions could\nhelp alleviate this issue?",
    "options": {
      "A": "Increase the heap size allocated to the Cloudera Manager Server Java process.",
      "B": "Reduce the monitoring frequency of the Cloudera Manager Agents.",
      "C": "Enable the Cloudera Manager API throttling to limit excessive API calls.",
      "D": "Migrate the Cloudera Manager Server to a more powerful host.",
      "E": "Disable HTTPS for the CM UI."
    },
    "correctAnswers": [
      "A",
      "B",
      "C",
      "D"
    ],
    "explanation": "Increasing heap size, reducing monitoring frequency, enabling API throttling, and migrating to a more powerful host can all help reduce CPU utilization on the Cloudera Manager Server. Disabling HTTPS, while possibly reducing some CPU load related to encryption, is not a recommended or significant solution and creates a security risk."
  },
  {
    "id": 18,
    "question": "How can you configure Cloudera Manager to send alerts via email when a service goes down?",
    "options": {
      "A": "By configuring an SMTP server in the Cloudera Manager settings and defining alert publishing rules.",
      "B": "By installing a separate email client on the Cloudera Manager Server.",
      "C": "By using the 'cm-api' command-line tool to configure email notifications.",
      "D": "By configuring an external monitoring system (e.g., Nagios) to poll Cloudera Manager's API for service status.",
      "E": "Email alerts are not supported in CDP on-premises."
    },
    "correctAnswers": [
      "A"
    ],
    "explanation": "Cloudera Manager has built-in email alert functionality. You configure an SMTP server in the Cloudera Manager settings and then create alert publishing rules that specify when and how to send email notifications. While option D is a valid approach, it is not directly configuring Cloudera Manager."
  },
  {
    "id": 19,
    "question": "After upgrading Cloudera Manager, you notice that the agents are continuously disconnecting and\nreconnecting. What is the MOST likely cause?",
    "options": {
      "A": "Incorrect firewall settings are blocking communication between the Cloudera Manager Server and the Agents.",
      "B": "The Cloudera Manager Agents were not upgraded to the same version as the Cloudera Manager Server.",
      "C": "The time is not synchronized between the Cloudera Manager Server and the Agents.",
      "D": "Insufficient memory allocated to the Cloudera Manager Agents.",
      "E": "The CM server has run out of disk space."
    },
    "correctAnswers": [
      "B"
    ],
    "explanation": "A version mismatch between the Cloudera Manager Server and the Agents is a common cause of connection issues after an upgrade. While other options could contribute, a version mismatch is the most likely root cause in this scenario. Firewall issues and time synchronization problems usually exist before the upgrade."
  },
  {
    "id": 20,
    "question": "Which of the following statements accurately describe the role of the Cloudera Management Service\n(CMS)?",
    "options": {
      "A": "It's a single monolithic service responsible for all cluster management tasks.",
      "B": "It's a collection of services (e.g., Activity Monitor, Host Monitor, Reports Manager) that provides monitoring, reporting, and diagnostic capabilities for the CDP cluster.",
      "C": "It is required only for clusters with Kerberos enabled.",
      "D": "It acts as a proxy server for all communication between the Cloudera Manager Server and the Agents.",
      "E": "The CMS is not necessary if using Cloudera Data Platform Private Cloud Base."
    },
    "correctAnswers": [
      "B"
    ],
    "explanation": "The Cloudera Management Service (CMS) is a suite of services responsible for monitoring, reporting, and diagnostics within the CDP cluster. It is not a single monolithic service (A), is required regardless of Kerberos (C & E), and doesn't act as a proxy (D)."
  },
  {
    "id": 21,
    "question": "You need to configure a custom alert in Cloudera Manager that triggers when the disk utilization on a\nspecific host exceeds 90%. Where within Cloudera Manager would you configure this?",
    "options": {
      "A": "Under the 'Roles' tab for the specific host.",
      "B": "Under the 'Configuration' tab for the Cloudera Management Service, then creating a new Metric Monitor.",
      "C": "Under the 'Hosts' tab, selecting the host, and then creating a new 'Host Template'.",
      "D": "Using the 'cm-api' command-line tool with a custom Python script.",
      "E": "Alerts cannot be configured at the host level. They must be configured at the service level."
    },
    "correctAnswers": [
      "B"
    ],
    "explanation": "Custom alerts, including those based on host metrics like disk utilization, are configured within the Cloudera Management Service settings, specifically by creating a new Metric Monitor. You define the metric to monitor (disk utilization), the threshold (90%), and the triggering conditions."
  },
  {
    "id": 22,
    "question": "Which configuration setting directly impacts the amount of historical data that Cloudera Manager stores\nfor metrics and monitoring purposes?",
    "options": {
      "A": "The java_heapsize’ setting for the Cloudera Manager Server.",
      "B": "The ‘event.expiry’ setting in the Cloudera Manager database configuration.",
      "C": "The metrics time to_live’ setting in the Cloudera Management Service configuration.",
      "D": "The setting in the Cloudera Manager Agent configuration.",
      "E": "The max concurrent_scanS setting for YARN."
    },
    "correctAnswers": [
      "C"
    ],
    "explanation": "The setting within the Cloudera Management Service configuration directly controls how long Cloudera Manager retains historical metrics data. A shorter TTL will save disk space but limit the scope of historical analysis. Setting B controls the event expiry for activity records within CM. Not metric data."
  },
  {
    "id": 23,
    "question": "You are facing performance issues with the Cloudera Manager UI. You suspect that the queries to the\nCM database are slow What tool can you use to analyze the database performance and identify slow\nqueries?",
    "options": {
      "A": "The Cloudera Manager UIitself provides detailed database performance metrics.",
      "B": "You can use standard database monitoring tools specific to your database type (e.g., ‘mysqltop’ for MySQL, for PostgreSQL).",
      "C": "The ‘cm-audit’ command-line tool provides detailed database query logs.",
      "D": "The Cloudera Diagnostic Bundle automatically captures database performance snapshots.",
      "E": "The 'jstack' command can be used on the CM server process to identify database bottlenecks."
    },
    "correctAnswers": [
      "B"
    ],
    "explanation": "While Cloudera Manager provides some performance metrics, detailed database performance analysis requires using database- specific monitoring tools. \"mysqltop', (or similar tools for Oracle) are appropriate for diagnosing slow queries. Other options are not specific to database query analysis. ‘jstacks’ is for thread dumps."
  },
  {
    "id": 24,
    "question": "Your Cloudera Manager server runs out of disk space. Which of the following directories should you\ninvestigate first to free up space?",
    "options": {
      "A": "/opt/cloudera/cm/run/cloudera-scm-server/cmf/reports",
      "B": "/var/log/cloudera-scm-server",
      "C": "/var/lib/cloudera-scm-server",
      "D": "/opt/cloudera/parcel-cache",
      "E": "/tmp"
    },
    "correctAnswers": [
      "A",
      "B"
    ],
    "explanation": "Ivar/log/cloudera-scm-server stores logs and /opt/cloudera/cm/run/cloudera-scm-server/cmf/reports store historical CM reports which are both strong candidates for taking up significant disk space and should be investigated first. /var/lib contains the database data and other persistent data which are important to system operation and shouldn't be cleared without knowing consequences and backups. Parcel cache is a location for downloaded packages and can be large but shouldn't grow without bound and should be considered after CM reports and log directories. /tmp is not specific to Cloudera Manager and should be investigated only as a general troubleshooting step."
  },
  {
    "id": 25,
    "question": "You need to migrate your Cloudera Manager server to a new host with a different IP address. Assuming\nyou are using an external database, what steps are crucial to ensure a smooth transition?",
    "options": {
      "A": "Simply install Cloudera Manager on the new host, point it to the existing database, and start the server. The agents will automatically connect.",
      "B": "Install Cloudera Manager on the new host, point it to the existing database, update the ‘server_host’ property in the file on all agent hosts, and restart the agents.",
      "C": "Back up the Cloudera Manager database, restore it on the new host, and update the agent configurations to point to the new server's IP address.",
      "D": "Export the Cloudera Manager configuration using the CM API, import it on the new host, and update the agent configurations to point to the new server's IP address.",
      "E": "You must rebuild the entire cluster and cannot migrate the Cloudera Manager server."
    },
    "correctAnswers": [
      "B"
    ],
    "explanation": "The crucial steps are installing Cloudera Manager on the new host, connecting it to the existing database (preserving your metadata), and updating the agent configurations with the new server's hostname/lP address . The agents need to know where the server is to communicate. Options A, C, D are incorrect because agent reconfiguration is mandatory."
  },
  {
    "id": 26,
    "question": "Which of the following are critical security considerations when designing a Cloudera CDP on-premises\ndeployment?",
    "options": {
      "A": "Network segmentation to isolate different CDP components.",
      "B": "Regularly patching and updating the operating system and CDP software.",
      "C": "Implementing strong authentication mechanisms, such as Kerberos, for all users and services.",
      "D": "Data encryption at rest and in transit.",
      "E": "Ignoring default passwords as they are secure."
    },
    "correctAnswers": [
      "A",
      "B",
      "C",
      "D"
    ],
    "explanation": "Network segmentation reduces the attack surface. Regular patching fixes vulnerabilities. Strong authentication prevents unauthorized access. Data encryption protects sensitive data. Ignoring default passwords can lead to serious exploits."
  },
  {
    "id": 27,
    "question": "You are configuring Kerberos for your Cloudera CDP on-premises cluster. Which configuration file is\nprimarily responsible for defining Kerberos client settings?",
    "options": {
      "A": "/etc/krb5.conf",
      "B": "/etc/hadoop/conf/core-site.xml",
      "C": "/etc/security/limits.conf",
      "D": "/var/log/krb5kdc.log",
      "E": "/etc/default/security"
    },
    "correctAnswers": [
      "A"
    ],
    "explanation": "/etc/krb5.conf is the primary configuration file for Kerberos client settings, defining realms, KDCs, and other Kerberos-related parameters. core-site.xml is for Hadoop core settings. limits.conf defines resource limits. krb5kdc.log logs Kerberos Key Distribution Center activity, and /etc/default/security configures other security parameters."
  },
  {
    "id": 28,
    "question": "Which of the following are valid methods for encrypting data at rest in HDFS within a Cloudera CDP on-\npremises environment?",
    "options": {
      "A": "HDFS Encryption Zones",
      "B": "Transparent Data Encryption (TDE) at the filesystem level",
      "C": "Encrypting individual files using GPG",
      "D": "Using a firewall to restrict access.",
      "E": "Disabling auditing."
    },
    "correctAnswers": [
      "A",
      "B"
    ],
    "explanation": "HDFS Encryption Zones provide a framework for encrypting data within specific directories in HDFS. Transparent Data Encryption encrypts the data at the filesystem level, making it unreadable without the encryption key. Encrypting individual files via GPG is not a scalable or manageable solution within a cluster. Firewalls restrict network access, not data at rest. Disabling auditing is a security risk."
  },
  {
    "id": 29,
    "question": "In a Kerberized Cloudera CDP cluster, what is the purpose of the 'kinit' command?",
    "options": {
      "A": "To initialize a new Kerberos principal in the KDC.",
      "B": "To obtain a Kerberos ticket-granting ticket (TGT) for a user or service principal.",
      "C": "To list all available Kerberos realms.",
      "D": "To delete a Kerberos principal from the KDC.",
      "E": "To configure firewalls."
    },
    "correctAnswers": [
      "B"
    ],
    "explanation": "The ‘kinit' command is used to obtain a Kerberos ticket-granting ticket (TGT) for a user or service principal, allowing them to authenticate to other Kerberized services without providing their password each time. Initializing or deleting principals is done via 'kadmin' or 'kadmin.locar."
  },
  {
    "id": 30,
    "question": "You need to implement Role-Based Access Control (RBAC) in your Cloudera CDP on-premises cluster.\nWhich component(s) within CDP are directly involved in managing and enforcing RBAC policies?",
    "options": {
      "A": "Cloudera Manager",
      "B": "Ranger",
      "C": "Hue",
      "D": "ZooKeeper",
      "E": "Kafka"
    },
    "correctAnswers": [
      "A",
      "B"
    ],
    "explanation": "Cloudera Manager provides the interface for managing users and assigning roles. Ranger is the policy engine that enforces the access policies defined through Cloudera Manager or Ranger's own I-Jl. Hue and Kafka are data access and streaming tools, respectively. ZooKeeper is a distributed coordination service."
  },
  {
    "id": 31,
    "question": "A security audit reveals that several users are accessing sensitive data without proper authorization. You\nsuspect that users have obtained Kerberos tickets for service principals they shouldn't have. How can\nyou revoke Kerberos tickets for specific users or services?",
    "options": {
      "A": "Using the 'kdestroy' command on each client machine.",
      "B": "Using the 'kadmin’ interface to expire or disable the Kerberos principals.",
      "C": "Restarting the entire Cloudera CDP cluster.",
      "D": "Reformatting the HDFS filesystem.",
      "E": "Deleting the user accounts in the OS"
    },
    "correctAnswers": [
      "B"
    ],
    "explanation": "The 'kadmin’ interface provides tools to manage Kerberos principals, including expiring or disabling them, which effectively revokes their Kerberos tickets. ‘kdestrov’ only deletes the local ticket cache, not the underlying principal. Restarting the cluster or reformatting HDFS is not a solution. Deleting user accounts prevents login, but does not revoke Kerberos tickets issued to those users while they were active."
  },
  {
    "id": 32,
    "question": "Which of the following configurations will enhance the security of communication within a Cloudera CDP\non-premises cluster? Assume Kerberos is already enabled.",
    "options": {
      "A": "Enabling TLS/SSL encryption for all services.",
      "B": "Configuring firewalls to restrict network access between services.",
      "C": "Disabling auditing.",
      "D": "Using default passwords for all services.",
      "E": "Granting all users administrative privileges."
    },
    "correctAnswers": [
      "A",
      "B"
    ],
    "explanation": "TLS/SSL encryption secures the communication channel, preventing eavesdropping. Firewalls restrict network access, limiting the potential damage from compromised services. Disabling auditing weakens security. Using default passwords or granting excessive privileges are security risks."
  },
  {
    "id": 33,
    "question": "You are setting up Ranger policies for Hive. You want to grant a specific group, 'data analysts', read\naccess to all tables in the 'sales_data' database, but prevent them from creating or deleting tables.\nWhich Ranger policy configuration achieves this?",
    "options": {
      "A": "Grant 'data_analysts' 'SELECT privilege on 'sales_data'.",
      "B": "Grant 'data_analysts' 'ALL' privilege on 'sales_data' database, then deny 'CREATE' and 'DROP' privileges.",
      "C": "Grant 'data _analysts' 'SELECT' privilege on 'sales_data' database.",
      "D": "Grant 'data_analysts' 'READ' privilege on Hive metastore.",
      "E": "Grant 'data_analysts' 'ALL' privilege on the Hive service."
    },
    "correctAnswers": [
      "A"
    ],
    "explanation": "Granting 'SELECT' privilege on 'sales_data'. grants read access to all tables within the 'sales_data' database. Granting 'ALL' then denying specific privileges can lead to unexpected behavior due to policy evaluation order. 'READ' on the metastore is not sufficient. 'ALL' on the service is overly permissive."
  },
  {
    "id": 34,
    "question": "How does HDFS Auditing help in maintaining a secure Cloudera CDP on-premises environment?",
    "options": {
      "A": "By recording all access attempts to HDFS data, enabling security administrators to track unauthorized access and potential breaches.",
      "B": "By automatically encrypting all data written to HDFS.",
      "C": "By preventing users from accessing HDFS data without Kerberos authentication.",
      "D": "By automatically backing up HDFS data to a remote location.",
      "E": "By replacing damaged blocks in HDFS"
    },
    "correctAnswers": [
      "A"
    ],
    "explanation": "HDFS auditing captures access attempts, providing an audit trail for security analysis. It does not encrypt data, enforce Kerberos, backup data, or repair damaged blocks."
  },
  {
    "id": 35,
    "question": "You need to implement data masking for sensitive fields (e.g., credit card numbers) in Hive tables. Which\nRanger plugin allows you to achieve this?",
    "options": {
      "A": "Ranger KMS plugin",
      "B": "Ranger Hive plugin",
      "C": "Ranger HDFS plugin",
      "D": "Ranger Kafka plugin",
      "E": "Ranger Solr plugin"
    },
    "correctAnswers": [
      "B"
    ],
    "explanation": "The Ranger Hive plugin allows you to define policies for data masking and row-level filtering within Hive. The Ranger KMS plugin manages encryption keys. The HDFS and Kafka plugins manage access to HDFS data and Kafka topics, respectively. Ranger Solr plugin manages access to Solr data."
  },
  {
    "id": 36,
    "question": "You are troubleshooting a Kerberos authentication issue. A user is receiving the error 'KDC has no\nsupport for encryption type'. What is the most likely cause and how would you resolve it?",
    "options": {
      "A": "Cause: The user's Kerberos principal is locked. Resolution: Unlock the principal using 'kadmin’ .",
      "B": "Cause: The client and KDC are not configured to use a common encryption type. Resolution: Modify the '/etc/krb5.conf file on the client and KDC to ensure a shared encryption type is enabled.",
      "C": "Cause: The Kerberos ticket has expired. Resolution: Run 'kinit -R to renew the ticket.",
      "D": "Cause: The KDC is not running. Resolution: Start the KDC service.",
      "E": "Cause: The user is not a member of the correct OS group. Resolution: add the user to the group"
    },
    "correctAnswers": [
      "B"
    ],
    "explanation": "The 'KDC has no support for encryption type' error indicates a mismatch in supported encryption types between the client and the KDC. Modifying Vetc/krb5.conf on both the client and KDC to enable a shared encryption type (e.g., aes256-cts) is the correct solution. Principal lockouts, expired tickets, or a stopped KDC would result in different errors."
  },
  {
    "id": 37,
    "question": "Which of the following security features are available to protect data in transit in a Cloudera CDP\ncluster?",
    "options": {
      "A": "SSL/TLS",
      "B": "SSH Tunneling",
      "C": "Ipsec",
      "D": "Kerberos Authentication",
      "E": "Data at Rest Encryption"
    },
    "correctAnswers": [
      "A",
      "B",
      "C"
    ],
    "explanation": "SSL/TLS, SSH Tunneling and IPSec can all be used for securing data in transit, however, Kerberos is an Authentication mechanism, not encryption and 'Data at Rest Encryption' ensures protection of physically stored data and not during transit."
  },
  {
    "id": 38,
    "question": "You are planning a CDP on-premises deployment for an organization with strict data governance\nrequirements. Which of the following are crucial considerations during the planning phase regarding\ndata lineage and security?",
    "options": {
      "A": "Choosing a CMDB (Configuration Management Database) integrated with Atlas to track metadata changes.",
      "B": "Implementing Ranger policies at the HDFS directory level to control access based on user roles and groups.",
      "C": "Ignoring Kerberos configuration until after the cluster is operational to simplify initial setup.",
      "D": "Defining a detailed data retention policy and configuring appropriate archival strategies.",
      "E": "Limiting the number of Ranger plugins to reduce complexity and improve performance."
    },
    "correctAnswers": [
      "A",
      "B",
      "D"
    ],
    "explanation": "Data governance requires careful planning around lineage (Atlas integration), security (Ranger policies), and retention policies. Kerberos is crucial from the start, and limiting Ranger plugins can compromise security requirements."
  },
  {
    "id": 39,
    "question": "During the sizing exercise for your CDP on-premises cluster, you need to estimate the required storage\ncapacity for HDFS. Which of the following factors significantly influence this estimation?",
    "options": {
      "A": "The replication factor configured for HDFS blocks.",
      "B": "The number of compute nodes in the cluster.",
      "C": "The compression codec used for storing data.",
      "D": "The size of the NameNode's heap memory.",
      "E": "The expected data ingestion rate and retention period."
    },
    "correctAnswers": [
      "A",
      "C",
      "E"
    ],
    "explanation": "HDFS storage sizing is directly impacted by replication factor (more copies, more storage), compression (reduces storage footprint), and the amount of data ingested and retained. The number of compute nodes affects processing power, and NameNode heap size relates to metadata management, not raw storage capacity."
  },
  {
    "id": 40,
    "question": "You are deploying a new CDP on-premises cluster with high availability (HA) enabled for HDFS\nNameNode. Which of the following components are essential for ensuring seamless failover in case of a\nNameNode failure?",
    "options": {
      "A": "Quorum Journal Manager (QJM).",
      "B": "ZooKeeper.",
      "C": "Oozie Workflow Engine.",
      "D": "Standalone Backup NameNode.",
      "E": "ResourceManager HA."
    },
    "correctAnswers": [
      "A",
      "B"
    ],
    "explanation": "QJM is responsible for sharing edits between active and standby NameNodes, while ZooKeeper manages the automatic failover process by monitoring the active NameNode's health. Oozie is a workflow scheduler, Backup NameNode is deprecated, and ResourceManager HA pertains to YARN."
  },
  {
    "id": 41,
    "question": "Which of the following network configurations is generally recommended for optimal performance and\nsecurity in a CDP on-premises cluster?",
    "options": {
      "A": "A single flat network for all cluster components to simplify management.",
      "B": "Segmenting the network into separate VLANs for different service tiers (e.g., compute, storage, management).",
      "C": "Disabling inter-node communication to minimize network traffic.",
      "D": "Using a dedicated private network for inter-node communication within the cluster.",
      "E": "Relying solely on the public internet for all data transfers."
    },
    "correctAnswers": [
      "B",
      "D"
    ],
    "explanation": "Network segmentation (VLANs) enhances security and performance by isolating traffic. A dedicated private network for inter-node communication minimizes latency and contention. A flat network is insecure and doesn't scale well. Disabling inter-node communication would prevent the cluster from functioning."
  },
  {
    "id": 42,
    "question": "You are planning a CDP on-premises deployment that includes both batch processing (using Spark) and\nreal-time streaming analytics (using Kafka and Flink). How should you configure the YARN resource\nqueues to ensure fair resource allocation and prevent resource starvation?",
    "options": {
      "A": "Create a single large YARN queue for all applications to maximize resource utilization.",
      "B": "Configure separate YARN queues for Spark, Kafka, and Flink, with appropriate resource limits and priorities.",
      "C": "Use the default YARN queue for all applications without any modifications.",
      "D": "Disable YARN queue management and rely on the default scheduler behavior.",
      "E": "Use capacity scheduler and configure resources based on guaranteed capacity and maximum capacity for each Queue."
    },
    "correctAnswers": [
      "B",
      "E"
    ],
    "explanation": "Separate YARN queues with resource limits and priorities are crucial for managing resources in a mixed workload environment. This prevents one application from consuming all resources and starving others. The capacity scheduler, with configurations like guaranteed and maximum capacity, provides predictable resource allocation."
  },
  {
    "id": 43,
    "question": "After deploying your CDP on-premises cluster, you observe slow query performance in Impala’. You\nsuspect that the issue might be related to metadata synchronization with the Hive Metastore. How can\nyou diagnose and troubleshoot this problem?",
    "options": {
      "A": "Restart all Impala daemons to force a full metadata refresh.",
      "B": "Use the ‘INVALIDATE METADATA’ command in Impala to refresh metadata for specific tables or databases.",
      "C": "Examine the Impala daemon logs for errors related to Hive Metastore connectivity or metadata retrieval.",
      "D": "Use 'COMPUTE STATS in Hive for all tables to refresh the statistics used by Impala query optimizer.",
      "E": "Reduce number of impalad processes"
    },
    "correctAnswers": [
      "B",
      "C",
      "D"
    ],
    "explanation": "'INVALIDATE METADATA’ allows targeted metadata refresh. Analyzing Impala daemon logs helps identify connection or retrieval issues. 'COMPUTE STATS' in Hive refreshes table statistics which Impala uses for query optimization. Restarting all daemons is a blunt approach. Reducing number of Impalad processes reduces parallelism."
  },
  {
    "id": 44,
    "question": "You're deploying a CDP cluster with Ranger KMS (Key Management Server) and need to encrypt data at\nrest in HDFS. Which configuration steps are essential to ensure Ranger KMS can successfully encrypt and\ndecrypt data?",
    "options": {
      "A": "Configure the 'hadoop.security.key.provider.path' property in core-site.xml to point to the Ranger KMS URI.",
      "B": "Create encryption zones in HDFS and associate them with encryption keys managed by Ranger KMS.",
      "C": "Install the Ranger KMS client libraries on all nodes in the cluster.",
      "D": "Disable Kerberos authentication to simplify the encryption process.",
      "E": "Grant appropriate permissions to users and groups in Ranger KMS to access and manage encryption keys."
    },
    "correctAnswers": [
      "A",
      "B",
      "E"
    ],
    "explanation": "The 'hadoop.security.key.provider.path' property tells HDFS where to find the key provider. Encryption zones link directories to keys. Ranger KMS permissions control key access. Ranger KMS client libraries are server-side components. Kerberos is required for secure operation."
  },
  {
    "id": 45,
    "question": "Your organization wants to leverage cloud storage (e.g., AWS S3, Azure Blob Storage) for archival and\nbackup of data from your CDP on-premises cluster. What configuration is necessary to enable this\nfunctionality?",
    "options": {
      "A": "Configure core-site.xml with the appropriate cloud storage endpoint and credentials (e.g., &fs.s3a.endpoint’, Sfs.s3a.access.key’ , ‘fs.s3a.secret.key’",
      "B": "Install the relevant cloud storage connector libraries (e.g., Hadoop AWS, Hadoop Azure) on all nodes in the cluster.",
      "C": "Create external tables in Hive that point directly to the data stored in cloud storage.",
      "D": "Ensure that the cloud storage bucket is publicly accessible to all users.",
      "E": "Setup replication between the HDFS cluster and the cloud storage using DistCp or similar tools."
    },
    "correctAnswers": [
      "A",
      "B",
      "E"
    ],
    "explanation": "Cloud storage access requires configuring endpoints and credentials, installing connector libraries, and using tools like DistCp to transfer data. External tables can access data, but archival needs replication. Publicly accessible buckets are a security risk."
  },
  {
    "id": 46,
    "question": "You are planning to upgrade your CDP on-premises cluster from a previous version. Describe a proper\nrollback procedure?",
    "options": {
      "A": "Take backups for all configurations before starting upgrades. Then stop CDP services in reverse order after failed upgrade and restore configurations",
      "B": "If the upgrade fails, simply restart the Cloudera Manager Server and the cluster will automatically revert to the previous state.",
      "C": "Take snapshots of all virtual machines in the cluster before starting the upgrade. If the upgrade fails, revert to the snapshots.",
      "D": "Backing up the Hive metastore database and restoring it in case of failure, backing up HDFS data, backing up CM database",
      "E": "No rollback is possible and we should always plan for the upgrade to be successful."
    },
    "correctAnswers": [
      "A",
      "C",
      "D"
    ],
    "explanation": "Backups of the Hive metastore database, HDFS data and all configurations are crucial. VM snapshots provide a fast, full-system rollback capability. Cloudera Manager does not automatically revert and upgrades can sometimes fail. Plan the rollback procedure during the Planning for Deployment itself."
  },
  {
    "id": 47,
    "question": "While planning a deployment, you need to account for node failures in the cluster. How do you handle\ndecommissioning and re- commissioning of datanodes in CDP?",
    "options": {
      "A": "Decommissioning datanodes involves stopping the DataNode service and removing the node from the Cloudera Manager. All data on the node is automatically replicated to other nodes.",
      "B": "Recommissioning datanodes involves adding the node back to Cloudera Manager, starting the DataNode service, and allowing HDFS to rebalance the data.",
      "C": "You must manually copy data off the datanode before decommissioning or it will be lost.",
      "D": "Before recommissioning, ensure that the old data directories are wiped or formatted to avoid potential conflicts.",
      "E": "Data replication will automatically take care of decommissioning and decommission the node"
    },
    "correctAnswers": [
      "A",
      "B",
      "D"
    ],
    "explanation": "Decommissioning gracefully moves data off the node (automatic replication). Recommissioning involves adding the node back to the cluster and data rebalancing. Old data directories must be cleaned. Manual copying isn't required if you decomission via CM."
  },
  {
    "id": 48,
    "question": "You are setting up Kerberos authentication for your CDP cluster. A client application outside the cluster\nneeds to access HDFS. What steps are necessary to allow the application to authenticate with Kerberos?",
    "options": {
      "A": "The application must obtain a Kerberos ticket-granting ticket (TGT) using ‘kinit' with a valid principal and keytab.",
      "B": "The application's Kerberos configuration file (krb5.conf) must be configured to point to the Kerberos realm and KDC (Key Distribution Center) of the CDP cluster.",
      "C": "The application must be added to the 'hadoop.proxyuser’ configuration in core-site.xml.",
      "D": "The application must use simple authentication.",
      "E": "The application user must be granted the permission in ranger."
    },
    "correctAnswers": [
      "A",
      "B",
      "E"
    ],
    "explanation": "The application needs a Kerberos TGT. The Kerberos configuration needs to be correct. It needs to use Ranger to grant the required permissions. simple authentication wont work."
  },
  {
    "id": 49,
    "question": "Your company’s security policy requires all data at rest to be encrypted and all data in transit to be\nencrypted. As a CDP administrator, what should you do to ensure the cluster follows these policies?",
    "options": {
      "A": "Enable encryption at rest using HDFS encryption zones and configure Ranger KMS to manage encryption keys.",
      "B": "Enable TLS (Transport Layer Security) for all CDP services to encrypt data in transit.",
      "C": "Disable Kerberos authentication, since encryption already provides sufficient security.",
      "D": "Rely solely on network firewalls to protect data in transit.",
      "E": "Configure the right role based permissions and create required Tag based policies"
    },
    "correctAnswers": [
      "A",
      "B",
      "E"
    ],
    "explanation": "HDFS encryption zones with Ranger KMS provide data-at-rest encryption. TLS encrypts data in transit. Kerberos is essential for authentication. Firewalls are necessary but not sufficient for data-in-transit protection and the application user has right permissions."
  },
  {
    "id": 50,
    "question": "You are configuring a CDP Private Cloud Base cluster in an isolated network (no direct internet access).\nWhich of the following are essential considerations for successful deployment and operation?",
    "options": {
      "A": "Implementing a local artifact repository (e.g., Artifactory, Nexus) containing all necessary RPMs and dependencies.",
      "B": "Configuring a local DNS server that can resolve all internal hostnames and forward external resolution requests to a designated internet-connected DNS server (if allowed).",
      "C": "Ensuring all nodes have direct SSH access to the Cloudera Manager server.",
      "D": "Setting up a Network Time Protocol (NTP) server within the isolated network and synchronizing all nodes to it.",
      "E": "Using Cloudera's public YUM repository directly through a proxy server without any local caching."
    },
    "correctAnswers": [
      "A",
      "B",
      "D"
    ],
    "explanation": "A, B, and D are crucial. A local artifact repository provides necessary software packages. A local DNS server resolves internal hostnames, and NTP ensures time synchronization across the cluster. C is not directly related to isolated network requirements, although it is good practice. E is incorrect because relying solely on a proxy without local caching creates a single point of failure and impacts performance within an isolated network, and also violates the network isolation policy."
  },
  {
    "id": 51,
    "question": "Your organization mandates that all data at rest in your CDP cluster within an isolated network must be\nencrypted using keys managed within the same isolated network. Which of the following components\nand configurations are necessary to satisfy this requirement?",
    "options": {
      "A": "Using Cloudera Navigator Key Trustee Server (KTS) deployed within the isolated network.",
      "B": "Configuring HDFS encryption with a Key Management Server (KMS) located outside the isolated network, but accessible via HTTPS.",
      "C": "Enabling encryption at the filesystem level on each node using LUKS.",
      "D": "Using an HSM (Hardware Security Module) located within the isolated network and integrated with the KMS.",
      "E": "Relying solely on cloud provider-managed encryption keys because they offer superior security."
    },
    "correctAnswers": [
      "A",
      "D"
    ],
    "explanation": "To comply with the isolated network requirement, keys must be managed within the network. KTS and HSM integration fulfill this. KTS manages keys within the isolated network. HSM provides hardware- backed key storage and cryptographic operations, also within the isolated network. KMS located outside the network violates isolation. Filesystem encryption is a viable option, but not centrally managed through KMS and doesn't address all components within CDP. Option E violates network isolation."
  },
  {
    "id": 52,
    "question": "You are troubleshooting an issue where nodes in your CDP cluster within an isolated network are unable\nto register with the Cloudera Manager server. You suspect a network issue. Which of the following tools\nand techniques are most effective for diagnosing the problem?",
    "options": {
      "A": "Using ‘ping' and ‘traceroute' from the nodes to the Cloudera Manager server hostname and IP address.",
      "B": "Using ‘netcat’ (nc) or 'telnet to verify connectivity to the Cloudera Manager server on port 7182.",
      "C": "Examining the Cloudera Manager server's logs for registration errors.",
      "D": "Relying solely on the Cloudera Manager UIfor connection status and error messages.",
      "E": "Temporarily disabling the firewall on all nodes to rule out firewall issues."
    },
    "correctAnswers": [
      "A",
      "B",
      "C"
    ],
    "explanation": "A, B, and C are effective. ‘ping' and ‘traceroute' identify basic network connectivity issues. ‘netcatTtelnet’ verify connectivity to the Cloudera Manager port. Examining server logs provides error context. The Cloudera Manager UIcan be helpful, but doesn't replace direct troubleshooting. Disabling the firewall is a security risk and should be avoided if possible. The preferred strategy is to examine the firewall rules."
  },
  {
    "id": 53,
    "question": "You are configuring network settings for a new CDP Private Cloud Base cluster in an isolated network.\nThe cluster requires communication with an external Active Directory server for authentication. Which\nof the following approaches is most secure and recommended for enabling this communication?",
    "options": {
      "A": "Opening all ports between the cluster nodes and the Active Directory server.",
      "B": "Configuring a firewall to allow only the necessary ports (e.g., LDAP, LDAPS) between specific cluster nodes (e.g., those running the Identity Management service) and the Active Directory server.",
      "C": "Placing the Active Directory server within the isolated network.",
      "D": "Configuring a VPN tunnel between the isolated network and the network containing the Active Directory server, allowing unrestricted traffic flow.",
      "E": "Forwarding all DNS requests from the isolated network to the Active Directory server's DNS server."
    },
    "correctAnswers": [
      "B"
    ],
    "explanation": "Option B provides the most secure approach. Restricting access to only necessary ports and specific nodes minimizes the attack surface. Option A is insecure. Option C might not be feasible or desirable. Option D defeats the purpose of network isolation. Option E could expose internal network information."
  },
  {
    "id": 54,
    "question": "You need to upgrade your CDP cluster in an isolated environment. You have downloaded the necessary\nparcels to a local artifact repository. How do you configure Cloudera Manager to use this local repository\nfor parcel distribution?",
    "options": {
      "A": "Modify the property in the Cloudera Manager's advanced configuration to point to the local artifact repository URL.",
      "B": "Set the property in the Cloudera Manager's configuration to the local artifact repository URL.",
      "C": "Add the local artifact repository URL as a remote repository in the Cloudera Manager Admin Console under Parcels > Edit Settings.",
      "D": "Use the command-line tool to update the Cloudera Manager's parcel repository configuration.",
      "E": "Mount the local artifact repository as a network drive on each node in the cluster."
    },
    "correctAnswers": [
      "C"
    ],
    "explanation": "The most straightforward way is to add the local artifact repository URL as a remote repository through the Cloudera Manager Admin Console. This allows Cloudera Manager to discover and distribute parcels from the local repository. Options A, B, and D refer to internal properties that are not directly modifiable or are deprecated. Option E is not practical or supported."
  },
  {
    "id": 55,
    "question": "Your Security team wants to ensure only specific users are allowed to access data via Hive/lmpala in an\nisolated environment. What are the key components and configurations required to enable fine-grained\nauthorization in this scenario?",
    "options": {
      "A": "Enable Apache Ranger and integrate it with Hive and Impala for authorization policies. Ensure Ranger is deployed within the isolated network.",
      "B": "Use only Hive's built-in SQL standard-based authorization.",
      "C": "Configure Sentry with the appropriate privileges for each user.",
      "D": "Rely on the underlying file system permissions for data access control.",
      "E": "Configure Kerberos authentication without implementing any authorization framework."
    },
    "correctAnswers": [
      "A"
    ],
    "explanation": "Apache Ranger provides centralized, fine-grained authorization policies for Hive and Impala and integrates well with CDP. It must be deployed inside the isolated network to maintain security and isolation. Hive's SQL standard authorization is limited. Sentry is deprecated in CDP. File system permissions are insufficient for comprehensive data access control. Kerberos handles authentication but not authorization."
  },
  {
    "id": 56,
    "question": "Assume you have a CDP cluster in an isolated network without direct internet access. You have\nconfigured a local YUM repository. After adding a new node to the cluster, the Cloudera Manager Agent\nfails to install because it cannot find necessary packages. What is the MOST likely cause and how can\nyou resolve this?",
    "options": {
      "A": "The node's 'letc/yum.repos.d/' configuration is pointing to the public Cloudera YUM repository instead of your local mirror. Modify the repository configuration on the new node to point to the local YUM repository URL.",
      "B": "The Cloudera Manager Agent requires direct internet access to download its dependencies. Provide temporary internet access to the node during installation.",
      "C": "The local YUM repository is not properly synchronized with the Cloudera YUM repository. Run ‘yum repodata’ on the YUM server to regenerate the metadata.",
      "D": "The firewall on the new node is blocking access to the local YUM repository. Configure the firewall to allow access to the YUM repository port (e.g., 80 or 443).",
      "E": "The newly added node has not yet been registered within Cloudera Manager. Complete host inspection from Cloudera Manager I-II."
    },
    "correctAnswers": [
      "A"
    ],
    "explanation": "The most common cause is an incorrect YUM repository configuration on the new node. Modifying '/etc/yum.repos.dP to point to the local mirror solves this. Requiring internet access defeats the isolation requirement. 'yum repodata’ regenerates metadata but doesn't address the configuration issue. Firewall issues and registration problems are less likely if the core issue is incorrect repository configuration. Re-running host inspection from CM also won't solve the YUM configuration error."
  },
  {
    "id": 57,
    "question": "You have a Spark application that needs to access data stored in Hive tables within your isolated CDP\ncluster. The application is running outside the cluster on a separate analytics server, also within the\nisolated network, but not managed by Cloudera Manager. How do you configure secure and authorized\naccess for this Spark application to the Hive data?",
    "options": {
      "A": "Configure a JDBC connection from the Spark application to the HiveServer2 service, enabling Kerberos authentication and authorization.",
      "B": "Grant the Spark application's user account full access to all Hive tables.",
      "C": "Disable Kerberos authentication on the HiveServer2 service.",
      "D": "Copy the Hive metastore database to the analytics server.",
      "E": "Create a separate Hive metastore specifically for the Spark application."
    },
    "correctAnswers": [
      "A"
    ],
    "explanation": "A JDBC connection with Kerberos authentication provides secure and authorized access. It leverages the existing security framework of the cluster. Granting full access is a security risk. Disabling Kerberos is not secure. Copying the metastore is complex and doesn't address authorization. A separate metastore is unnecessary and complicates data management."
  },
  {
    "id": 58,
    "question": "You are implementing a multi-tenant CDP cluster in an isolated network. Each tenant requires a\ndedicated HDFS namespace and compute resources. Which of the following architectural approaches\nwould you recommend for achieving resource isolation and data security in this scenario?",
    "options": {
      "A": "Use HDFS quotas to limit storage for each tenant and YARN queues to manage compute resources, all within a single cluster.",
      "B": "Create separate CDP clusters for each tenant.",
      "C": "Use HDFS ACLs for data isolation and rely on operating system-level resource limits for compute isolation.",
      "D": "Utilize Kubernetes for resource isolation while still using a single Cloudera Data Platform cluster.",
      "E": "Configure separate namespaces within the Hive metastore for each tenant."
    },
    "correctAnswers": [
      "A"
    ],
    "explanation": "HDFS quotas and YARN queues provide a balance between resource isolation and management overhead within a single cluster, without violating the isolated network requirement. Separate clusters increase management complexity. HDFS ACLs are more granular, but do not provide adequate resource isolation without quotas. While Kubernetes can be used with CDP, it is a complex solution for basic tenant isolation and might increase management overhead inside a completely isolated network. Hive namespaces do not provide compute resource isolation."
  },
  {
    "id": 59,
    "question": "You are tasked with setting up monitoring for your CDP cluster in an isolated network. Since you do not\nhave external access, how would you configure monitoring and alerting without relying on cloud-based\nsolutions? Select all that apply.",
    "options": {
      "A": "Implement a local instance of Prometheus and Grafana within the isolated network to collect metrics and visualize dashboards.",
      "B": "Configure Cloudera Manager to send email alerts via an internal SMTP server.",
      "C": "Forward all system logs to an external, cloud-based logging service using a secure VPN connection.",
      "D": "Utilize Cloudera Manager's built-in monitoring capabilities and dashboards exclusively.",
      "E": "Setup Nagios to monitor the hosts, including CPU Utilization, Disk Space and Network bandwidth."
    },
    "correctAnswers": [
      "A",
      "B",
      "D",
      "E"
    ],
    "explanation": "A, B, and D are valid solutions. Prometheus and Grafana offer comprehensive monitoring and visualization within the isolated network. Configuring Cloudera Manager to send email alerts via an internal SMTP server provides notifications. Cloudera Manager's built-in monitoring offers basic monitoring capabilities. Forwarding logs to the cloud violates the isolation requirement. Nagios would be an option, which can give CPU Utilization, Disk Space and Network bandwidth metrics to the monitoring team."
  },
  {
    "id": 60,
    "question": "You are managing a CDP cluster in an isolated network. Your data science team needs to use Python\nlibraries not included in the base Anaconda distribution provided with CDP. What is the recommended\napproach to install and manage these custom Python libraries within the isolated environment? Assume\nno direct internet access.",
    "options": {
      "A": "Download the required Python packages (wheels or source distributions) from a machine with internet access, transfer them to a local repository within the isolated network, and use 'pip' with the ‘-- find-linkS option to install them.",
      "B": "Use ‘conda’ to create a new environment and install packages directly from the Anaconda Cloud using a proxy server.",
      "C": "Manually copy the Python library files to the appropriate directories within the base Anaconda environment on each node.",
      "D": "Build a custom Anaconda distribution including the required libraries and distribute it to all nodes.",
      "E": "Install Miniconda on each node and download dependencies using ‘conda install' command."
    },
    "correctAnswers": [
      "A"
    ],
    "explanation": "Option A is the most practical and recommended approach. It leverages 'pips and a local repository to manage dependencies without internet access. Using a proxy server for Anaconda Cloud access might be possible but introduces complexity and potential security concerns. Manually copying files is error- prone and difficult to maintain. Building a custom Anaconda distribution is complex and requires significant effort. Miniconda requires internet access and might not be an ideal solution for a fully air- gapped network."
  },
  {
    "id": 61,
    "question": "An organization with a Cloudera Data Platform (CDP) on-premises deployment in an isolated network\nrequires a secure method for transmitting data to an external cloud environment for specific processing\ntasks. Which of the following methods represents the MOST secure approach to accomplish this while\nminimizing the risk of exposing the entire on-premises infrastructure?",
    "options": {
      "A": "Create a dedicated, one-way data diode that only allows data to flow from the on-premises CDP cluster to a secure data ingestion point in the cloud environment.",
      "B": "Establish a permanent VPN tunnel between the on-premises network and the cloud environment, allowing unrestricted access to all resources.",
      "C": "Utilize a secure file transfer protocol (SFTP) to periodically copy data to a publicly accessible cloud storage bucket.",
      "D": "Implement a custom application that directly writes data to a cloud-based database using exposed API endpoints.",
      "E": "Use Cloudera Replication Manager to replicate data to the cloud after ensuring correct network configurations are in place."
    },
    "correctAnswers": [
      "A"
    ],
    "explanation": "A data diode ensures a unidirectional data flow, preventing any potential backdoors or lateral movement from the cloud environment to the on-premises CDP cluster. This offers the highest level of security. Options B, C, and D introduce significant security risks by allowing bidirectional communication or exposing the data to potential vulnerabilities. Cloudera Replication Manager could be a valid approach but relies on the network connectivity which may not be highly secure in isolated environments."
  },
  {
    "id": 62,
    "question": "You are configuring High Availability (HA) for the NameNode in your on-premises CDP cluster. You've set\nup two NameNode hosts and configured shared storage. After initial setup, you notice frequent failovers\noccurring. Which of the following could be the MOST likely cause of these frequent failovers, assuming\nthe network is stable and hosts are healthy?",
    "options": {
      "A": "Incorrect configuration of the 'dfs.ha.automatic-failover.enabled' property in 'hdfs-site.xmr.",
      "B": "Insufficient memory allocated to the ZKFailoverController (ZKFC) processes on both NameNode hosts.",
      "C": "Mismatching 'dfs.nameserviceS value between the client configuration and the NameNodes' configuration.",
      "D": "Incorrect quorum configuration in ZooKeeper, leading to split-brain scenarios.",
      "E": "Using NFS as shared storage for the EditLog, leading to synchronization issues."
    },
    "correctAnswers": [
      "B"
    ],
    "explanation": "Frequent failovers, even with shared storage and stable network, are often caused by resource contention on the ZKFC processes. Insufficient memory can cause ZKFC to incorrectly report the active NameNode as unhealthy, triggering unnecessary failovers. While NFS can be problematic, it usually manifests as data corruption rather than frequent failovers. Other options are less likely to cause frequent failovers after the initial setup if the initial config was correct."
  },
  {
    "id": 63,
    "question": "In a CDP on-premises deployment with Kerberos enabled and High Availability configured for the\nNameNode, which configuration file requires modification BOTH on the client machines and on the\nNameNode hosts to ensure proper communication after a failover?",
    "options": {
      "A": "'core-site.xml'",
      "B": "‘ krb5.conf",
      "C": "'hdfs-site.xmr",
      "D": "‘yarn-site.xml'",
      "E": "mapred-site.xmr"
    },
    "correctAnswers": [
      "A"
    ],
    "explanation": "The ‘core-site.xmr file contains the 'fs.defaultFS' property, which defines the logical name service for HDFS. This value is used by clients and other Hadoop components to locate the active NameNode. After a failover, this property needs to point to the HA nameservice (e.g., 'hdfs://mycluster') rather than a specific NameNode host. This update is necessary on both client machines and NameNode hosts to ensure proper communication after a failover."
  },
  {
    "id": 64,
    "question": "You are tasked with setting up HA for the Hive Metastore in your CDP on-premises cluster. Which of the\nfollowing is the MOST critical configuration step to ensure seamless failover and data consistency?",
    "options": {
      "A": "Configuring multiple HiveServer2 instances behind a load balancer.",
      "B": "Using a highly available, external database (e.g., PostgreSQL with replication) for the Hive Metastore.",
      "C": "Enabling HiveServer2 authentication using Kerberos.",
      "D": "Configuring the ‘hive.metastore.uris’ property to point to all Metastore instances.",
      "E": "Setting up replication for the Hive warehouse directory in HDFS."
    },
    "correctAnswers": [
      "B"
    ],
    "explanation": "The MOST critical step for Hive Metastore HA is using a highly available external database. The Metastore contains the metadata that describes the structure of your Hive tables. If this metadata is lost or corrupted during a failover, you could lose access to your data. Configuring multiple HiveServer2 instances behind a load balancer (option A) only ensures availability of the HiveServer2 service, not the Metastore itself. A replicated warehouse directory helps with data availability, but doesn't solve metadata unavailability."
  },
  {
    "id": 65,
    "question": "You are troubleshooting an issue where applications are failing to connect to the ResourceManager in\nyour HA-enabled CDP cluster. The ResourceManager failover appears to be working correctly. After\nchecking the logs, you find errors related to 'Connection refused'. Which of the following is the MOST\nlikely cause?",
    "options": {
      "A": "The ‘yarn.resourcemanager.ha.id’ property is not consistently configured across all ResourceManager instances.",
      "B": "The firewall on the ResourceManager host is blocking incoming connections on the ResourceManager ports.",
      "C": "The ‘yarn.resourcemanager.hostname’ property is set to the hostname of the standby ResourceManager instance in 'yarn-site.xml'.",
      "D": "The ‘yarn.resourcemanager.address’ property is incorrectly configured to point to a specific ResourceManager instance instead of the HA nameservice address.",
      "E": "ZooKeeper is not running or is unreachable."
    },
    "correctAnswers": [
      "D"
    ],
    "explanation": "In an HA setup, clients should connect to the ResourceManager using a logical name service or a load balancer, not a specific ResourceManager instance. 'yarn.resourcemanager.address (and related properties like 'yarn.resourcemanager.webapp.address') should point to this abstract address. If it's pointing to a specific instance, clients will fail to connect when that instance is not the active one. The other options could cause issues, but 'Connection refused' strongly suggests an addressing problem."
  },
  {
    "id": 66,
    "question": "You need to implement HA for the Oozie server in your CDP on-premises cluster. Which of the following\nstatements regarding Oozie HA configuration are TRUE ? (Select TWO)",
    "options": {
      "A": "Oozie HA relies on a shared file system (e.g., HDFS) to store the Oozie database.",
      "B": "Oozie HA requires a dedicated ZooKeeper ensemble separate from the one used by HDFS.",
      "C": "All Oozie server instances must connect to the same external database for storing workflow definitions and runtime information.",
      "D": "Oozie HA is automatically enabled when you configure HA for HDFS and YARN.",
      "E": "The ‘oozie.ha.system.id' property must be unique across all Oozie server instances in the HA setup."
    },
    "correctAnswers": [
      "A",
      "C"
    ],
    "explanation": "Oozie HA requires a shared file system (typically HDFS) to store the Oozie database and workflow definitions. All Oozie server instances must connect to the same external database (e.g., MySQL, PostgreSQL) for storing workflow definitions and runtime information, ensuring data consistency during failover. Option B is incorrect; Oozie typically uses the same ZooKeeper ensemble as HDFS and YARN. Option D is incorrect; Oozie HA needs to be explicitly configured. Option E is incorrect; ‘oozie.ha.system.id' should be identical across all instances. It defines the HA group."
  },
  {
    "id": 67,
    "question": "Which of the following mechanisms does the ZKFailoverController (ZKFC) use to determine the health\nstatus of the NameNode in a High Availability (HA) configuration?",
    "options": {
      "A": "Regularly checking the NameNode's JMX metrics for memory usage and CPU utilization.",
      "B": "Monitoring the NameNode's heartbeat signals sent to the JournalNodes.",
      "C": "Performing periodic ‘hdfs dfsadmin -report' commands and analyzing the output.",
      "D": "Using a configured health check script Cdfs.ha.fencing.methodS) to verify NameNode responsiveness.",
      "E": "Listening for notifications from the NameNode about its current state (active or standby) via RPC."
    },
    "correctAnswers": [
      "D"
    ],
    "explanation": "The ZKFC uses the configured fencing methods ('dfs.ha.fencing.methods') to determine the health of the NameNode. This usually involves a script that checks the NameNode's responsiveness and attempts to prevent split-brain scenarios by fencing off the old active NameNode. While the NameNode does interact with JournalNodes (option B), the ZKFC doesn't directly monitor those interactions for health status. The other options are not the primary mechanisms used by ZKFC for health checks."
  },
  {
    "id": 68,
    "question": "You are planning to implement HA for your NameNode using QJM (Quorum Journal Manager). What is\nthe minimum number of JournalNodes required for a QJM-based HA configuration to tolerate one\nJournalNode failure?",
    "options": {
      "A": "2",
      "B": "3",
      "C": "4",
      "D": "5",
      "E": "1"
    },
    "correctAnswers": [
      "B"
    ],
    "explanation": "For a QJM-based HA configuration to tolerate one JournalNode failure, you need a minimum of three JournalNodes. This ensures that a majority (quorum) of JournalNodes are always available to record EditLog transactions, even if one JournalNode is down. A quorum is (N/2) + 1, where N is the number of JournalNodes."
  },
  {
    "id": 69,
    "question": "After configuring HA for the ResourceManager in your CDP cluster, you notice that applications are\nbeing submitted successfully, but they are not starting. The ResourceManager logs show errors related\nto 'Invalid transition at ACTIVE to STANDBY'. What is the most likely cause of this issue?",
    "options": {
      "A": "The ‘yarn.resourcemanager.ha.enabled' property is set to in 'yarn-site.xmr.",
      "B": "ZooKeeper is not properly configured or is unreachable by the ResourceManager instances.",
      "C": "The ‘yarn.resourcemanager.recovery.enabled' property is set to 'false' in ‘yarn-site.xmr.",
      "D": "The ResourceManager state store is corrupted.",
      "E": "Incorrect fencing configuration is preventing the standby ResourceManager from transitioning to active."
    },
    "correctAnswers": [
      "C"
    ],
    "explanation": "The error 'Invalid transition at ACTIVE to STANDBY' strongly suggests that the ResourceManager is not configured to recover its state. The 'yarn.resourcemanager.recovery.enabled’ property controls whether the ResourceManager attempts to recover its state from a state store (typically ZooKeeper). If this property is set to 'false' , the ResourceManager will not be able to transition to the STANDBY state correctly, preventing failover from working as expected. While ZooKeeper connectivity (option B) is crucial, the error message points directly to the recovery mechanism being disabled."
  },
  {
    "id": 70,
    "question": "You are implementing HA for your HDFS cluster. You chose Quorum Journal Manager (QJM) as your\nshared edit log storage. After you formatted the NameNodes and started the JournalNodes, the standby\nNameNode fails to synchronize with the active NameNode. The standby NameNode logs show the\nfollowing error: 'Failed to start standby services'. Further digging reveals that the root cause is:\n'org.apache.hadoop.hdfs.server.namenode.SafeModeException: Cannot create directory /foo/bar.\nName node is in safe mode.' Which of the following steps would resolve this issue and allow the standby\nNameNode to synchronize? (Select TWO)",
    "options": {
      "A": "Manually create the '/foo/bar’ directory on HDFS via the active NameNode.",
      "B": "Manually create the '/foo/bar’ directory on the standby NameNode local filesystem.",
      "C": "Take the active NameNode out of safemode using the command ‘hdfs dfsadmin -safemode leave'.",
      "D": "Restart the JournalNodes.",
      "E": "Increase the heap size of the NameNode."
    },
    "correctAnswers": [
      "A",
      "C"
    ],
    "explanation": "The error message indicates that the NameNode is in safemode and cannot create the directory '/foo/bar’ . The synchronization process requires creating directories and files in HDFS. Therefore, two actions are required: 1 . Create the directory S/foo/bar’ on the active NameNode, which will then be replicated to the standby. This is a one-time task if the directory is supposed to be on HDFS. 2. Take the NameNode out of safemode, as it prevents any modifications to the filesystem. Option A is necessary because it populates the directory expected. Option C is critical since being in safemode prohibits making any modifications to the filesystem. Restarting JournalNodes or increasing NameNode heap size are unrelated to this specific safemode issue. Directory S/foo/bar’ has to be in HDFS, not local standby namenode."
  },
  {
    "id": 71,
    "question": "You are responsible for ensuring high availability of the HBase Master in your CDP on-premises cluster.\nWhich of the following configuration parameters directly controls the number of backup HBase Master\ninstances that will be started?",
    "options": {
      "A": "hbase.master.distributed’",
      "B": "‘hbase.backup.masters’",
      "C": "hbase.zookeeper.quorum’",
      "D": "‘hbase.master.wait.for.regionserver.expiration'",
      "E": "hbase.rootdir’"
    },
    "correctAnswers": [
      "A"
    ],
    "explanation": "The 'hbase.master.distributed' parameter is what instructs HBase to start more than one master. Setting it to ‘true’ enables the distributed mode, which allows for multiple backup masters to be started. Setting it to 'false' will configure single master for HBASE clusteL All backup instances participate in master election via Zookeeper. 'hbase.backup.masterS specifies the hostnames for backup masters. 'hbase.zookeeper.quorum’ defines the ZooKeeper ensemble, which is required for HA, but does not control the number of backup Masters. hbase.master.wait.for.regionserver.expiration’ related to region servers and 'hbase.rootdirs specifies the HDFS path for HBase data."
  },
  {
    "id": 72,
    "question": "Consider the following scenario: You have an HA-enabled HDFS cluster with NameNode HA configured\nusing QJM. During a maintenance window, you shut down all the JournalNodes. Subsequently, the\nactive NameNode crashes. What will be the immediate state of the HDFS cluster and its ability to serve\nread/write requests?",
    "options": {
      "A": "The standby NameNode will automatically become active and the cluster will continue serving requests with minimal interruption.",
      "B": "The standby NameNode will attempt to become active but will fail, and the cluster will be unavailable for both read and write requests.",
      "C": "The standby NameNode will become active, but only read requests will be served. Write requests will fail until the JournalNodes are restarted.",
      "D": "The cluster will enter read-only mode until the active NameNode is manually recovered.",
      "E": "DataNodes will continue to serve data from their local caches, but any new data written will be lost."
    },
    "correctAnswers": [
      "B"
    ],
    "explanation": "With QJM, the NameNodes rely on a quorum of JournalNodes to maintain a consistent view of the filesystem. If all JournalNodes are down, the standby NameNode cannot become active because it cannot verify the latest state of the filesystem. The standby cannot read edits from journal nodes. The crash of active namenode will render HA setup unfunctional without journal node availability. This makes HDFS completely unavailable until the JournalNodes are restored. Therefore, option B is correct."
  },
  {
    "id": 73,
    "question": "You are setting up ResourceManager HA in your CDP cluster using automatic failover. You have noticed\nthat even though both\nResourceManagers are running, only one is ever active and the other always remains in standby mode.\nThe ZKFailoverController logs show frequent messages like 'Failed to acquire lock; another ZKFC is likely\nalready running'. What could be the cause?",
    "options": {
      "A": "The ‘yarn.resourcemanager.ha.rm-ids’ property contains duplicate ResourceManager IDs.",
      "B": "The clock synchronization between the ResourceManager hosts and the ZooKeeper ensemble is significantly skewed.",
      "C": "Firewall rules are blocking communication between the ZKFailoverController and the ZooKeeper ensemble.",
      "D": "The ZKFailoverController process is running multiple times on the same host for one or both ResourceManager instances.",
      "E": "The ‘yarn.resourcemanager.hostname’ property is not set correctly."
    },
    "correctAnswers": [
      "D"
    ],
    "explanation": "The error message 'Failed to acquire lock; another ZKFC is likely already running' indicates that multiple ZKFC processes are competing to become the active leader for the ResourceManager HA group. The most likely cause is that the ZKFC process has been started multiple times on the same host for one or both ResourceManager instances. This often happens due to misconfigured init scripts or systemd units. ZooKeeper relies heavily on accurate time for leader election and session management."
  },
  {
    "id": 74,
    "question": "You are configuring NameNode High Availability in CDP on-premise. Which of the following steps is\nMANDATORY for ensuring proper failover functionality?",
    "options": {
      "A": "Deploying a Quorum Journal Manager (QJM) and configuring the 'dfs.namenode.shared.edits.dir’ property.",
      "B": "Setting up a manual failover procedure involving SSH access to the standby NameNode.",
      "C": "Configuring the Resource Manager to point to both NameNodes.",
      "D": "Creating a symbolic link from the active NameNode's data directory to the standby NameNode's data directory.",
      "E": "Enabling NFS gateway on both namenodes."
    },
    "correctAnswers": [
      "A"
    ],
    "explanation": "Using a Quorum Journal Manager (QJM) and setting the ‘dfs.namenode.shared.edits.dir’ property correctly allows the standby NameNode to stay synchronized with the active NameNode by replaying edits from the shared journal. This ensures seamless failover. Manual failover is not ideal and QJM is the automated approach for HA."
  },
  {
    "id": 75,
    "question": "When planning backups for HDFS data in a CDP on-premise cluster, which of the following factors should\nbe considered to minimize the impact on cluster performance and ensure data consistency?",
    "options": {
      "A": "Run backups during peak hours to ensure all recent data is included.",
      "B": "Use distcp with a large number of mappers to maximize throughput, regardless of current cluster load.",
      "C": "Schedule backups during off-peak hours and use incremental backups where possible.",
      "D": "Disable all other jobs running on the cluster during the backup process.",
      "E": "Use HDFS snapshots and distcp for efficient data replication."
    },
    "correctAnswers": [
      "C",
      "E"
    ],
    "explanation": "Backups should be scheduled during off-peak hours to minimize the impact on production workloads. Incremental backups can reduce the amount of data transferred, and using HDFS snapshots allows for consistent data replication through distcp."
  },
  {
    "id": 76,
    "question": "You're tasked with configuring automatic failover for the ResourceManager (RM) in your CDP on-\npremise cluster. Which of the following components are essential for enabling this functionality?",
    "options": {
      "A": "Zookeeper",
      "B": "NameNode",
      "C": "ResourceManager state store",
      "D": "NodeManager",
      "E": "Quorum Journal Manager (QJM)"
    },
    "correctAnswers": [
      "A",
      "C"
    ],
    "explanation": "Zookeeper is used for leader election and maintaining the active ResourceManager state. The ResourceManager state store (typically embedded or a separate database) persists application state, enabling failover to a standby ResourceManager with minimal application interruption. QJM is for NameNode and not ResourceManager failover."
  },
  {
    "id": 77,
    "question": "Which configuration property controls the maximum number of concurrent distcp tasks when running\nbackups in CDP on-premise?",
    "options": {
      "A": "'distcp.map.count’",
      "B": "‘mapreduce.job.reduces’",
      "C": "mapreduce.job.mapS",
      "D": "'distcp.dynamic.mappers’",
      "E": "yarn.app.mapreduce.am.resource.mb'"
    },
    "correctAnswers": [
      "A"
    ],
    "explanation": "The 'distcp.map.count’ property in the Hadoop configuration controls the number of map tasks that distcp will use to copy data. Increasing this value can improve the speed of the backup, but it also increases the load on the cluster."
  },
  {
    "id": 78,
    "question": "You are observing frequent failovers between two NameNodes in your HA-enabled CDP on-premise\ncluster. The logs show errors related to fencing. What is the MOST likely cause?",
    "options": {
      "A": "Incorrect configuration of the Zookeeper quorum.",
      "B": "The standby NameNode is running on a machine with insufficient resources.",
      "C": "The fencing methods configured for the NameNodes are failing to reliably prevent split-brain scenarios.",
      "D": "The 'dfs.ha.automatic-failover.enabled' property is set to false.",
      "E": "There is a network connectivity issue between the two NameNodes."
    },
    "correctAnswers": [
      "C"
    ],
    "explanation": "Fencing is critical for preventing split-brain scenarios in HA clusters. If fencing methods are failing, it means that the active NameNode is not being reliably shut down or isolated before the standby NameNode becomes active, leading to data corruption and frequent failovers."
  },
  {
    "id": 79,
    "question": "Consider a scenario where you need to back up only the metadata of your Hive metastore in CDP on-\npremise. Which command would be MOST appropriate?",
    "options": {
      "A": "\"hdfs dfs -cp /user/hive/warehouse Ibackup/hive’",
      "B": "mysqldump -u hive -p hive > hive_metastore_backup.sqr (assuming MySQL metastore)",
      "C": "'distcp hdfs://nameservicel luser/hive/warehouse hdfs://backupcluster/backup/hive’",
      "D": "Shive —service metastore_admin -backup'",
      "E": "‘pg_dump -U hive -d hive > hive_metastore_backup.sqr (assuming PostgreSQL metastore)"
    },
    "correctAnswers": [
      "B",
      "E"
    ],
    "explanation": "Backing up the Hive metastore involves backing up the database that stores the metadata. ‘mysqldump’ (for MySQL) or ‘pg_dump’ (for PostgreSQL) are the appropriate tools for this. The 'hdfs dfS and 'distcp’ commands copy data in HDFS, not the metastore database itself."
  },
  {
    "id": 80,
    "question": "You are using Cloudera Manager to manage your CDP on-premise cluster. You want to configure alerts\nto notify you when the average write latency to the NameNode journal nodes exceeds a certain\nthreshold. Where would you configure this alert?",
    "options": {
      "A": "In the HDFS service's configuration properties within Cloudera Manager.",
      "B": "In the Cloudera Manager Admin Console, under 'Alerts' -> 'Create Alert'.",
      "C": "Directly in the journal node's ‘hdfs-site.xmr configuration file.",
      "D": "Using the Cloudera Manager API with a custom script.",
      "E": "Configuring an external monitoring tool like Prometheus."
    },
    "correctAnswers": [
      "B"
    ],
    "explanation": "Cloudera Manager provides a central interface for managing and monitoring your cluster. Alerts are configured within the Cloudera Manager Admin Console under the 'Alerts' section."
  },
  {
    "id": 81,
    "question": "In the context of HDFS snapshots, what is the primary advantage of using them for backups compared to\ndirectly copying the data?",
    "options": {
      "A": "Snapshots are always faster because they create a full copy of the data instantaneously.",
      "B": "Snapshots consume less storage space because they only store the differences between the snapshot and the current state of the files.",
      "C": "Snapshots are inherently more secure than direct data copies.",
      "D": "Snapshots allow you to restore individual files, while direct copies only allow for restoring entire directories.",
      "E": "Snapshots eliminate the need for a secondary storage location."
    },
    "correctAnswers": [
      "B"
    ],
    "explanation": "HDFS snapshots are space-efficient because they employ a copy-on-write mechanism. Only modifications made after the snapshot are stored, leading to significantly less storage consumption compared to a full data copy. They are not a replacement for backup storage. Snapshots offer a point-in- time view that you then copy somewhere else for backup."
  },
  {
    "id": 82,
    "question": "You have a critical Hive table that needs to be backed up regularly. The data volume is very large. You\nwant to create a backup solution that minimizes downtime during the backup process and provides a\npoint-in-time consistent view of the table. Which approach would be MOST suitable?",
    "options": {
      "A": "Using 'hdfs dfs -cp' to copy the table's data directory.",
      "B": "Using 'EXPORT TABLE and ‘IMPORT TABLE commands in Hive.",
      "C": "Creating an HDFS snapshot of the table's data directory and then using 'distcp’ to copy the snapshot to a backup location.",
      "D": "Using Hive replication.",
      "E": "Using ‘BACKUP TABLE and 'RESTORE TABLE commands."
    },
    "correctAnswers": [
      "C"
    ],
    "explanation": "Creating an HDFS snapshot provides a point-in-time consistent view of the table's data. Using 'distcp’ to copy the snapshot allows you to back up the data without significantly impacting ongoing write operations to the table, as the snapshot represents a fixed state. EXPORT/IMPORT has limitations. Hive replication can replicate to another cluster which is a good backup strategy, but snapshots are still useful. Hive backup and restore are newer features but snapshots and distcp may still be better for very large tables."
  },
  {
    "id": 83,
    "question": "Your organization mandates that all data at rest in your CDP on-premise cluster must be encrypted.\nWhich of the following components require specific configuration to enable encryption at rest?",
    "options": {
      "A": "HDFS",
      "B": "YARN",
      "C": "Hive Metastore",
      "D": "ZooKeeper",
      "E": "Oozie"
    },
    "correctAnswers": [
      "A",
      "C"
    ],
    "explanation": "HDFS requires the creation of encryption zones and key management configuration. The Hive Metastore, particularly if it contains sensitive metadata, needs to be encrypted through database- specific mechanisms. YARN, Zookeeper, and Oozie don't directly store persistent data that would be subject to encryption at rest in the same way as HDFS data or the Hive Metastore. While Zookeeper and Oozie have configurations, encryption at rest usually relates to HDFS and metastores."
  },
  {
    "id": 84,
    "question": "You have configured HA for your HDFS NameNodes using Quorum Journal Manager (QJM). After a\nfailover, you notice that the new active NameNode is not able to start properly. Upon investigation, you\nfind that the shared edits directory used by QJM contains corrupted data’. What is the MOST likely\nreason for this corruption?",
    "options": {
      "A": "The QJM servers are running on machines with different operating systems.",
      "B": "The number of QJM servers is not sufficient to tolerate the failure of one or more servers.",
      "C": "The 'dfs.namenode.shared.edits.dir’ property is not configured correctly on all NameNodes.",
      "D": "The clock skew between the NameNodes and the QJM servers is too high.",
      "E": "The QJM servers' disk space is full."
    },
    "correctAnswers": [
      "B"
    ],
    "explanation": "QJM uses a quorum-based approach to ensure the consistency of the edits log. If the number of QJM servers is not sufficient (e.g., less than 3), the failure of even one server can lead to a loss of quorum and potential data corruption in the shared edits directory. If the server numbers are correct corruption is unlikely. But corruption may appear if you don't have enough QJM nodes to allow a quorum."
  },
  {
    "id": 85,
    "question": "You are responsible for backing up a large HBase table in your CDP on-premise cluster. Which method\noffers the best combination of efficiency and minimal impact on the running HBase service?",
    "options": {
      "A": "Using 'hbase org.apache.hadoop.hbase.mapreduce.CopyTable' to copy the table to another location.",
      "B": "Taking an HBase snapshot and then using ‘ExportSnapshot’ to export the snapshot data to HDFS.",
      "C": "Disabling the HBase table and then copying the underlying HFiles directly from HDFS.",
      "D": "Performing a full cluster backup using the Cloudera Manager.",
      "E": "Using the ‘truncate command."
    },
    "correctAnswers": [
      "B"
    ],
    "explanation": "Taking an HBase snapshot is a fast and non-blocking operation that creates a point-in-time consistent view of the table. Using ExportSnapshot' allows you to efficiently export the snapshot data to HDFS for backup purposes with minimal impact on the running HBase service. Disabling the table leads to downtime. CopyTable can put load on the cluster. Truncate would delete the data."
  },
  {
    "id": 86,
    "question": "You are designing a multi-tenant Cloudera CDP on-premises cluster where each tenant requires strict\nisolation for compute resources. Which resource management technique provides the strongest\nguarantee of isolation between tenants?",
    "options": {
      "A": "Using YARN queues with fair scheduler and resource limits.",
      "B": "Implementing Ranger policies to control access to data directories.",
      "C": "Deploying multiple Cloudera Management Service (CMS) instances, one for each tenant.",
      "D": "Utilizing Kubernetes namespaces to deploy separate YARN clusters for each tenant.",
      "E": "Configuring Knox to serve as a gateway, authenticating and authorizing all tenant requests."
    },
    "correctAnswers": [
      "D"
    ],
    "explanation": "Kubernetes namespaces allow for the creation of completely isolated environments, including separate YARN clusters, offering the strongest isolation. YARN queues provide resource isolation, but within a single cluster. Ranger and Knox focus on authorization and authentication, not resource isolation at the compute level. Multiple CMS instances don't guarantee compute isolation."
  },
  {
    "id": 87,
    "question": "In a Cloudera CDP on-premises deployment, you need to configure High Availability (HA) for the\nNameNode. Which of the following steps are crucial for ensuring a successful HA setup?",
    "options": {
      "A": "Configuring a shared edit log storage (e.g., using NFS or a quorum-based journal node).",
      "B": "Deploying a ZooKeeper quorum to manage active NameNode election.",
      "C": "Ensuring both NameNodes have identical hardware specifications.",
      "D": "Setting up automatic failover using fencing methods (e.g., STONITH).",
      "E": "Configuring multiple Replication factor with in HDFS file system"
    },
    "correctAnswers": [
      "A",
      "B",
      "D"
    ],
    "explanation": "A shared edit log is essential for NameNode HA to maintain state consistency. ZooKeeper manages the active NameNode election process. Automatic failover with fencing prevents split-brain scenarios. Identical hardware is recommended, not strictly crucial. Replication factor is related to data redundancy not related to name node fail over."
  },
  {
    "id": 88,
    "question": "Which of the following configuration settings directly impacts the multi-tenancy capabilities of YARN in a\nCloudera CDP on-premises environment?",
    "options": {
      "A": "The ‘yarn.scheduler.capacity.maximum-am-resource-percent’ property.",
      "B": "The yarn.resourcemanager.ha.enabled' property.",
      "C": "The 'fs.defaultFS' property in core-site.xml.",
      "D": "The 'dfs.nameservices’ property in hdfs-site.xml.",
      "E": "The ‘yarn.nodemanager.resource.memory-mb' property"
    },
    "correctAnswers": [
      "A"
    ],
    "explanation": "‘yarn.scheduler.capacity.maximum-am-resource-percent' limits the percentage of cluster resources that can be used by application masters, preventing a single tenant from monopolizing resources. ‘yarn.resourcemanager.ha.enabled’ enables HA for the ResourceManager. 'fs.defaultFS and 'dfs.nameservices' configure the default filesystem. ‘yarn.nodemanager.resource.memory-mb' defines total available memory on NM"
  },
  {
    "id": 89,
    "question": "You have a multi-tenant Cloudera CDP cluster. One tenant reports slow query performance. Upon\ninvestigation, you find they are consuming a disproportionate amount of CPU resources. Which YARN\nqueue property can you adjust to limit the CPU usage for that specific tenant?",
    "options": {
      "A": "yarn.scheduler.fair.queue..weight’",
      "B": "yarn.scheduler.fair.queue..maxResourceS",
      "C": "yarn.scheduler.fair.queue..minResources'",
      "D": "yarn.scheduler.fair.queue..maxAMShare'",
      "E": "‘yarn.scheduler.capacity.queue..maximum-capacity’"
    },
    "correctAnswers": [
      "B"
    ],
    "explanation": "‘yarn.scheduler.fair.queue..maxResourceS lets you define the maximum resources (CPU and memory) a queue can use. While 'weight' influences share allocation, 'maxResources' sets a hard limit. ‘minResourceS guarantees a minimum allocation. ‘maxAMShare’ limits AM resource usage. ‘yarn.scheduler.capacity.queue..maximum-capacity’ applies to the CapacityScheduler"
  },
  {
    "id": 90,
    "question": "A Cloudera CDP on-premises cluster has several tenants sharing the same Hive Metastore. You want to\nensure that one tenant cannot access data belonging to another tenant through Hive. Which is the\nMOST effective security measure to achieve this?",
    "options": {
      "A": "Implement Hive authorization using SQL standards-based authorization (Storage Based Authorization) with Ranger.",
      "B": "Configure separate HiveServer2 instances for each tenant.",
      "C": "Create separate Kerberos principals for each tenant.",
      "D": "Enable Hive's built-in authorization (HiveServer2 authorization).",
      "E": "Isolate each tenant's data in separate HDFS clusters."
    },
    "correctAnswers": [
      "A"
    ],
    "explanation": "SQL standards-based authorization (Storage Based Authorization) with Ranger provides granular access control based on SQL privileges and policies defined in Ranger, allowing you to restrict access to specific databases, tables, or even columns. Separate HiveServer2 instances offer some isolation, but still rely on the same Metastore and underlying data. Kerberos principals handle authentication but not authorization within Hive. Hive's built-in authorization is less powerful than Ranger. Separate HDFS clusters increase complexity significantly."
  },
  {
    "id": 91,
    "question": "You are planning to migrate a Cloudera CDH 5 cluster to CDP on-premises, focusing on high availability.\nWhich component's HA configuration requires the most significant architectural change during the\nmigration?",
    "options": {
      "A": "HDFS NameNode",
      "B": "YARN ResourceManager",
      "C": "Hive Metastore",
      "D": "Oozie Server",
      "E": "Spark History Server"
    },
    "correctAnswers": [
      "B"
    ],
    "explanation": "While all components benefit from HA, the YARN ResourceManager HA configuration often undergoes the most significant architectural change because CDH 5 HA configuration is active/standby, while CDP uses a more distributed active/active ResourceManager HA using LevelDB as its state store that better utilizes cluster resources. CDH5 using Zookeeper and automatic failover, while CDP leverages improved LevelDB state management and active/active configuration. NameNode HA is generally similar in concept, though implementation details differ. Hive Metastore, Oozie, and Spark History Server HA are relatively straightforward migrations."
  },
  {
    "id": 92,
    "question": "Which of the following are valid fencing mechanisms used in HDFS NameNode High Availability (HA) to\nprevent split-brain scenarios?",
    "options": {
      "A": "STONITH (Shoot The Other Node In The Head)",
      "B": "Shared Storage Fencing",
      "C": "Resource Manager Fencing",
      "D": "Kafka Based Fencing",
      "E": "ZooKeeper Fencing"
    },
    "correctAnswers": [
      "A",
      "B"
    ],
    "explanation": "STONITH is a common fencing method that physically powers off or resets the failed NameNode. Shared Storage Fencing prevents the failed NameNode from writing to the shared edit log. Resource Manager Fencing, Kafka and Zookeeper fencing are not direct fencing mechanisms for HDFS NameNode HA."
  },
  {
    "id": 93,
    "question": "You have a Cloudera CDP on-premises cluster with multiple tenants. One tenant is running a Spark\napplication that is causing excessive disk I/O on the DataNodes, impacting other tenants. How can you\nmitigate this issue using YARN?",
    "options": {
      "A": "Configure YARN node labels and assign the tenant's application to a specific set of nodes with faster storage.",
      "B": "Increase the replication factor of the data used by the Spark application.",
      "C": "Implement a YARN queue with a lower priority for the tenant's application.",
      "D": "Enable data locality optimizations in Spark.",
      "E": "Throttle the number of executors used by the application through spark-defaults.conf"
    },
    "correctAnswers": [
      "A"
    ],
    "explanation": "YARN node labels allow you to tag specific nodes with characteristics (e.g., 'fast-storage') and then target applications to those nodes. Increasing replication impacts storage usage, not I/O. Lowering queue priority might delay execution, but not address the I/O issue directly. Data locality is a general optimization, not a specific mitigation for excessive I/O. Throttling the number of executors could mitigate somewhat the load but node label is the best option."
  },
  {
    "id": 94,
    "question": "A critical Impala daemon in your CDP cluster fails. You have configured Impala HA. Which process is\nresponsible for automatically restarting the failed Impala daemon?",
    "options": {
      "A": "Cloudera Manager Agent",
      "B": "ZooKeeper",
      "C": "Impala Statestore",
      "D": "YARN ResourceManager",
      "E": "Systemd"
    },
    "correctAnswers": [
      "A"
    ],
    "explanation": "The Cloudera Manager Agent monitors the health of the Impala daemons and is responsible for automatically restarting them in case of a failure, provided that auto-restart is enabled in the Cloudera Manager configuration."
  },
  {
    "id": 95,
    "question": "Your organization requires different tenants in a Cloudera CDP on-premises cluster to use different\nversions of Spark. How can you achieve this level of isolation and version control?",
    "options": {
      "A": "Deploy multiple Spark3 services, each configured with a specific version of Spark, and assign each tenant to a dedicated Spark3 service.",
      "B": "Use YARN's containerization features to isolate each tenant's Spark applications with specific versioned libraries.",
      "C": "Configure the ‘spark-defaults.conf' file with different Spark versions for each tenant's user account.",
      "D": "Utilize Kubernetes and Spark on Kubernetes to deploy separate Spark clusters for each tenant, each with its desired Spark version.",
      "E": "Use the ‘spark.version’ property to dynamically select the Spark version at runtime for each tenant's application."
    },
    "correctAnswers": [
      "D"
    ],
    "explanation": "Kubernetes and Spark on Kubernetes allows for complete isolation, including using different Spark versions. The Spark driver and executors run in isolated containers. Deploying Multiple Spark3 services not best way because the Spark3 service in CDP meant to provide spark functionality instead of isolating tenants, it is better to isolate by other means. YARN containers provide some isolation but not complete version isolation. User specific ‘spark-defaults.conf not possible. The ‘spark.version’ property doesn't exist."
  },
  {
    "id": 96,
    "question": "You observe frequent \"Out of Memory errors in your Cloudera CDP on-premises cluster, primarily\naffecting a specific tenant. This tenant is running complex Hive queries. Which of the following actions\ncan you take to mitigate these errors without impacting other tenants?",
    "options": {
      "A": "Adjust the 'hive.tez.container.size' property for the specific YARN queue assigned to the tenant.",
      "B": "Increase the 'hive.auto.convert.join.noconditionaltask.size’ property globally in Hive's configuration.",
      "C": "Implement resource quotas in Ranger to limit the tenant's overall resource consumption.",
      "D": "Force all queries to use MapReduce as the execution engine.",
      "E": "Adjust the global ‘yarn.nodemanager.resource.memory-mb' property"
    },
    "correctAnswers": [
      "A"
    ],
    "explanation": "Adjusting 'hive.tez.container.size’ for the tenant's YARN queue allows you to control the memory allocated to Tez containers, preventing OOM errors for that tenant without impacting others. Increasing 'hive.auto.convert.join.noconditionaltask.size' could lead to increased memory consumption. Resource quotas in Ranger primarily control access, not memory allocation within a job. Forcing MapReduce is usually less efficient. ‘yarn.nodemanager.resource.memory-mb' affects all NM containers."
  },
  {
    "id": 97,
    "question": "You are configuring HA for the Hive Metastore in your Cloudera CDP on-premises cluster. You choose to\nuse a highly available database as the backend. Which of the following is the MOST important factor to\nconsider when selecting the database?",
    "options": {
      "A": "The database's ability to handle a large number of concurrent connections.",
      "B": "The database's compatibility with the Hive Metastore schema.",
      "C": "The database's storage capacity.",
      "D": "The database's backup and recovery capabilities.",
      "E": "The database's geographical proximity to the Hadoop cluster."
    },
    "correctAnswers": [
      "A"
    ],
    "explanation": "The Hive Metastore needs to serve a large number of concurrent requests from HiveServer2 instances, Impala, and other services. Therefore, the database's ability to handle concurrent connections is the most critical factor. While compatibility is necessary, most common databases are compatible. Storage capacity is usually not a major concern for the Metastore. Backup and recovery are important, but secondary to concurrency. Geographical proximity can help with latency, but is less important than concurrency."
  },
  {
    "id": 98,
    "question": "During a CDP Private Cloud Base installation using Cloudera Manager, which of the following statements\nregarding the use of custom parcel repositories is MOST accurate?",
    "options": {
      "A": "Custom parcel repositories are automatically detected by Cloudera Manager without any configuration.",
      "B": "You must explicitly add the URL of each custom parcel repository in Cloudera Manager's settings before Cloudera Manager can use them.",
      "C": "Custom parcel repositories can only be used if they are accessible via HTTPS with a valid SSL certificate.",
      "D": "Cloudera Manager only supports a single custom parcel repository.",
      "E": "Custom parcel repositories can be used for any service in CDP, including services not provided by Cloudera."
    },
    "correctAnswers": [
      "B"
    ],
    "explanation": "Cloudera Manager requires explicit configuration for custom parcel repositories. You need to add the URL in Cloudera Manager Admin Console to allow it to use them."
  },
  {
    "id": 99,
    "question": "You are tasked with automating the installation of CDP Private Cloud Base using the Cloudera Manager\nAPI. Which of the following API endpoints would be used to trigger the installation of a service?",
    "options": {
      "A": "/api/v31/clusters/{clusterName}/services/{serviceName}/commands/start",
      "B": "/api/v31/clusters/{clusterName}/commands/deployClientConfig",
      "C": "/api/v31/clusters/{clusterName}/services/{serviceName}/commands/install",
      "D": "/api/v31/clusters/{clusterName}/commands/installServices",
      "E": "/api/v31/clusters/{clusterName}/services/{serviceName}/commands/deployClientConfig"
    },
    "correctAnswers": [
      "C"
    ],
    "explanation": "The endpoint is used to trigger the installation of a specific service on a cluster through the Cloudera Manager API."
  },
  {
    "id": 100,
    "question": "During a Cloudera Manager installation, you encounter an error indicating that the database schema\nversion is incompatible. Which of the following steps are MOST likely to resolve this issue? (Select TWO)",
    "options": {
      "A": "Upgrade Cloudera Manager to the latest version.",
      "B": "Drop and recreate the Cloudera Manager database schema.",
      "C": "Downgrade the database server to an older version.",
      "D": "Run the Cloudera Manager schema upgrade tool.",
      "E": "Reinstall the operating system on all nodes."
    },
    "correctAnswers": [
      "A",
      "D"
    ],
    "explanation": "An incompatible database schema version typically indicates a need to upgrade Cloudera Manager to a version compatible with the existing schema or to use the schema upgrade tool provided by Cloudera Manager to update the schema to the required version. Dropping and recreating the schema is generally a last resort and should be done with extreme caution."
  },
  {
    "id": 101,
    "question": "After installing Cloudera Manager, you are unable to connect to the Cloudera Manager Admin Console\nvia your web browser. Which of the following could be the MOST likely cause?",
    "options": {
      "A": "The Cloudera Manager Server process is not running.",
      "B": "The Cloudera Manager Agent is not installed on the Cloudera Manager Server host.",
      "C": "The Cloudera Manager license has expired.",
      "D": "The web browser cache needs to be cleared.",
      "E": "The Cloudera Manager Server is configured to listen on the wrong port."
    },
    "correctAnswers": [
      "A"
    ],
    "explanation": "If you cannot connect to the Cloudera Manager Admin Console, the most likely cause is that the Cloudera Manager Server process is not running. You can check the status of the process using systemctl or a similar command."
  },
  {
    "id": 102,
    "question": "You are configuring a high-availability Cloudera Manager deployment. Which of the following databases\nare supported for the Cloudera Manager Server's metadata?",
    "options": {
      "A": "MySQL/MariaDB, PostgreSQL, Oracle",
      "B": "MongoDB, Cassandra, HBase",
      "C": "SQLite, Derby, HSQLDB",
      "D": "Redis, Memcached, etcd",
      "E": "Only Oracle is supported for HA."
    },
    "correctAnswers": [
      "A"
    ],
    "explanation": "Cloudera Manager supports MySQL/MariaDB, PostgreSQL, and Oracle as databases for its metadata. These databases are suitable for high-availability deployments due to their robustness and support for replication."
  },
  {
    "id": 103,
    "question": "You are deploying CDP Private Cloud Base and need to configure a specific Java Development Kit (JDK)\nfor Cloudera Manager. Where would you typically specify the JDK path during installation?",
    "options": {
      "A": "In the ‘cm-admin-env.sh' file on the Cloudera Manager Server host.",
      "B": "In the ‘cloudera-scm-agent.inr file on each host.",
      "C": "Through the Cloudera Manager Admin Console after installation.",
      "D": "By setting the environment variable in the system-wide environment configuration.",
      "E": "In the /etc/cloudera-scm-server/config.ini file"
    },
    "correctAnswers": [
      "A"
    ],
    "explanation": "The ‘cm-admin-env.sm file on the Cloudera Manager Server host is the typical location to specify the JDK path for Cloudera Manager. This allows you to control the specific JDK version used by Cloudera Manager."
  },
  {
    "id": 104,
    "question": "You are using Cloudera Manager to deploy a new CDP Private Cloud Base cluster. The installation fails\nwith an error related to insufficient swap space on one of the hosts. How would you MOST effectively\naddress this issue?",
    "options": {
      "A": "Disable swap on all hosts in the cluster.",
      "B": "Increase the amount of physical RAM on the host.",
      "C": "Increase the swap space on the host by modifying the swap file size or adding a swap partition.",
      "D": "Ignore the warning and proceed with the installation.",
      "E": "Decrease the amount of physical RAM on other hosts to compensate."
    },
    "correctAnswers": [
      "C"
    ],
    "explanation": "The most effective way to address insufficient swap space is to increase the swap space on the affected host. This can be done by modifying the swap file size or adding a swap partition. While increasing RAM is beneficial, it doesn't directly address the swap space requirement."
  },
  {
    "id": 105,
    "question": "Which of the following is the MOST accurate description of the role of the Cloudera Manager Agent?",
    "options": {
      "A": "It is responsible for managing the Cloudera Manager Server.",
      "B": "It monitors the health of the operating system on each host.",
      "C": "It executes commands and monitors the services on each host in the cluster, reporting back to the Cloudera Manager Server.",
      "D": "It provides a web interface for managing the cluster.",
      "E": "It only installs packages; it does not monitor services."
    },
    "correctAnswers": [
      "C"
    ],
    "explanation": "The Cloudera Manager Agent is responsible for executing commands and monitoring services on each host in the cluster. It reports the status of these services back to the Cloudera Manager Server, allowing the server to manage and monitor the entire cluster."
  },
  {
    "id": 106,
    "question": "During the host inspection phase of a CDP installation using Cloudera Manager, you receive an error\nmessage indicating that the clocks on the hosts are not synchronized. Which service should you\nconfigure on all cluster nodes to resolve this issue and ensure that clocks are synchronized?",
    "options": {
      "A": "DNS",
      "B": "NTP",
      "C": "LDAP",
      "D": "Kerberos",
      "E": "Syslog"
    },
    "correctAnswers": [
      "B"
    ],
    "explanation": "NTP (Network Time Protocol) is the standard protocol for synchronizing the clocks of computers over a network. Configuring NTP on all cluster nodes ensures that their clocks are synchronized, resolving the clock synchronization error during the host inspection phase."
  },
  {
    "id": 107,
    "question": "You are installing CDP Private Cloud Base on a cluster with limited internet access. Which of the\nfollowing strategies would be the MOST effective for managing the installation and deployment of\nsoftware packages?",
    "options": {
      "A": "Configure each host to use a public proxy server.",
      "B": "Download all necessary parcels and packages to a local repository and configure Cloudera Manager to use this repository.",
      "C": "Manually install all software packages on each host.",
      "D": "Use a satellite server for patching.",
      "E": "Disable all network services on the cluster."
    },
    "correctAnswers": [
      "B"
    ],
    "explanation": "Downloading all necessary parcels and packages to a local repository and configuring Cloudera Manager to use this repository is the most effective strategy for managing software installations in an environment with limited internet access. This approach allows you to control the packages installed and avoid relying on a constant internet connection."
  },
  {
    "id": 108,
    "question": "After installing CDP, you find that certain services are failing to start with errors related to Kerberos\nauthentication. Which of the following steps are necessary to verify Kerberos configuration issues during\nCDP installation and startup? (Select TWO)",
    "options": {
      "A": "Verify that the KDC (Key Distribution Center) is reachable and running.",
      "B": "Ensure that all hosts in the cluster have synchronized clocks using NTP.",
      "C": "Reformat all disks in the cluster.",
      "D": "Disable SELinux.",
      "E": "Check the service logs for Kerberos-related error messages."
    },
    "correctAnswers": [
      "A",
      "E"
    ],
    "explanation": "Verifying that the KDC is reachable and running is crucial for Kerberos authenticatiom Additionally, checking the service logs for Kerberos-related error messages provides valuable insights into the specific authentication failures. While clock synchronization (B) is important for Kerberos to work, the question is asking about verifying the config issues not how to prevent them initially."
  },
  {
    "id": 109,
    "question": "You are automating CDP Private Cloud Base cluster creation through Cloudera Manager API. You want to\nensure that specific host templates are applied to different groups of hosts during installation. What API\ncall and parameter is primarily used to map hosts to host templates?",
    "options": {
      "A": "/api/v31/clusters/{clusterName}/hosts, use the 'rackld' parameter to assign templates.",
      "B": "/api/v31/clusters/{clusterName}/hosts/{hostld}, use the 'hostTemplateName' parameter to assign templates.",
      "C": "/api/v31/clusters/{clusterName}/hostTemplates, create separate host templates for each rack.",
      "D": "/api/v31/clusters/{clusterName}/hosts, after installation templates are not assigned to hosts.",
      "E": "Host templates can only be assigned through the IJI; API does not support them."
    },
    "correctAnswers": [
      "B"
    ],
    "explanation": "The endpoint allows you to modify individual host configurations. The 'hostTemplateName’ parameter enables you to associate a specific host with a pre-defined host template, allowing for customized configurations based on host roles or hardware specifications."
  },
  {
    "id": 110,
    "question": "You are deploying Cloudera CDP on-premises using Cloudera Manager. During the installation process,\nyou encounter the following error: 'ERROR: Could not find a suitable JDK on host You have verified that a\nsupported JDK is installed. Which of the following steps is MOST likely to resolve this issue?",
    "options": {
      "A": "Ensure the 'JAVA HOME' environment variable is correctly set on the host and accessible by the Cloudera Manager Agent.",
      "B": "Restart the Cloudera Manager Server and Agent.",
      "C": "Manually install the JDK using the Cloudera Manager package repository.",
      "D": "Disable the Cloudera Manager Agent firewall.",
      "E": "Re-run the Cloudera Manager Server installation process from the beginning."
    },
    "correctAnswers": [
      "A"
    ],
    "explanation": "The most common reason for this error is an incorrectly configured 'JAVA HOME' environment variable. Cloudera Manager Agents rely on this variable to locate the JDK. Restarting the services might help but directly setting JAVA_HOME is more reliable. Manual installation via Cloudera Manager is a valid option, but checking JAVA HOME is the quicker first step."
  },
  {
    "id": 111,
    "question": "Which of the following statements are TRUE regarding the use of the Cloudera Manager Agent\nheartbeat mechanism during the installation of core services in a CDP on-premises cluster?",
    "options": {
      "A": "The Cloudera Manager Agent heartbeat is used to monitor the health and status of the host.",
      "B": "The Cloudera Manager Server uses the heartbeat to detect the presence of the Agent on a given host.",
      "C": "If the heartbeat is lost, the Cloudera Manager Server immediately uninstalls all services on the host.",
      "D": "The heartbeat interval is configurable via the Cloudera Manager I-Jl.",
      "E": "Heartbeat information is stored in ZooKeeper."
    },
    "correctAnswers": [
      "A",
      "B",
      "D"
    ],
    "explanation": "The Cloudera Manager Agent heartbeat serves to inform the Cloudera Manager Server of the host's status and agent availability. It is crucial for monitoring and management. A lost heartbeat doesn't immediately trigger service uninstallation; rather, it flags the host as unhealthy and allows for intervention. The interval can indeed be configured. Heartbeat information is primarily stored and managed within the Cloudera Manager Server, not directly in ZooKeeper, although ZooKeeper might be used for related coordination tasks."
  },
  {
    "id": 112,
    "question": "You are configuring the Kerberos security settings for your CDP on-premises cluster using Cloudera\nManager. Which of the following steps is MANDATORY before enabling Kerberos?",
    "options": {
      "A": "Install and configure a Key Distribution Center (KDC) that is accessible from all nodes in the cluster.",
      "B": "Configure Cloudera Manager to use LDAP for authentication.",
      "C": "Enable TLS/SSL encryption for all services in the cluster.",
      "D": "Back up all data stored in HDFS.",
      "E": "Configure the cluster to use a highly available NameNode."
    },
    "correctAnswers": [
      "A"
    ],
    "explanation": "A KDC is essential for Kerberos to function. It handles authentication and authorization by issuing and managing Kerberos tickets. The other options are good practices, but the KDC is absolutely required for Kerberos to work."
  },
  {
    "id": 113,
    "question": "After deploying a new CDP on-premises cluster, you observe that the NameNode is consistently failing\nto start. Upon inspecting the NameNode logs, you find the following error message: 'java.io.lOException:\nIncompatible namespacelD'. What is the MOST likely cause of this issue?",
    "options": {
      "A": "The NameNode's metadata directory contains an older version of the namespace image.",
      "B": "The NameNode is configured to use an incorrect port number.",
      "C": "The NameNode does not have sufficient memory allocated.",
      "D": "The NameNode's data directories are corrupted.",
      "E": "The NameNode is unable to connect to the ZooKeeper quorum."
    },
    "correctAnswers": [
      "A"
    ],
    "explanation": "An incompatible namespacelD indicates that the NameNode is trying to start with a namespace image (metadata) that is either from a different cluster or an older/incompatible version. This often happens after a failed upgrade or a misconfiguration during initial setup."
  },
  {
    "id": 114,
    "question": "You are tasked with automating the installation of CDP core services on-premises using a script. Which\nof the following methods is the MOST recommended way to interact with Cloudera Manager for\nautomated deployment?",
    "options": {
      "A": "Directly modifying the Cloudera Manager database.",
      "B": "Using the Cloudera Manager API.",
      "C": "Manually editing the Cloudera Manager configuration files.",
      "D": "Using SSH to execute commands on the Cloudera Manager Server.",
      "E": "Executing SQL queries against the Hive Metastore directly."
    },
    "correctAnswers": [
      "B"
    ],
    "explanation": "The Cloudera Manager API provides a stable and supported interface for interacting with Cloudera Manager programmatically. Modifying the database or configuration files directly is highly discouraged as it can lead to instability and is not supported."
  },
  {
    "id": 115,
    "question": "During the initial deployment of a CDP on-premises cluster, you are configuring the database settings for\nthe Hive Metastore. Which database types are officially supported by Cloudera for production\nenvironments?",
    "options": {
      "A": "Derby",
      "B": "MySQL",
      "C": "PostgreSQL",
      "D": "Oracle",
      "E": "SQLite"
    },
    "correctAnswers": [
      "B",
      "C",
      "D"
    ],
    "explanation": "Derby is only suitable for development or testing, not for production. MySQL, PostgreSQL, and Oracle are the supported database types for the Hive Metastore in a production environment. SQLite is not supported."
  },
  {
    "id": 116,
    "question": "You are installing the Data Hub cluster in CDP On-Premises, and the installation fails with the error\n\"Insufficient Disk Space\". You have checked and confirmed that each node has enough disk space. What\ncould be the reason for the error?",
    "options": {
      "A": "The root partition on one or more nodes is full.",
      "B": "The '/tmp’ directory on one or more nodes is full.",
      "C": "The user running the Cloudera Manager Agent does not have write permissions to the installation directory.",
      "D": "The HDFS data directories are not properly configured.",
      "E": "The Cloudera Manager Server's disk is full."
    },
    "correctAnswers": [
      "B"
    ],
    "explanation": "During installation, the '/tmp’ directory is used for temporary files. Even if other partitions have sufficient space, a full '/tmp’ can cause installation failures. Checking and cleaning 7tmp’ is a common troubleshooting step. Although other issues can cause failures, '/tmp’ filling up is a prevalent culprit in this scenario."
  },
  {
    "id": 117,
    "question": "You're setting up Ranger KMS for encryption at rest. You have configured the connection to Vault, but\nRanger KMS fails to start. The logs show a 'javax.net.ssl.SSLHandshakeException'. Which of the following\nis the MOST likely cause?",
    "options": {
      "A": "The Vault server is not running.",
      "B": "Ranger KMS is not properly configured with the Vault token.",
      "C": "The Vault certificate is not trusted by the Ranger KMS server.",
      "D": "The Ranger KMS database is corrupted.",
      "E": "Firewall rules are blocking communication between Ranger KMS and Vault."
    },
    "correctAnswers": [
      "C"
    ],
    "explanation": "An SSLHandshakeException generally indicates a problem with the SSL/TLS handshake. In this case, the most likely cause is that Ranger KMS does not trust the certificate presented by Vault. This can happen if Vault is using a self-signed certificate or a certificate signed by a CA that Ranger KMS doesn't recognize. While the other issues might cause problems, the SSLHandshakeException points directly to a certificate trust issue."
  },
  {
    "id": 118,
    "question": "During the Cloudera Manager Agent installation, you encounter the following error: 'Connection\nrefused'. Assuming the Cloudera Manager Server is running and reachable via ping, what are the\npossible root causes of this issue?",
    "options": {
      "A": "The Cloudera Manager Agent is not running on the host.",
      "B": "A firewall is blocking communication on the Cloudera Manager Agent port (default 7182).",
      "C": "The Cloudera Manager Agent is configured to use an incorrect hostname or IP address for the Cloudera Manager Server.",
      "D": "The hostname resolution is not working correctly.",
      "E": "SELinux is preventing the agent from connecting."
    },
    "correctAnswers": [
      "B",
      "C",
      "D",
      "E"
    ],
    "explanation": "A 'Connection refused' error means that the connection attempt was actively refused by the target host. This can be caused by a firewall blocking the connection, the agent being configured to connect to the wrong address, hostname resolution issues preventing the agent from finding the server, or SELinux blocking the connection. The Agent not running wouldn't lead to 'connection refused' but 'connection timeout'."
  },
  {
    "id": 119,
    "question": "You are installing core services in a CDP cluster and need to ensure high availability for the NameNode.\nWhich of the following approaches is the MOST effective way to achieve NameNode HA using Cloudera\nManager?",
    "options": {
      "A": "Configure a single NameNode with a large amount of memory and CPU resources.",
      "B": "Deploy two NameNodes in an Active/Passive configuration using a shared edit log storage.",
      "C": "Manually configure multiple NameNodes and synchronize their metadata using rsync.",
      "D": "Configure a single NameNode with automatic failover to a backup server.",
      "E": "Deploy multiple NameNodes in an Active/Active configuration with distributed metadata management."
    },
    "correctAnswers": [
      "B"
    ],
    "explanation": "The standard and recommended approach for NameNode HA in CDP is to use two NameNodes in an Active/Passive configuration. The shared edit log storage (e.g., using a Quorum Journal Manager - QJM) ensures that the passive NameNode can quickly take over if the active NameNode fails. While other solutions exist, this is the most supported and easiest to manage within Cloudera Manager."
  },
  {
    "id": 120,
    "question": "While configuring TLS encryption for Kafka brokers during CDP installation, you encounter the following\nerror: ‘java.security.cert.CertificateException: No subject alternative names present’. What steps can\nyou take to troubleshoot and resolve this issue?",
    "options": {
      "A": "Disable TLS encryption for the Kafka brokers.",
      "B": "Regenerate the Kafka broker certificates with the correct Subject Alternative Names (SANs) including the broker's hostname and IP address.",
      "C": "Update the Kafka broker's 'server.properties’ file to ignore the certificate validation.",
      "D": "Import the Kafka broker's certificate into the Java keystore on the Cloudera Manager Server.",
      "E": "Ensure all broker hostnames resolve to the same IP address."
    },
    "correctAnswers": [
      "B"
    ],
    "explanation": "The error 'No subject alternative names present' means that the Kafka client (or another broker) is trying to connect to a Kafka broker using a hostname or IP address that is not listed in the Subject Alternative Name (SAN) extension of the broker's certificate. The correct solution is to regenerate the certificate with the appropriate SANs. While importing the certificate might seem like a solution, it just masks the problem instead of fixing it at the root."
  },
  {
    "id": 121,
    "question": "You are upgrading your CDP on-premises cluster from CDH 6.x to CDP 7.x. After the upgrade, you notice\nthat some of your MapReduce jobs are failing with ClassNotFoundException. The missing classes are\nrelated to custom dependencies included in your old CDH environment. Which of the following is the\nMOST effective approach to resolve this issue in CDP?",
    "options": {
      "A": "Copy the missing JAR files directly into the [lib directory of the Hadoop installation on each node.",
      "B": "Modify the Hadoop classpath using the 'hadoop-env.sm script to include the location of the missing JAR files.",
      "C": "Package the custom dependencies into a shaded JAR and include it with your MapReduce job.",
      "D": "Recompile the MapReduce jobs against the CDP 7.x Hadoop libraries and redeploy them.",
      "E": "Use Cloudera Manager to deploy the missing JARs to all nodes in the cluster."
    },
    "correctAnswers": [
      "C",
      "D"
    ],
    "explanation": "Copying JARs directly or modifying ‘hadoop-env.sh' is generally discouraged as it can lead to dependency conflicts and makes the cluster harder to manage. The best approaches are to either package the dependencies into a shaded JAR to isolate them from the system classpath (C) or to recompile the jobs against the CDP 7 .x libraries to ensure compatibility (D). In some scenarios, recompilation (D) might be necessary, but using a shaded JAR (C) provides better isolation. Using Cloudera Manager for deployment is not directly supported in this fashion."
  },
  {
    "id": 122,
    "question": "You're deploying a CDP on-premises cluster using parcels. After distributing the parcels to the cluster\nnodes, you notice that the activation process consistently fails. Which of the following is the MOST likely\ncause?",
    "options": {
      "A": "The Cloudera Manager Agent user lacks sufficient permissions to write to the parcel staging directory.",
      "B": "The parcel's checksum does not match the checksum stored in the Cloudera Manager database.",
      "C": "The /opt/cloudera/parcel directory is not created on all cluster nodes.",
      "D": "The Cloudera Manager Server's database is corrupted.",
      "E": "Insufficient disk space on the nodes in /opt/cloudera/parcel."
    },
    "correctAnswers": [
      "A",
      "B",
      "E"
    ],
    "explanation": "Parcel activation requires proper permissions for the Cloudera Manager Agent user to unpack the parcel. A mismatching checksum indicates data corruption during transfer or storage. Ensure you have enough disk space on each node."
  },
  {
    "id": 123,
    "question": "You are upgrading your CDP on-premises cluster using parcels. During the upgrade process, one of the\nservices fails to start after the activation of the new parcel. Examining the service logs, you find errors\nrelated to missing libraries. What's the most probable reason?",
    "options": {
      "A": "The new parcel version has a dependency on a library not present on the system.",
      "B": "The old parcel version was not properly deactivated before activating the new parcel.",
      "C": "The Cloudera Manager Server was not restarted after the parcel activation.",
      "D": "The service's configuration files were not updated to reflect the new parcel location.",
      "E": "Incorrect user permissions on the service's binaries within the new parcel."
    },
    "correctAnswers": [
      "A"
    ],
    "explanation": "New parcel versions can introduce dependencies on libraries not present in the base OS or previous parcel versions. This is a common cause of service startup failures after upgrades."
  },
  {
    "id": 124,
    "question": "You are configuring a local parcel repository for your CDP on-premises cluster After configuring the\nrepository URL in Cloudera Manager, the parcels do not appear. What is the first step to troubleshoot\nthis?",
    "options": {
      "A": "Restart the Cloudera Manager Server.",
      "B": "Verify that the Cloudera Manager Agent user has read permissions on the parcel files in the local repository.",
      "C": "Check the Cloudera Manager Server logs for errors related to accessing the parcel repository.",
      "D": "Run the ‘sudo yum update' command on all cluster nodes.",
      "E": "Verify that a .sha file exists for each .parcel file in the repository and that the SHA matches."
    },
    "correctAnswers": [
      "C",
      "E"
    ],
    "explanation": "Checking the Cloudera Manager Server logs provides immediate insight into any connectivity or permission issues accessing the repository. Also, verification of .sha files are important for parcel validation."
  },
  {
    "id": 125,
    "question": "When installing a parcel, Cloudera Manager reports a 'checksum mismatch' error. What steps should\nyou take to resolve this issue?",
    "options": {
      "A": "Download the parcel file again from the official Cloudera repository.",
      "B": "Disable checksum verification in Cloudera Manager (not recommended).",
      "C": "Verify the SHA checksum of the downloaded parcel against the checksum provided by Cloudera.",
      "D": "Restart the Cloudera Manager Server.",
      "E": "Delete the incomplete parcel from lopt/cloudera/parcel and retry."
    },
    "correctAnswers": [
      "A",
      "C",
      "E"
    ],
    "explanation": "The first step is to re-download the parcel, as it's likely corrupted during the initial download. Then, verify the downloaded parcel's checksum against the official checksum to confirm its integrity."
  },
  {
    "id": 126,
    "question": "You have activated a new parcel on your CDP cluster. However, one of the services continues to use the\nbinaries from the older parcel.\nWhat's the most likely reason, assuming service configuration is correct?",
    "options": {
      "A": "The service was not restarted after the parcel activation.",
      "B": "The /var/run directory is pointing to older version files.",
      "C": "The service's environment variables are still pointing to the old parcel directory.",
      "D": "The service is using a cached version of the binaries.",
      "E": "The active parcel symlink in /opt/cloudera/parcels is not updated correctly."
    },
    "correctAnswers": [
      "A",
      "C",
      "E"
    ],
    "explanation": "Services need to be restarted to use the new binaries. Also environment variable settings and broken symlinks are likely root causes of the services using the older version files."
  },
  {
    "id": 127,
    "question": "You are deploying a custom service on CDP on-premises using a parcel. The service requires specific\nsystem-level libraries. How should you ensure these libraries are available on all cluster nodes?",
    "options": {
      "A": "Include the required libraries directly within the custom service parcel.",
      "B": "Install the libraries using the system package manager (e.g., yum, apt) on each node before distributing the parcel.",
      "C": "Create a separate parcel containing the required libraries and ensure it's activated before the custom service parcel.",
      "D": "Configure Cloudera Manager to automatically install the libraries during parcel distribution.",
      "E": "Update /etc/ld.so.conf to include the path to custom libraries."
    },
    "correctAnswers": [
      "B",
      "C"
    ],
    "explanation": "Distributing OS level packages and the use of dependencies parcels are recommended options."
  },
  {
    "id": 128,
    "question": "You are tasked with upgrading a specific service within your CDP cluster to a newer version provided as\na parcel. However, this upgrade should only affect a subset of the cluster nodes. How can you achieve\nthis?",
    "options": {
      "A": "It is not possible to target parcel activation to specific nodes. Parcels are always activated cluster- wide.",
      "B": "Manually distribute the parcel to the targeted nodes and activate it through the command line.",
      "C": "Use Cloudera Manager's host template feature to apply different parcel configurations to different node groups.",
      "D": "Modify the Cloudera Manager Agent configuration on the targeted nodes to point to a different parcel repository.",
      "E": "Use Cloudera Manager API to script custom parcel deployments to target hosts."
    },
    "correctAnswers": [
      "C",
      "E"
    ],
    "explanation": "Host Templates and CM API's allow for precise application of configurations and parcels to specific nodes."
  },
  {
    "id": 129,
    "question": "After activating a new parcel, you notice increased CPU utilization across the cluster. You suspect the\nnew service version is causing this. How can you quickly revert to the previous parcel version?",
    "options": {
      "A": "Deactivate the new parcel and activate the previous parcel version through Cloudera Manager.",
      "B": "Manually replace the binaries in /opt/cloudera/parcels with the older versions.",
      "C": "Restore the Cloudera Manager database to a backup taken before the parcel activation.",
      "D": "Reinstall the older service version using the system package manager.",
      "E": "Rollback option from Cloudera Manager IJI."
    },
    "correctAnswers": [
      "A",
      "E"
    ],
    "explanation": "The fastest way to revert is to deactivate the new parcel and activate the previous version through Cloudera Manager and rolling back . This ensures that all services are using the older binaries."
  },
  {
    "id": 130,
    "question": "Your CDP cluster has multiple services installed via parcels. You need to determine the exact parcel\nversion used by a specific service instance. How can you achieve this?",
    "options": {
      "A": "Check the service's process ID (PID) file for the parcel version.",
      "B": "Examine the service's configuration files for references to the parcel directory.",
      "C": "Use the Cloudera Manager API to query the service instance for its parcel version.",
      "D": "Check the service logs for startup messages indicating the parcel version.",
      "E": "Login to cloudera manager and navigate to specific service and then you can determine parcel version under configuration tab."
    },
    "correctAnswers": [
      "C",
      "D",
      "E"
    ],
    "explanation": "Cloudera Manager API and the service logs often contain information about the parcel version being used. Also UI can show such details."
  },
  {
    "id": 131,
    "question": "You have downloaded a parcel and its corresponding ' .sha' file. However, when attempting to verify the\nchecksum using 'sha256sum' , the output doesn't match the value in the .sha' file, but you are sure you\nhave downloaded the correct file. What might be causing this discrepancy?",
    "options": {
      "A": "The .sha’ file is corrupted.",
      "B": "The encoding of the .sha’ file is incorrect (e.g., UTF-16 instead of UTF-8).",
      "C": "You are using the wrong hashing algorithm (e.g., 'md5sum’ instead of ‘sha256sum').",
      "D": "There's a hidden character (like a newline) at the end of the checksum in the .sha’ file.",
      "E": "Permissions are incorrect on the parcel file."
    },
    "correctAnswers": [
      "B",
      "C",
      "D"
    ],
    "explanation": "Incorrect encoding or the presence of hidden characters in the .sha’ file can alter the calculated checksum. Also, using the incorrect hash algorithm will obviously lead to a mismatch."
  },
  {
    "id": 132,
    "question": "You are using a custom parcel repository served over HTTPS. However, Cloudera Manager is unable to\ndownload parcels from the repository, throwing SSL errors. What steps should you take to resolve this\nissue?",
    "options": {
      "A": "Disable SSL verification in Cloudera Manager (highly discouraged).",
      "B": "Import the SSL certificate of the HTTPS server into the Java truststore used by the Cloudera Manager Server.",
      "C": "Ensure that the HTTPS server is using a valid SSL certificate signed by a trusted Certificate Authority (CA).",
      "D": "Configure the Cloudera Manager Agent to use HTTP instead of HTTPS for accessing the repository.",
      "E": "Ensure the hostname used in the URL exactly matches the CN/SAN in the SSL certificate."
    },
    "correctAnswers": [
      "B",
      "C",
      "E"
    ],
    "explanation": "Importing the certificate into the truststore allows the Java-based Cloudera Manager Server to trust the HTTPS server. Also validating that the certificate is issued by CA and hostname validations are important to resolve SSL issues."
  },
  {
    "id": 133,
    "question": "Assume you are facing issues related to Parcel distribution. The below Python script is intended to\ndownload parcel files. Find out which of the below code Snippet will resolve the problem if CM API call is\nfailing due to invalid credentials, connection refused errors or invalid URL etc. Assume other code is\ncorrect. In other words provide code to handle exceptions",
    "options": {
      "A": "B.",
      "C": "D.",
      "E": "Answer: A,E"
    },
    "correctAnswers": [
      "A",
      "E"
    ],
    "explanation": "Snippet A, E incorporates robust error handling using a ‘try...except’ block to catch ‘requests.exceptions.RequestException’ , which covers various potential issues, including connection errors, invalid URLs, and HTTP errors. Additionally, using 'response.raise_for_status()' is to make sure that the URL exists, if not its thrown exception."
  },
  {
    "id": 134,
    "question": "You are deploying a CDP Private Cloud Base cluster with Kerberos enabled. After the initial deployment,\nyou notice that some Hadoop services are failing to start, with errors related to keytab files. What are\nthe MOST likely causes of this issue? (Select TWO)",
    "options": {
      "A": "The keytab files were not properly distributed to all nodes in the cluster.",
      "B": "The Kerberos principal used for the service does not exist in the KDC.",
      "C": "The clocks on the Hadoop nodes are not synchronized with the KDC server.",
      "D": "Incorrect permissions are set on the keytab files, preventing the service from reading them.",
      "E": "The Java Cryptography Extension (JCE) Unlimited Strength Jurisdiction Policy Files are not installed."
    },
    "correctAnswers": [
      "A",
      "D"
    ],
    "explanation": "Keytab files are essential for Kerberos authentication. If they are not distributed correctly, or the service doesn't have the correct permissions to read them, services will fail to start. Clock skew is also a common Kerberos issue, but is not directly related to keytab distribution. While JCE can be a cause of other Kerberos issues, its is not the first cause to investigate in this case. Ensuring that Kerberos principals exist in the KDC is part of pre-deployment setup."
  },
  {
    "id": 135,
    "question": "During the installation of CDP Private Cloud Base, you encounter an error in Cloudera Manager related\nto network connectivity between the Cloudera Manager Server and the hosts. Specifically, you receive a\nmessage indicating that the 'Ping Test' failed. What is the MOST likely root cause of this issue?",
    "options": {
      "A": "The Cloudera Manager Agent is not installed on the target hosts.",
      "B": "Firewall rules are blocking ICMP traffic between the Cloudera Manager Server and the hosts.",
      "C": "The /etc/hosts file is not correctly configured on either the Cloudera Manager Server or the hosts.",
      "D": "There is a DNS resolution problem, preventing the Cloudera Manager Server from resolving the hostnames of the target hosts.",
      "E": "Insufficient memory is allocated to the Cloudera Manager Server."
    },
    "correctAnswers": [
      "B"
    ],
    "explanation": "The 'Ping Test' in Cloudera Manager specifically checks for basic network connectivity using ICMP. A firewall blocking ICMP is the most direct and common reason for this test to fail. While DNS resolution issues and incorrect /etc/hosts entries can cause connectivity problems, the 'Ping Test' is primarily reliant on ICMP, and firewall settings are more likely cause a Ping test failure directly."
  },
  {
    "id": 136,
    "question": "You are deploying a new CDP Private Cloud Base cluster and want to automate the host configuration\nprocess. Which of the following methods is the MOST efficient and recommended approach?",
    "options": {
      "A": "Manually configure each host through the Cloudera Manager Admin Console.",
      "B": "Use a custom shell script to install the Cloudera Manager Agent and perform initial configurations on each host.",
      "C": "Leverage automated deployment tools such as Ansible, Puppet, or Chef, in conjunction with Cloudera Manager's API.",
      "D": "Create a custom RPM package containing the Cloudera Manager Agent and manually install it on each host.",
      "E": "Use Cloudera Altus Director to provision and configure the cluster."
    },
    "correctAnswers": [
      "C"
    ],
    "explanation": "Automated deployment tools like Ansible, Puppet, and Chef are designed for infrastructure as code. Using these tools in conjunction with the Cloudera Manager API allows for consistent and repeatable deployments, reducing manual errors and saving time. Cloudera Altus Director has been replaced with newer technologies like CDP and Cloudera Manager's API, so is not correct."
  },
  {
    "id": 137,
    "question": "You have deployed a CDP Private Cloud Base cluster and are using Ranger for centralized security. You\nneed to configure Ranger to authorize access to Hive resources based on user roles. Which of the\nfollowing steps are necessary to achieve this? (Select TWO)",
    "options": {
      "A": "Create Ranger policies that map user roles to specific Hive resources and permissions.",
      "B": "Configure the Hive metastore to synchronize with the Ranger authorization policies.",
      "C": "Enable the Ranger Hive plugin in the Hive service configuration within Cloudera Manager.",
      "D": "Import the Hive metastore schema directly into Ranger.",
      "E": "Set the 'hive.security.authorization.manager’ property in ‘hive-site.xmr to ‘org.apache.hadoop.hive.ql.security.authorization.plugin.RangerHiveAuthorizer’."
    },
    "correctAnswers": [
      "A",
      "C"
    ],
    "explanation": "Ranger policies define the access control rules based on roles and resources. Enabling the Ranger Hive plugin ensures that Hive uses Ranger for authorization. Configuring Hive metastore to directly synchronize with Ranger policies is not a standard procedure. The 'hive.security.authorization.manager’ property is critical for enabling Ranger authorization. Importing the metastore schema directly into Ranger is unnecessary, Ranger plugin automatically handles that connection."
  },
  {
    "id": 138,
    "question": "During a CDP Private Cloud Base deployment, you are configuring the Cloudera Manager database. You\nchoose to use an external PostgreSQL database. Which of the following steps are CRITICAL to ensure\nthat Cloudera Manager can successfully connect to and utilize the PostgreSQL database? (Select TWO)",
    "options": {
      "A": "Install the PostgreSQL JDBC driver on the Cloudera Manager Server host.",
      "B": "Create a dedicated PostgreSQL user and database for Cloudera Manager with appropriate permissions.",
      "C": "Configure the PostgreSQL server to listen on all interfaces (0.0.0.0).",
      "D": "Disable SELinux on the Cloudera Manager Server host.",
      "E": "Ensure that the PostgreSQL server's ‘pg_hba.conf file allows connections from the Cloudera Manager Server's IP address."
    },
    "correctAnswers": [
      "A",
      "E"
    ],
    "explanation": "The JDBC driver is required for Java applications (like Cloudera Manager) to interact with PostgreSQL. The ‘pg_hba.conf file controls which hosts are allowed to connect to the PostgreSQL server. Creating a dedicated user and database are recommended best practices but are not strictly critical for basic connectivity, as the 'postgres' superuser could technically be used. Disabling SELinux is generally not recommended. Configuring PostgreSQL to listen on all interfaces is a potential security risk. The correct way to enable Cloudera Manager access to PostgreSQL is to allow only the necessary connections via the ‘pg_hba.conf file."
  },
  {
    "id": 139,
    "question": "You are using Cloudera Manager to deploy a new service within your CDP Private Cloud Base cluster.\nThe deployment fails with a 'Configuration validation failed' error. How do you MOST effectively\ntroubleshoot this issue?",
    "options": {
      "A": "Examine the Cloudera Manager Server logs for detailed error messages and configuration conflicts.",
      "B": "Check the service's logs on each host in the cluster.",
      "C": "Review the Cloudera Manager Agent logs on each host.",
      "D": "Search the Cloudera Knowledge Base for known issues related to the service and configuration parameters.",
      "E": "Restart the Cloudera Manager Server."
    },
    "correctAnswers": [
      "A"
    ],
    "explanation": "Configuration validation errors typically originate within the Cloudera Manager Server's logic, which validates configurations before deploying them to agents. Cloudera Manager logs will contain the most relevant and detailed error messages related to the configuration validation failure. Checking service or agent logs will generally not give the reason for this configuration validation error. A restart is unlikely to help."
  },
  {
    "id": 140,
    "question": "You're setting up a highly available (HA) Cloudera Manager Server. Which of the following components\nare REQUIRED to achieve HA for Cloudera Manager? (Select TWO)",
    "options": {
      "A": "A load balancer to distribute traffic between the active and passive Cloudera Manager Servers.",
      "B": "A shared Cloudera Manager database accessible to both the active and passive servers.",
      "C": "A distributed file system (e.g., HDFS) to store Cloudera Manager configuration files.",
      "D": "A dedicated ZooKeeper ensemble to manage the failover process.",
      "E": "A separate network interface card for each Cloudera Manager Server instance."
    },
    "correctAnswers": [
      "A",
      "B"
    ],
    "explanation": "A load balancer directs traffic to the active Cloudera Manager server, automatically switching to the passive server in case of failure. A shared database is essential to maintain consistent state between the active and passive instances. While HDFS is used by CDP, it is not used for Cloudera Manager configuration files during HA setup. ZooKeeper is used by some services within CDP but not directly by Cloudera Manager HA. Multiple NICs are not a requirement for HA of Cloudera Manager."
  },
  {
    "id": 141,
    "question": "You are deploying a CDP Private Cloud Base cluster with a large number of nodes (over 200). You notice\nthat the Cloudera Manager Server is experiencing performance issues, particularly during service\ndeployments and configuration updates. Which of the following strategies would be MOST effective in\nimproving the performance of the Cloudera Manager Server? (Select TWO)",
    "options": {
      "A": "Increase the heap size allocated to the Cloudera Manager Server process.",
      "B": "Migrate the Cloudera Manager database to a faster storage system (e.g., SSD).",
      "C": "Reduce the number of monitoring metrics collected by Cloudera Manager agents.",
      "D": "Distribute the Cloudera Manager Agent load across multiple Cloudera Manager Servers.",
      "E": "Disable the Cloudera Manager Event Server."
    },
    "correctAnswers": [
      "A",
      "B"
    ],
    "explanation": "Increasing the heap size allows the Cloudera Manager Server to handle more data in memory, reducing the need for disk I/O. Moving the database to faster storage significantly improves the speed of database queries, which are frequent during deployments and updates. Reducing metrics can help, but the impact is less significant than heap size or database performance. Distributing Agent load is not possible. Disabling the event server impacts monitoring, but does not directly address performance."
  },
  {
    "id": 142,
    "question": "You have deployed a CDP Private Cloud Base cluster with both HDFS and Ozone services. You want to\nconfigure Hive to use Ozone as its storage backend instead of HDFS for a specific set of tables. What\nconfiguration steps must you take within Hive to achieve this?",
    "options": {
      "A": "Set the property in ‘core-site.xml' to the Ozone namespace (e.g., '03fs://bucket.omserviceidP).",
      "B": "Create a new Hive warehouse directory on Ozone and configure Hive to use it for the specific tables using the ‘hive.metastore.warehouse.dir’ property.",
      "C": "Use the 'LOCATION’ clause in the 'CREATE TABLE statement to specify the Ozone path for each table you want to store on Ozone (e.g., 'CREATE TABLE my_table LOCATION '03fs://bucket.omserviceid/my_table'$).",
      "D": "Set the ‘ozone.enabled' property to ‘true' in 'hive-site.xmr.",
      "E": "Configure the Ozone delegation token for the Hive user in the Ranger policies."
    },
    "correctAnswers": [
      "C"
    ],
    "explanation": "The ‘LOCATION' clause in the ‘CREATE TABLE statement allows you to specify the exact storage location for each table, overriding the default HDFS location. The 'fs.defaultFS' affects all access to the filesystem. While a new warehouse directory would be required, it is the LOCATION clause that binds a table to that location. The 'ozone.enabled' property is not a standard Hive property and Ozone delegation token configuartion is related to authorization, not storage destination."
  },
  {
    "id": 143,
    "question": "You have deployed a CDP Private Cloud Base cluster. You notice that the Cloudera Manager Server is\nconsuming a high amount of CPU resources. After investigation, you determine that the resource\nutilization is due to frequent garbage collection (GC) cycles. Which of the following configuration\nchanges would be MOST effective in reducing the frequency of GC cycles and improving Cloudera\nManager Server performance?",
    "options": {
      "A": "Increase the maximum heap size C-Xmk) allocated to the Cloudera Manager Server process.",
      "B": "Reduce the number of monitored services in the cluster.",
      "C": "Switch to a different garbage collection algorithm (e.g., GIGC instead of CMS).",
      "D": "Decrease the frequency of Cloudera Manager's metric collection intervals.",
      "E": "Enable JVM compression."
    },
    "correctAnswers": [
      "A"
    ],
    "explanation": "Increasing the maximum heap size gives the JVM more memory to work with, reducing the need for frequent garbage collection cycles. While reducing monitored services or metric collection intervals may have a slight impact, the primary bottleneck is usually the available heap space. Switching GC algorithms can help in some cases, but increasing heap size is the first and most effective step. Enabling JVM compression is not directly related to decreasing Garbage collection."
  },
  {
    "id": 144,
    "question": "You are automating the deployment of a CDP Private Cloud Base cluster using Ansible. You need to\nconfigure the 'cm_api' module in Ansible to authenticate with the Cloudera Manager API using a\nusername and password. Which of the following code snippets demonstrates the correct way to define\nthe authentication parameters in your Ansible playbook?",
    "options": {
      "A": "B.",
      "C": "D.",
      "E": "Answer: A"
    },
    "correctAnswers": [
      "A"
    ],
    "explanation": "The ‘cm_api' module in Ansible uses the 'hostname', 'port’ , ‘username' , and ‘password' parameters for authentication. The example demonstrates the correct way to define these parameters within the 'cm_api' task. It is important to use 'username' and 'password' for credentials, as ‘user' and ‘pass' or other attribute names will not be understood by the Ansible module."
  },
  {
    "id": 145,
    "question": "You are planning the upgrade of your CDP Private Cloud Base cluster from version 7.1 .x to 7.2.x. Before\ninitiating the upgrade process through Cloudera Manager, what are the MOST critical pre-upgrade tasks\nyou MUST perform to ensure a successful upgrade? (Select TWO)",
    "options": {
      "A": "Back up the Cloudera Manager Server database and the Cloudera Manager Agent configuration directories.",
      "B": "Run the Cloudera Manager pre-upgrade health checks and resolve any identified issues.",
      "C": "Download and install the latest version of the Cloudera Manager Agent on all hosts in the cluster.",
      "D": "Update all operating system packages on all hosts in the cluster to the latest versions.",
      "E": "Disable all running services in the cluster to prevent data corruption during the upgrade."
    },
    "correctAnswers": [
      "A",
      "B"
    ],
    "explanation": "Backing up the Cloudera Manager database and Agent configurations ensures that you can restore the cluster to its previous state in case of an upgrade failure. Running the pre-upgrade health checks identifies potential issues that could cause the upgrade to fail, allowing you to resolve them proactively. While updating OS packages is good practice, it is not a mandatory pre-upgrade task. Agents are upgraded by Cloudera Manager and don't require manual intervention before upgrade. Services do not need to be disabled as part of the upgrade process."
  },
  {
    "id": 146,
    "question": "You are troubleshooting a slow-running MapReduce job. Cloudera Manager shows the YARN queue is\nconsistently at 100% utilization. Which of the following actions would be MOST effective in improving\njob performance and overall cluster efficiency, assuming fair share scheduler is in use?",
    "options": {
      "A": "Increase the ‘yarn.scheduler.minimum-allocation-mb' and ‘yarn.scheduler.maximum-allocation-mb' properties globally.",
      "B": "Create a new YARN queue with a higher guaranteed resource allocation and submit the job to that queue. Also make sure queue placement is properly configured.",
      "C": "Reduce the number of mappers and reducers for the job to decrease resource contention.",
      "D": "Increase the ‘mapreduce.map.memory.mb’ and 'mapreduce.reduce.memory.mb' properties for the job.",
      "E": "Disable preemption on the default queue to ensure running jobs are never killed."
    },
    "correctAnswers": [
      "B"
    ],
    "explanation": "Creating a new YARN queue with a higher guaranteed resource allocation ensures the slow-running job gets the resources it needs without excessively impacting other jobs. Adjusting queue placement improves efficiency. While other options might provide temporary relief, they don't address the core issue of resource contention within a single over-utilized queue."
  },
  {
    "id": 147,
    "question": "You need to configure Cloudera Manager to send email alerts when disk utilization on a specific\nDataNode exceeds 85%. Which of the following steps accurately describes how to achieve this through\nCloudera Manager?",
    "options": {
      "A": "Navigate to the DataNode's configuration page, add a new custom alert rule, and set the threshold to 85% for disk utilization.",
      "B": "Create a new metric monitor in Cloudera Manager Monitoring, select the 'Disk Usage' metric for the DataNode, configure the alert threshold to 85%, and configure the email notification settings.",
      "C": "Edit the DataNode's advanced configuration snippet (Safety Valve) for core-site.xml to include parameters for disk usage monitoring and email alerts.",
      "D": "Go to Alerts Create Alert Select 'Host' as entity type Select 'Disk Used (%)' metric Configure threshold to 85% Configure notification settings.",
      "E": "Install and configure a third-party monitoring tool, as Cloudera Manager does not support disk utilization alerts."
    },
    "correctAnswers": [
      "D"
    ],
    "explanation": "Cloudera Manager provides a built-in alerting system. Creating an alert through Alerts -> Create Alert and selecting the appropriate entity type (Host in this case), metric ('Disk Used (%)'), threshold (85%), and notification settings is the standard way to configure such alerts. Other options are either incorrect or less efficient."
  },
  {
    "id": 148,
    "question": "You are tasked with enabling Kerberos authentication on your CDP cluster After initial setup, users\nreport issues accessing HDFS. When examining the DataNode logs, you see errors related to failed\nauthentication. Which of the following is the MOST likely cause of this issue?",
    "options": {
      "A": "The HDFS service principal is not properly configured in the 'krb5.conf file on the client machines.",
      "B": "The DataNode's keytab file does not contain the correct service principal for HDFS.",
      "C": "The users do not have valid Kerberos tickets (TGTs) or their tickets have expired.",
      "D": "The NameNode is not properly configured to use Kerberos.",
      "E": "The HDFS client is not configured to use secure mode (Kerberos)."
    },
    "correctAnswers": [
      "B"
    ],
    "explanation": "If DataNode logs show authentication failures after enabling Kerberos, the most likely reason is that the DataNode itself is unable to authenticate to the Kerberos KDC because its keytab file doesn't contain the correct service principal. While other options could contribute to the problem, the DataNode's inability to authenticate is the most direct cause in this scenario."
  },
  {
    "id": 149,
    "question": "You are using Cloudera Manager to manage a CDP cluster. You notice that the Cloudera Manager Agent\non one of the DataNodes is frequently restarting. Examining the agent's logs reveals OutOfMemoryError\nexceptions. How should you address this issue?",
    "options": {
      "A": "Increase the heap size allocated to the Cloudera Manager Agent on the affected DataNode via Cloudera Manager's configuration settings.",
      "B": "Decrease the ‘ulimit’ for the Cloudera Manager Agent process to prevent it from consuming excessive resources.",
      "C": "Disable the Cloudera Manager Agent on the affected DataNode to prevent further restarts.",
      "D": "Reinstall the DataNode service on the affected host using Cloudera Manager.",
      "E": "Decrease the frequency of health checks performed by the Cloudera Manager Agent."
    },
    "correctAnswers": [
      "A"
    ],
    "explanation": "OutOfMemoryError exceptions in the Cloudera Manager Agent logs indicate that the agent is running out of memory. Increasing the heap size allocated to the agent allows it to handle its workload without crashing. The configuration setting for the agent's heap size is usually found within Cloudera Manager's agent configuration."
  },
  {
    "id": 150,
    "question": "You need to roll back a configuration change made through Cloudera Manager that has negatively\nimpacted the performance of the Hive Metastore. What is the MOST efficient way to revert to the\nprevious working configuration?",
    "options": {
      "A": "Manually edit the 'hive-site.xmr file on all Hive Metastore hosts to revert the changes.",
      "B": "Restart the Hive Metastore service, which will automatically revert to the last known good configuration.",
      "C": "Use Cloudera Manager's 'Deploy Client Configuration' feature to re-deploy the client configuration, ovemriting the incorrect settings.",
      "D": "In Cloudera Manager, navigate to the Hive Metastore service, select 'Configuration Changes', identify the change, and click 'Revert to Previous Configuration'.",
      "E": "Restore the entire cluster from a backup."
    },
    "correctAnswers": [
      "D"
    ],
    "explanation": "Cloudera Manager provides a built-in mechanism to revert configuration changes. Navigating to 'Configuration Changes', identifying the specific change, and selecting 'Revert to Previous Configuration' is the most efficient and reliable way to roll back changes. This method leverages Cloudera Manager's version control and ensures consistency across the cluster."
  },
  {
    "id": 151,
    "question": "Which of the following statements are true regarding the use of Cloudera Manager API? (Select TWO)",
    "options": {
      "A": "The Cloudera Manager API can be used to automate cluster deployment and configuration tasks.",
      "B": "The Cloudera Manager API only supports the Python programming language.",
      "C": "The Cloudera Manager API allows for programmatic monitoring of cluster health and performance.",
      "D": "The Cloudera Manager API requires a separate license to be used.",
      "E": "The Cloudera Manager API can only be accessed from the Cloudera Manager server itself."
    },
    "correctAnswers": [
      "A",
      "C"
    ],
    "explanation": "The Cloudera Manager API is a REST API that enables automation of cluster management tasks such as deployment, configuration, and monitoring. It supports various programming languages and does not require a separate license. Access is not limited to the Cloudera Manager server."
  },
  {
    "id": 152,
    "question": "You have a critical Spark application that requires a specific version of a third-party library which\nconflicts with the version included in the default Spark environment managed by Cloudera Manager.\nHow can you isolate the environment to the spark application?",
    "options": {
      "A": "Modify the global Spark configuration in Cloudera Manager to include the specific library version, potentially impacting other Spark applications.",
      "B": "Create a separate Spark application directory on the cluster, manually install the required library version, and configure the Spark application to use that directory.",
      "C": "Package the specific library version with the Spark application's JAR file (uber JAR) and ensure it's loaded before the default Spark libraries.",
      "D": "Utilize Spark's 'spark.driver.extraClassPatm and ‘spark.executor.extraClassPatm configuration options to specify the location of the desired library version, creating an isolated environment for this application.",
      "E": "Force update the default spark installation using Cloudera Manager to the specific library version required by the Spark application."
    },
    "correctAnswers": [
      "D"
    ],
    "explanation": "Using ‘spark.driver.extraClassPatW and ‘spark.executor.extraClassPatW allows you to specify additional classpath entries for the driver and executors, effectively creating an isolated environment for the Spark application without affecting other applications or the default Spark installation. Uber JARs can work, but classpath management provides greater flexibility. Modifying global configs or forcing updates are generally not recommended for a single application's needs."
  },
  {
    "id": 153,
    "question": "You are monitoring a CDP cluster with Cloudera Manager and notice consistently high CPU utilization on\nthe NameNode. Which of the following could contribute to this issue? (Select TWO)",
    "options": {
      "A": "A large number of small files in HDFS.",
      "B": "An insufficient number of DataNodes in the cluster.",
      "C": "Frequent HDFS metadata operations (e.g., listing directories, getting file status).",
      "D": "High network latency between the NameNode and DataNodes.",
      "E": "Too many concurrent YARN applications running on the cluster."
    },
    "correctAnswers": [
      "A",
      "C"
    ],
    "explanation": "A large number of small files in HDFS puts a strain on the NameNode because it needs to store metadata for each file in memory. Frequent metadata operations also increase CPU utilization as the NameNode constantly needs to process these requests. The number of DataNodes or network latency could affect overall cluster performance, but they don't directly explain high CPU utilization on the NameNode itself. YARN applications impact the ResourceManager more directly."
  },
  {
    "id": 154,
    "question": "You have configured Cloudera Navigator integration with Cloudera Manager. However, you are not\nseeing any audit events for HDFS operations in Cloudera Navigator. What are the potential reasons for\nthis?",
    "options": {
      "A": "The HDFS auditing feature is not enabled in Cloudera Manager.",
      "B": "The Cloudera Navigator Audit Server is not running.",
      "C": "The HDFS service is not configured to send audit events to the Cloudera Navigator Audit Server.",
      "D": "All of the above.",
      "E": "The NameNode's heap size is too small."
    },
    "correctAnswers": [
      "D"
    ],
    "explanation": "For HDFS audit events to be visible in Cloudera Navigator, several conditions must be met: HDFS auditing must be enabled in Cloudera Manager, the Cloudera Navigator Audit Server must be running, and the HDFS service must be configured to send audit events to the server. If any of these conditions are not met, audit events will not be visible. The NameNode's heap size is not directly related to audit events."
  },
  {
    "id": 155,
    "question": "You are using the Cloudera Manager API to automate the deployment of a new service. You need to\nensure that the service is only deployed if all its dependencies are met. Which API endpoint would you\nuse to check the service's deployment readiness?",
    "options": {
      "A": "/api/v31/clusters/{clusterName}/services/{serviceName}/commands/deployClientConfig",
      "B": "/api/v31/clusters/{clusterName}/services/{serviceName}/commands/start",
      "C": "/api/v31/clusters/{clusterName}/services/{serviceName}/stage",
      "D": "/api/v31/clusters/{clusterName}/services/{serviceName}/commands/canStage",
      "E": "/api/v31/clusters/{clusterName}/services/{serviceName}/roles/{roleName}/commands/restart"
    },
    "correctAnswers": [
      "D"
    ],
    "explanation": "The endpoint is specifically designed to check whether a service is ready to be deployed (staged). It verifies if all dependencies are met before proceeding with the deployment. The other endpoints are for deploying client configurations, starting the service, staging without checking dependencies, or restarting a role."
  },
  {
    "id": 156,
    "question": "Assume you've enabled TLS encryption for all services in your CDP cluster through Cloudera Manager.\nAfter some time, one of the custom applications interacting with HDFS begins failing with SSL handshake\nerrors. You suspect there's an issue with the application's truststore. Where can you find the correct CA\ncertificate to add to the application's truststore?",
    "options": {
      "A": "The CA certificate is automatically distributed to all client machines through the Cloudera Manager Agent.",
      "B": "The CA certificate is located in the ‘krb5.conf file on the Cloudera Manager server.",
      "C": "You must manually generate a new CA certificate from the Cloudera Manager Admin Console.",
      "D": "The CA certificate used by Cloudera Manager is typically stored in a file within the Cloudera Manager server's configuration directory (e.g., '/var/lib/cloudera-scm-server/cert’ The exact path depends on the CM version and custom configuration.",
      "E": "The CA certificate is not required for client applications; only the server-side components need it."
    },
    "correctAnswers": [
      "D"
    ],
    "explanation": "When TLS is enabled, clients need to trust the server's certificate. This is done by adding the CA certificate that signed the server's certificate to the client's truststore. Cloudera Manager generates a CA certificate when TLS is enabled, and this certificate is typically stored on the Cloudera Manager server in a specific directory. The exact location depends on the CM version and custom settings. You need to retrieve this certificate and add it to the application's truststore."
  },
  {
    "id": 157,
    "question": "You are trying to diagnose a persistent issue with a Hive query failing due to insufficient memory. You\nneed to identify the specific YARN container that is running the Hive query and its resource usage at the\ntime of failure. Which of the following steps would you take to accomplish this using Cloudera Manager\nand available logs?",
    "options": {
      "A": "Examine the HiveServer2 logs for the query ID, then correlate the query ID with the YARN application ID in the ResourceManager logs. Finally, use the YARN application ID to find the specific container and its resource usage in the NodeManager logs on the node where the container ran.",
      "B": "Use Cloudera Manager's YARN Resource Manager UIto search for the Hive query by its SQL statement, which will directly display the associated container and its resource consumption.",
      "C": "Check the Hive Metastore logs for the query ID and associated container ID. The Metastore logs will directly provide resource usage information for each query.",
      "D": "Enable debug logging for the HiveServer2 service, rerun the query, and then analyze the debug logs to find the container ID and resource usage details.",
      "E": "Use the 'yarn top' command on the ResourceManager to identify the container consuming the most memory. This container is likely associated with the failing Hive query."
    },
    "correctAnswers": [
      "A"
    ],
    "explanation": "The most accurate approach involves tracing the Hive query through different logs. First, the HiveServer2 logs provide the query ID. This ID can be correlated with the YARN application ID in the ResourceManager logs. The YARN application ID allows you to pinpoint the specific container in the NodeManager logs on the node where the container was executed. The NodeManager logs contain detailed resource usage information for each container. While Cloudera Manager provides some visibility into YARN applications, it does not directly link Hive queries to specific containers and their resource usage with the same level of detail. Other options are either inaccurate or inefficient."
  },
  {
    "id": 158,
    "question": "You're tasked with enabling Kerberos authentication for an existing CDP on-premises cluster using\nCloudera Manager. After running the Kerberization wizard and restarting the cluster, some jobs are\nfailing with 'GSS initiate failed' errors. What are the MOST likely causes?",
    "options": {
      "A": "Incorrectly configured DNS resolution for Kerberos principals.",
      "B": "Missing or incorrect entries in the '/etc/krb5.conf file on client machines.",
      "C": "Keytab files are not properly distributed or have incorrect permissions.",
      "D": "Clock skew between the KDC and cluster nodes exceeding the Kerberos tolerance.",
      "E": "The Cloudera Manager Agent is unable to communicate with the KDC."
    },
    "correctAnswers": [
      "A",
      "B",
      "C",
      "D"
    ],
    "explanation": "Kerberos authentication issues frequently arise from DNS problems (A), incorrect krb5.conf (B), keytab issues (C), and clock skew (D). While (E) is possible, it's less likely if the Kerberization wizard completed successfully. Addressing these areas helps to resolve the 'GSS initiate failed' errors."
  },
  {
    "id": 159,
    "question": "A user reports that a YARN application consistently fails with a 'java.lang.OutOfMemoryError: Java heap\nspace' error. As a CDP administrator, what configuration setting(s) within Cloudera Manager can you\nadjust to address this issue for ALL applications, while minimizing impact to other applications?",
    "options": {
      "A": "Increase the ‘yarn.scheduler.maximum-allocation-mb' property for the YARN service.",
      "B": "Increase the ‘yarn.nodemanager.resource.memory-mb' property for the YARN service.",
      "C": "Modify the ‘mapreduce.map.java.opts' and 'mapreduce.reduce.java.opts’ properties within the YARN service configuration.",
      "D": "Adjust the JVM heap size C-Xmk) within the ApplicationMaster's launch context via ‘yarn.app.mapreduce.am.resource.mb' and the corresponding yarn.app.mapreduce.am.command-optS.",
      "E": "Change 'yarn.nodemanager.vmem-check-enabled' to false"
    },
    "correctAnswers": [
      "D"
    ],
    "explanation": "Adjusting the ApplicationMaster's memory allocation (D) allows you to control the heap size for MapReduce applications without impacting other application types or the overall YARN capacity. Modifying ‘mapreduce.map.java.opts and 'mapreduce.reduce.java.opts' would impact all MapReduce jobs, which might be too broad. 'yarn.nodemanager.resource.memory-mb' and 'yarn.scheduler.maximum-allocation-mb' control total resources available but doesn't directly address individual AM heap size. Disabling Virtual Memory check is not a solution for OOM errors."
  },
  {
    "id": 160,
    "question": "You're using Cloudera Manager to configure a new Kafka topic. You want to ensure high availability and\ndurability for the messages published to this topic. Which of the following configuration options are\nMOST important to consider?",
    "options": {
      "A": "Setting ‘default.replication.factor’ to at least 3.",
      "B": "Setting ‘min.insync.replicas’ to be greater than 1.",
      "C": "Setting ‘num.partitions’ to the number of brokers in the cluster.",
      "D": "Setting ‘auto.create.topics.enable' to 'false'.",
      "E": "Enabling auto topic creation using ‘auto.create.topics.enable=true'"
    },
    "correctAnswers": [
      "A",
      "B"
    ],
    "explanation": "The ‘default.replication.factor’ (A) determines how many copies of each message are stored, providing fault tolerance. The 'min.insync.replicas' (B) setting ensures that a minimum number of replicas have acknowledged a write before it's considered successful, preventing data loss. While increasing ‘num.partitionS (C) can improve throughput, it's not directly related to HAIdurability. Disabling topic auto- creation (D) is more about security and control than HAIdurability and enabling topic auto- creation is about management, not HA or durability."
  },
  {
    "id": 161,
    "question": "You're troubleshooting a slow-running Hive query in your CDP on-premises cluster. You suspect the\nissue is related to insufficient resources allocated to the HiveServer2 service. Using Cloudera Manager,\nwhich metric(s) would provide the MOST direct indication of resource contention within the HiveServer2\nprocess?",
    "options": {
      "A": "CPU Usage (%) and Memory Usage (GB) of the HiveServer2 host.",
      "B": "HiveServer2's 'JVM Heap Usage' and 'Garbage Collection Time' metrics.",
      "C": "Network 1/0 on the HiveServer2 host.",
      "D": "HDFS Read and Write throughput.",
      "E": "The total number of active Hive queries."
    },
    "correctAnswers": [
      "B"
    ],
    "explanation": "JVM Heap Usage and Garbage Collection Time (B) directly reflect the resource consumption and efficiency within the HiveServer2 process. High JVM heap usage coupled with long garbage collection times strongly suggests memory pressure. Host CPU and memory usage (A) provide a general overview, but not specific to the HiveServer2 process. Network I/O (C) and HDFS throughput (D) are relevant to data access but not necessarily indicative of resource contention within HiveServer2 itself. Number of active queries shows load but not performance inside the process."
  },
  {
    "id": 162,
    "question": "After upgrading your CDP on-premises cluster, you notice that the DataNode service is repeatedly failing\nto start on one of the nodes. The Cloudera Manager agent is running, and the DataNode logs show the\nfollowing error: ‘java.lang.lllegalArgumentException: Invalid configuration value detected:\ndfs.datanode.failed.volumes.tolerated'. What is the MOST likely cause and how would you resolve it\nusing Cloudera Manager?",
    "options": {
      "A": "The 'dfs.datanode.failed.volumes.tolerated' property is set to a percentage value, which is no longer supported. Update the property to an absolute number using Cloudera Manager's HDFS service configuration.",
      "B": "The DataNode is unable to connect to the NameNode. Verify NameNode HA configuration and network connectivity using Cloudera Manager.",
      "C": "The DataNode's disk space is full. Use Cloudera Manager to check disk utilization and free up space.",
      "D": "There's a mismatch in the HDFS version between the DataNode and NameNode. Upgrade the DataNode using Cloudera Manager's rolling restart functionality.",
      "E": "The DataNode user doesn't have permission to access HDFS directories. Grant necessary access rights using Cloudera Manager."
    },
    "correctAnswers": [
      "A"
    ],
    "explanation": "The error message indicates that the 'dfs.datanode.failed.volumes.tolerated' property has an invalid value. In newer HDFS versions, percentage values for this property are deprecated, requiring an absolute number representing the maximum number of failed volumes tolerated. You would correct this by changing the property value in Cloudera Manager (A)."
  },
  {
    "id": 163,
    "question": "You are managing a CDP cluster with Spark jobs failing due to resource contention. Which of the\nfollowing configuration changes in Cloudera Manager would be MOST effective in isolating Spark\nworkloads and ensuring they get sufficient resources?",
    "options": {
      "A": "Increase the ‘yarn.nodemanager.resource.memory-mb' property for the YARN service to allocate more memory to each NodeManager.",
      "B": "Create a dedicated YARN queue specifically for Spark jobs and configure resource limits for that queue in the YARN service configuration.",
      "C": "Increase the ‘spark.driver.memory’ and ‘spark.executor.memory’ properties in the Spark service configuration.",
      "D": "Enable preemption for the default YARN queue to allow Spark jobs to take resources from other applications.",
      "E": "Reduce the 'dfs.block.size' property in HDFS."
    },
    "correctAnswers": [
      "B"
    ],
    "explanation": "Creating a dedicated YARN queue (B) provides the most effective isolation by allowing you to control the resources allocated specifically to Spark jobs. This ensures that Spark jobs have a guaranteed level of resources regardless of other workloads. Increasing yarn.nodemanager.resource.memory-mb' (A) might help but doesn't guarantee isolation. Adjusting Spark memory properties (C) affects individual job resource usage, not overall cluster resource allocation. Enabling preemption can cause instability. (D) has not guaranteed isolation."
  },
  {
    "id": 164,
    "question": "You want to configure automatic failover for the NameNode in your HA-enabled HDFS cluster. Using\nCloudera Manager, you have configured a ZooKeeper quorum and verified its health. However,\nautomatic failover is not working as expected. Which of the following configuration steps are essential\nto ensure automatic failover functions correctly?",
    "options": {
      "A": "Ensure that the 'dfs.ha.automatic-failover.enabled' property is set to 'true' in the HDFS service configuration.",
      "B": "Configure the 'dfs.nameserviceS and 'dfs.ha.namenodes.[nameservice ID]' properties correctly, specifying the NameNode logical names and IDs.",
      "C": "Deploy and configure the 'ZKFailoverController’ (ZKFC) process on each NameNode host using Cloudera Manager.",
      "D": "Ensure that all DataNodes are configured to use the NameNode logical names defined in Sdfs.nameservicess.",
      "E": "Manually start the secondary NameNode after failover."
    },
    "correctAnswers": [
      "A",
      "B",
      "C",
      "D"
    ],
    "explanation": "Automatic failover requires enabling the feature Cdfs.ha.automatic-failover.enabled=true’ - A), correct NameNode logical name configuration Cdfs.nameserviceS and 'dfs.ha.namenodes' - B), deploying and configuring ZKFC (C), and ensuring all DataNodes use the logical names (D). Option (E) contradicts the automatic nature of the failover; no manual intervention is needed in automatic failover, making it the incorrect answer."
  },
  {
    "id": 165,
    "question": "You are trying to apply a custom configuration to a specific role group (e.g., 'datanode_base') in\nCloudera Manager using the API. You want to update the property 'dfs.datanode.max.locked.memory'\nto '4294967296'. Which of the following curl commands is the MOST appropriate to achieve this?\nAssume you have a valid authentication token.",
    "options": {
      "A": "Option A",
      "B": "Option B",
      "C": "Option C",
      "D": "Option D",
      "E": "Option E"
    },
    "correctAnswers": [
      "A"
    ],
    "explanation": "The correct command is (A). Cloudera Manager's API requires a PUT request to update configurations. The data should be a JSON array with 'items' containing the 'name' and 'value' of the property. Other options are syntactically incorrect or use the wrong HTTP method."
  },
  {
    "id": 166,
    "question": "You've noticed that the Cloudera Manager Server is consuming a high amount of CPU resources. After\ninvestigation, you determine that the Cloudera Manager database (using PostgreSQL) is experiencing\nperformance issues. What are the MOST effective steps you can take to improve the performance of the\nCloudera Manager database directly ?",
    "options": {
      "A": "Increase the memory allocated to the Cloudera Manager Server process.",
      "B": "Analyze the PostgreSQL query logs and identify slow-running queries for optimization.",
      "C": "Run the 'ANALYZE command on the Cloudera Manager database to update statistics used by the query planner.",
      "D": "Increase the number of Cloudera Manager Agents reporting to the server.",
      "E": "Tune PostgreSQL configuration parameters such as and ‘work_mem' based on the available system resources."
    },
    "correctAnswers": [
      "B",
      "C",
      "E"
    ],
    "explanation": "The most direct ways to improve the database performance are analyzing and optimizing slow queries (B), updating table statistics (C), and tuning PostgreSQL configuration parameters (E). Increasing CM server memory (A) might indirectly help, but it doesn't directly address database performance. Increasing the number of agents (D) would likely worsen the issue."
  },
  {
    "id": 167,
    "question": "You are auditing the configuration of your CDP cluster and need to verify that all services are using TLS\nencryption for inter-service communication. How can you MOST efficiently achieve this verification\nusing Cloudera Manager?",
    "options": {
      "A": "Manually inspect the configuration files of each service on each host.",
      "B": "Use the Cloudera Manager API to retrieve the configuration for each service and check for the presence of TLS-related properties.",
      "C": "Run a custom script on each host to check for the presence of TLS-related configuration files.",
      "D": "Use Cloudera Manager's built-in search functionality to search for TLS-related configuration properties across all services.",
      "E": "Check Cloudera Manager's audit logs for TLS configuration changes."
    },
    "correctAnswers": [
      "D"
    ],
    "explanation": "Cloudera Manager's search functionality (D) provides the most efficient way to search for specific configuration properties (like those related to TLS) across all services. Manually inspecting files (A) is time-consuming and error-prone. Using the API (B) requires scripting and is less efficient than using the built-in search. A custom script (C) has similar drawbacks as A. Audit logs (E) only show changes, not the current state."
  },
  {
    "id": 168,
    "question": "You need to implement a configuration change to the Hive Metastore service that requires a rolling\nrestart. After applying the configuration change in Cloudera Manager, the rolling restart process fails\nwith the error 'Timeout waiting for Hive Metastore to become healthy'. What are the MOST likely causes\nfor this issue?",
    "options": {
      "A": "The Hive Metastore database connection is failing due to incorrect credentials or network issues.",
      "B": "The Hive Metastore service is experiencing high load and cannot start within the configured timeout period.",
      "C": "The Hive Metastore is configured to use an external database, and the database server is unavailable.",
      "D": "The Hive Metastore's heap size is insufficient, causing it to crash during startup.",
      "E": "The rolling restart timeout is configured too short."
    },
    "correctAnswers": [
      "A",
      "B",
      "C",
      "D"
    ],
    "explanation": "A timeout waiting for the Hive Metastore to become healthy can stem from several issues during startup. Database connectivity problems (A and C), high load (B), and insufficient heap (D) are all potential reasons why the Metastore might fail to start within the timeout. While increasing the timeout (E) might help, it doesn't address the underlying cause, so the other options are more likely root causes."
  },
  {
    "id": 169,
    "question": "Your organization has a requirement to rotate the encryption keys used by the HDFS encryption zones\non a regular basis. What steps must you take to ensure that all data is re-encrypted with the new key\nand that no data loss occurs during the key rotation process?",
    "options": {
      "A": "Use the ‘hdfs crypto key -rotate’ command to rotate the key. HDFS automatically handles the re- encryption of all data.",
      "B": "Use the ‘hdfs crypto key -rotate' command to rotate the key, then run the ‘hdfs dfs -reencrypt’ command on the encryption zone to re-encrypt all data.",
      "C": "Create a new encryption zone with the new key, copy all data to the new zone, and then delete the old encryption zone.",
      "D": "Disable the encryption zone, rotate the key, and then re-enable the encryption zone.",
      "E": "Rotate the key in the KMS, and all new writes will use the new key. Old data will remain encrypted with the old key, which is acceptable."
    },
    "correctAnswers": [
      "B"
    ],
    "explanation": "The correct process (B) involves rotating the key using 'hdfs crypto key -rotates and then explicitly re- encrypting the data within the encryption zone using 'hdfs dfs -reencrypt. Option (A) is incorrect because 'hdfs crypto key -rotate' does not automatically re-encrypt data. Options (C) and (D) are disruptive and unnecessary. Option (E) might be acceptable in some scenarios but doesn't meet the requirement to re-encrypt all data."
  },
  {
    "id": 170,
    "question": "You are troubleshooting a slow-running MapReduce job on your Cloudera Data Platform (CDP) on-\npremises cluster. After examining the logs, you notice excessive garbage collection (GC) pauses in the\nTaskTracker logs. Which of the following actions is MOST likely to improve the job's performance?",
    "options": {
      "A": "Increase the 'mapreduce.map.java.opts' and 'mapreduce.reduce.java.opts’ parameters to allocate more heap memory to the TaskTracker processes.",
      "B": "Reduce the number of concurrent tasks that each TaskTracker can run by adjusting the ‘mapreduce.tasktracker.map.tasks.maximum’ and mapreduce.tasktracker.reduce.tasks.maximum’ parameters.",
      "C": "Switch the garbage collector algorithm used by the TaskTrackers to a concurrent collector like Gl GC by setting the appropriate JVM options.",
      "D": "Increase the block size in HDFS to reduce the number of disk 1/0 operations performed by the MapReduce tasks.",
      "E": "All of the above are potentially valid actions."
    },
    "correctAnswers": [
      "E"
    ],
    "explanation": "Excessive GC pauses indicate the JVM is spending too much time reclaiming memory, impacting performance. Increasing heap size (A), reducing concurrent tasks (B), and switching to a more efficient GC algorithm (C) can all alleviate this. Increasing HDFS block size (D) helps with I/O but is less directly related to GC pauses."
  },
  {
    "id": 171,
    "question": "A user reports that their Hive query is failing with a 'java.lang.OutOfMemoryError: Java heap space'\nerror. You suspect the error is occurring during the execution of a UDF. Where would you MOST likely\nfind the relevant error details to confirm this?",
    "options": {
      "A": "The HiveServer2 log file C/var/log/hive/hive-server2.loØ)",
      "B": "The Hadoop ResourceManager log file C/var/log/hadoop-yarn/yarn-rm/’) on the ResourceManager node.",
      "C": "The Hive Metastore log file C/var/log/hive/hive-metastore.log')",
      "D": "The YARN container logs associated with the task executing the IJDF. These can be found via the YARN ResourceManager UIor using the ‘yarn logs command.",
      "E": "The operating system's syslog ('Ivar/log/syslog’ or ‘ Ivar/log/messageS)"
    },
    "correctAnswers": [
      "D"
    ],
    "explanation": "The ‘java.lang.OutOfMemoryError' during IJDF execution occurs within the YARN container running the task. The container logs, accessible via the YARN ResourceManager UIor the ‘yarn logs' command, contain the stack trace and details needed to diagnose the issue. HiveServer2 logs (A) are useful for general Hive errors, ResourceManager logs (B) for cluster resource issues, and Metastore logs (C) for Metastore-related problems."
  },
  {
    "id": 172,
    "question": "You are investigating an issue where HBase regions are frequently splitting. Which configuration\nproperty should you examine FIRST to determine if the splits are due to exceeding the maximum region\nsize?",
    "options": {
      "A": "'hbase.hregion.max.filesize'",
      "B": "'hbase.hregion.memstore.flush.size’",
      "C": "'hbase.regionserver.global.memstore.upperLimit",
      "D": "‘hbase.hregion.majorcompaction.period'",
      "E": "'hbase.hregion.split.policy'"
    },
    "correctAnswers": [
      "A"
    ],
    "explanation": "'hbase.hregion.max.filesize’ directly controls the maximum size a region can grow to before it is split. 'hbase.hregion.memstore.flush.size’ (B) controls when the memstore is flushed to disk. 'hbase.regionserver.global.memstore.upperLimit’ (C) controls the total memstore usage. 'hbase.hregion.majorcompaction.period' (D) controls the frequency of major compactions. 'hbase.hregion.split.policy’ (E) dictates the split policy not the maximum file size."
  },
  {
    "id": 173,
    "question": "A Data Engineer reports that their Spark application is consistently failing with ExecutorLostFailure\nexceptions. You suspect network connectivity issues between the driver and the executors. Which log\nfiles would you analyze to confirm this hypothesis?",
    "options": {
      "A": "The Spark driver log and the Spark executor logs on the nodes where the executors were running.",
      "B": "The YARN ResourceManager logs on the ResourceManager node.",
      "C": "The HDFS NameNode logs.",
      "D": "The Oozie server logs.",
      "E": "The Cloudera Manager agent logs on the host running the Spark Driver."
    },
    "correctAnswers": [
      "A"
    ],
    "explanation": "ExecutorLostFailure exceptions indicate that the driver cannot communicate with the executors. Analyzing the Spark driver log and the executor logs will reveal connection errors, timeouts, or other network-related problems. The YARN ResourceManager logs (B) provide information about resource allocation, but not necessarily network connectivity. HDFS logs (C) are irrelevant for network connectivity between Spark components. Oozie logs (D) are for workflow orchestration, and Cloudera Manager agent logs (E) primarily deal with management tasks."
  },
  {
    "id": 174,
    "question": "You notice that the HDFS DataNodes are frequently logging 'DiskOutOfSpaceException' errors, but the\nHDFS capacity utilization reported by Cloudera Manager is only at 80%. What is the MOST likely cause of\nthis discrepancy?",
    "options": {
      "A": "The DataNodes are configured with multiple data directories, and one or more of those directories are full.",
      "B": "The HDFS NameNode is reporting incorrect capacity utilization statistics.",
      "C": "The DataNodes have not been restarted recently, and the disk space information is stale.",
      "D": "The HDFS replication factor is set too high, consuming excessive disk space.",
      "E": "HDFS Quotas are restricting the space usable by specific users or directories even though the overall disk usage is low."
    },
    "correctAnswers": [
      "A"
    ],
    "explanation": "DataNodes often have multiple data directories. If one directory fills up, it triggers the ‘DiskOutOfSpaceException’ even if other directories have free space. Cloudera Manager reports overall HDFS capacity, so it doesn't reflect the individual directory issue. NameNode reporting inaccuracies (B) are less likely. Restarting DataNodes (C) won't solve a disk full issue. High replication (D) could exacerbate the issue but isn't the primary cause. HDFS Quotas (E) could also be the issue restricting users or directories."
  },
  {
    "id": 175,
    "question": "A Kerberized Hive query fails with the error 'javax.security.auth.login.LoginException: Cannot get key for\nprincipal hive/your.hive.server@YOUR.REALM from keytab /etc/security/keytabs/hive.service.keytab'.\nWhich of the following steps should you take to resolve this issue? (Select two)",
    "options": {
      "A": "Ensure that the keytab file '/etc/security/keytabs/hive.service.keytab' exists and is readable by the Hive user.",
      "B": "Verify that the Hive principal is present in the keytab file using the ‘klist -kt /etc/security/keytabs/hive.service.keytab’command.",
      "C": "Restart the Kerberos Key Distribution Center (KDC).",
      "D": "Update the Hive configuration property 'hive.security.authorization.enabled' to ‘false’ .",
      "E": "Ensure the system time is synchronized across all cluster nodes using N TP."
    },
    "correctAnswers": [
      "A",
      "B"
    ],
    "explanation": "The error indicates a problem with the Hive service's keytab file. A) confirms the keytab exists and is accessible. B) verifies the correct principal is present in the keytab. Restarting the KDC (C) is unnecessary unless there's a KDC issue. Disabling authorization (D) bypasses Kerberos but isn't a solution. Time synchronization (E) is important for Kerberos, but the error message points directly to a keytab problem."
  },
  {
    "id": 176,
    "question": "You are tasked with troubleshooting a Flume agent that is experiencing significant data loss. The agent is\nconfigured with a memory channel, a spooldir source, and an HDFS sink. What configuration change\nwould MOST directly address the data loss issue?",
    "options": {
      "A": "Switch the memory channel to a file channel.",
      "B": "Increase the capacity of the memory channel.",
      "C": "Enable transaction guarantees on the HDFS sink.",
      "D": "Increase the batch size of the spooldir source.",
      "E": "Implement a custom interceptor to filter out invalid events."
    },
    "correctAnswers": [
      "A"
    ],
    "explanation": "Memory channels are volatile; data is lost if the agent crashes. Switching to a file channel provides persistence and prevents data loss in case of agent failure. Increasing memory channel capacity (B) might delay data loss, but doesn't prevent it. Transaction guarantees on the HDFS sink (C) ensure data is written to HDFS atomically, but don't address loss in the channel. Source batch size (D) affects throughput, not data loss. An interceptor (E) filters data, it does not prevent loss."
  },
  {
    "id": 177,
    "question": "A user reports that their Impala query is consistently timing out. After examining the Impala logs, you\nsee numerous entries indicating 'Metadata load failure'. What is the MOST likely cause of this issue?",
    "options": {
      "A": "The Impala catalog server is overloaded and unable to serve metadata requests.",
      "B": "The HDFS NameNode is experiencing high latency, preventing Impala from accessing metadata.",
      "C": "The Hive Metastore is unavailable or returning errors.",
      "D": "There are insufficient resources (CPU, memory) allocated to the Impala executors.",
      "E": "The query is too complex and requires optimization."
    },
    "correctAnswers": [
      "C"
    ],
    "explanation": "Impala relies on the Hive Metastore for metadata about tables and partitions. 'Metadata load failure' strongly suggests a problem with the Metastore being unavailable or returning errors. While NameNode latency (B) can impact performance, the error message is more specific. Catalog server overload (A) is possible, but Metastore issues are more common. Insufficient executor resources (D) would likely lead to different error messages. Query complexity (E) might cause timeouts, but the 'Metadata load failure' is a more direct indicator of a Metastore problem."
  },
  {
    "id": 178,
    "question": "You are observing high CPU utilization on your ZooKeeper nodes. Which of the following actions is MOST\nlikely to reduce the CPU load on the ZooKeeper ensemble?",
    "options": {
      "A": "Increase the heap size allocated to the ZooKeeper processes.",
      "B": "Reduce the number of clients connecting to the ZooKeeper ensemble.",
      "C": "Increase the tick time for ZooKeeper sessions.",
      "D": "Enable authentication for ZooKeeper clients.",
      "E": "Move all ZooKeeper data to an SSD for faster access."
    },
    "correctAnswers": [
      "B"
    ],
    "explanation": "ZooKeeper's CPU usage is primarily driven by the number of client requests it needs to handle. Reducing the number of clients (B) directly reduces the processing load. Increasing heap size (A) might help with memory management but doesn't directly reduce CPU load. Increasing the tick time (C) affects session timeouts, not CPU usage. Authentication (D) adds to the CPU load. Faster storage (E) could improve performance but doesn't fundamentally address high CPU usage caused by excessive client requests."
  },
  {
    "id": 179,
    "question": "A newly deployed Spark application is failing with a 'java.lang.ClassNotFoundException'. You've verified\nthat the application's JAR file contains the missing class. What are the two MOST likely causes of this\nissue in a YARN cluster environment? (Select two)",
    "options": {
      "A": "The application's JAR file is not being distributed to the YARN containers running the Spark executors.",
      "B": "The Spark application is not specifying the correct class path for the JAR file.",
      "C": "The Hadoop classpath is corrupted on the client machine submitting the job.",
      "D": "The Kerberos ticket for the user submitting the job has expired.",
      "E": "The Spark driver program is using an incompatible version of Java."
    },
    "correctAnswers": [
      "A",
      "B"
    ],
    "explanation": "ClassNotFoundException indicates the JVM cannot find the class at runtime. A) directly addresses the issue of the JAR not being available in the container. B) refers to the spark application jar/packages are not explicitly specified in the Spark configuration. If the container does not know where to find the dependency, it will throw a class not found exception. Corrupted Hadoop classpath (C) is possible but less likely, as it would affect other Hadoop components. Kerberos ticket expiration (D) would cause authentication failures, not ClassNotFoundException. Incompatible Java version (E) could cause other issues but is less likely if the class is present in the JAR."
  },
  {
    "id": 180,
    "question": "You're managing a Cloudera CDP cluster where several services are experiencing intermittent\nconnection issues to the external database used for metadata storage. Analyzing the logs, you see\nrecurring errors related to database connection timeouts. Identify the TWO most appropriate strategies\nto mitigate this issue. (Select two)",
    "options": {
      "A": "Increase the connection timeout values for the services connecting to the external database.",
      "B": "Implement connection pooling on the services to reuse existing database connections.",
      "C": "Migrate the metadata to the embedded database provided by Cloudera CDR",
      "D": "Decrease the number of concurrent users accessing the services.",
      "E": "Disable auditing across all Cloudera CDP services."
    },
    "correctAnswers": [
      "A",
      "B"
    ],
    "explanation": "Connection timeouts directly indicate the connection is failing after a specific period. Increasing the timeout (A) gives the connection more time to succeed. Connection pooling (B) reduces the overhead of establishing new connections, making the database more efficient. Migrating to embedded database (C) is a major architectural change, while it will resolve external connection issue, it has its own drawback such as limited scalability. Decreasing concurrent users (D) might alleviate load but doesn't address the underlying connection problem. Disabling auditing (E) is unrelated to database connection issues."
  },
  {
    "id": 181,
    "question": "You are monitoring a Cloudera Data Platform on-premises cluster. You observe that the HDFS 'Available'\ndisk space is consistently low, and the 'Under-replicated Blocks' metric is increasing despite the cluster\nnot experiencing any node failures. After investigation, you discover a large number of recently deleted\nfiles are still occupying space in HDFS. What TWO actions should you take to remediate this situation?\n(Select two)",
    "options": {
      "A": "Run the HDFS balancer to redistribute blocks across DataNodes.",
      "B": "Execute the 'hdfs dfs -expunge' command to remove the deleted files from the trash directories.",
      "C": "Increase the replication factor for critical HDFS files.",
      "D": "Reduce the 'fs.trash.interval' configuration property to shorten the trash retention period.",
      "E": "Decommission underutilized DataNodes to consolidate data on fewer nodes."
    },
    "correctAnswers": [
      "B",
      "D"
    ],
    "explanation": "Deleted files in HDFS are moved to the trash and are retained for a configured period Cfs.trash.interval') before being permanently deleted. B) Immediately running ‘hdfs dfs -expunge' removes those files and frees up space. D) Reducing 'fs.trash.interval' ensures that files are automatically purged from the trash more frequently. Running the balancer (A) redistributes existing data, not deleted data. Increasing replication (C) would consume more space. Decommissioning nodes (E) might be a valid long-term strategy but doesn't address the immediate issue of trash occupying space."
  },
  {
    "id": 182,
    "question": "You need to retrieve the current health checks for all the hosts in your Cloudera Data Platform (CDP)\ncluster using the Cloudera Manager REST API. Which of the following REST API endpoints would you\nuse?",
    "options": {
      "A": "/api/v32/hosts/{hostld}/healthChecks",
      "B": "/api/v32/clusters/{clusterName}/hosts/healthChecks",
      "C": "/api/v32/hosts/healthChecks",
      "D": "/api/v32/clusters/{clusterName}/hosts/{hostld}/healthChecks",
      "E": "/api/v32/clusters/{clusterName}/hosts/healthSummary"
    },
    "correctAnswers": [
      "A"
    ],
    "explanation": "The correct endpoint is 7api/v32/hosts/{hostld}/healthChecks’. This allows you to target a specific host by its ID and retrieve its health checks. While provides a summary, it doesn't give detailed health check information for each host."
  },
  {
    "id": 183,
    "question": "You want to automate the deployment of a new configuration file to all DataNodes in your Hadoop\ncluster using the Cloudera Manager\nREST API. What is the correct sequence of API calls you need to execute? Assume you have the correct\ncluster name, service name, and config file content.",
    "options": {
      "A": "1. POST to /api/v32/clusters/{clusterName}/services/{serviceName}/config 2. POST to /api/v32/clusters/{clusterName}/commands/deployClientConfig",
      "B": "1. PUT to /api/v32/clusters/{clusterName}/services/{serviceName}/config 2. POST to /api/v32/clusters/{clusterName}/commands/refreshServiceConfigs",
      "C": "1. PUT to /api/v32/clusters/{clusterName}/services/{serviceName}/config 2. POST to /api/v32/clusters/{clusterName}/commands/deployClientConfig",
      "D": "1. POST to /api/v32/clusters/{clusterName)/services/{serviceName}/config 2. POST to /api/v32/clusters/{clusterName}/commands/restart",
      "E": "1. PUT to /api/v32/clusters/{clusterName}/services/{serviceName}/config 2. POST to /api/v32/clusters/{clusterName}/commands/restartService"
    },
    "correctAnswers": [
      "C"
    ],
    "explanation": "The correct sequence is to first use ‘PUT' to update the service configuration. 'POST ' is typically used for creating new resources. Then, use \"POST with 'Icommands/deployClientConfig' to deploy the updated configuration to the client nodes. Note that for core configuration changes, a service restart may be required instead of deployClientConfig, but deploying the client config is the appropriate immediate first action in many configuration change scenarios."
  },
  {
    "id": 184,
    "question": "You are writing a Python script to interact with the Cloudera Manager REST API to start a specific role in\nyour cluster. The role name is 'datanode-01.example.com'. Which of the following code snippets is the\nmost appropriate to achieve this?",
    "options": {
      "A": "B.",
      "C": "D.",
      "E": "Answer: C"
    },
    "correctAnswers": [
      "C"
    ],
    "explanation": "The correct URL structure to start a role through the Cloudera Manager API is You need to specify the cluster name and the service name to which the role belongs."
  },
  {
    "id": 185,
    "question": "You want to monitor the CPU usage of a specific host in your Cloudera CDP cluster using the Cloudera\nManager REST API. Which of the following API endpoints can be used to retrieve this metric?",
    "options": {
      "A": "/api/v32/metrics?query=cpu_percent",
      "B": "/api/v32/timeseries?query=cpu_percent&from=now-1h",
      "C": "/api/v32/hosts/{hostld}/metrics?query=cpu_percent",
      "D": "/api/v32/clusters/{clusterName}/hosts/{hostld}/metrics?query=cpu_percent",
      "E": "/api/v32/timeseries?query=select cpu_percent where category = HOST and entityName = {hostld}"
    },
    "correctAnswers": [
      "E"
    ],
    "explanation": "The '/api/v32/timeserieS endpoint is the appropriate endpoint to retrieve time-series data, including CPU usage. The query parameter format uses a selection syntax (select where category = and entityName = ). You need to specify the category (HOST) and the entityName (hostld) to target the specific host."
  },
  {
    "id": 186,
    "question": "You are using the Cloudera Manager API to create a new Hive service within an existing cluster named\n'MyCluster'. Which of the following JSON payloads is the most correct and complete (assuming a valid\nAPI version and authentication) for creating the Hive service with default settings, including necessary\ndependencies?",
    "options": {
      "A": "B.",
      "C": "D.",
      "E": "Answer: E"
    },
    "correctAnswers": [
      "E"
    ],
    "explanation": "Option E is the most comprehensive and accurate. While option A provides basic information, it lacks the cluster association and dependency on HDFS, which is crucial for Hive to function. Options B and C have syntax or missing element issues. Option D lists role types which may need to be created separately and are not required in initial service creation. Option E explicitly specifies the HDFS dependency, ensuring that Cloudera Manager can properly configure the Hive service. The cluster name is not passed in the body, it's passed as part of URL"
  },
  {
    "id": 187,
    "question": "You're trying to configure a new property, 'my.custom.property' , for the Hive Metastore service via the\nCloudera Manager API. You want this property to be reflected in the 'hive-site.xmr configuration file.\nHowever, after applying the configuration and restarting the service, the property is not present in the\nfile. What are the potential reasons and solutions? (Select all that apply)",
    "options": {
      "A": "The property was set using the wrong API endpoint. Configuration properties should be set via .",
      "B": "The property name is incorrect or contains invalid characters. Ensure the property name follows standard naming conventions.",
      "C": "The service was not restarted after applying the configuration changes. A restart is always required for new properties to take effect.",
      "D": "The property may not be recognized by the Hive Metastore service. Check the Hive documentation for supported properties.",
      "E": "The property has incorrect data type. API doesn't validate, therefore Hive will fail to pick up the setting."
    },
    "correctAnswers": [
      "A",
      "B",
      "D"
    ],
    "explanation": "Several reasons can cause the configuration to fail: A) The correct endpoint is. B) Incorrect property names or invalid characters prevent the service from recognizing the property. D) The Hive Metastore service might not recognize the property if it's not a supported configuration option. While restarting the service is often necessary, it won't help if the property is invalid or not supported.E) API doesn't validate, therefore Hive will fail to pick up the setting but after restart and therefore not valid answer."
  },
  {
    "id": 188,
    "question": "You're managing a Cloudera Data Platform (CDP) cluster and want to use the Cloudera Manager REST\nAPI to retrieve a list of all active YARN applications along with their resource usage (CPU, memory). How\ncan you achieve this using the API?",
    "options": {
      "A": "Option A",
      "B": "Option B",
      "C": "Option C",
      "D": "Option D",
      "E": "Option E"
    },
    "correctAnswers": [
      "D"
    ],
    "explanation": "The correct approach is to use to get a list of applications associated with the YARN service in the specified cluster, and filter the results to obtain only active applications. While other options may provide some information, they are not designed specifically to provide both the list of applications and their resource usage directly."
  },
  {
    "id": 189,
    "question": "You've updated the Java heap size for the Hive Metastore service using the Cloudera Manager API. After\nrestarting the service, you observe that the Metastore service is crashing repeatedly with\nOutOfMemoryErrors. What are the most likely reasons, assuming the new heap size is larger than the\nprevious one?",
    "options": {
      "A": "The new heap size is too large for the available physical memory on the host. The OS is unable to allocate the requested memory, leading to crashes.",
      "B": "The garbage collection settings are not optimized for the new heap size, causing frequent full GC pauses and eventual 00M errors.",
      "C": "The ‘ulimit’ settings for the Hive Metastore user are too restrictive, preventing the service from using the configured heap size.",
      "D": "The ‘hive-site.xmr is corrupted.",
      "E": "The initial heap size C-XmS) and maximum heap size ('-Xmx') are set to different values which can lead to JVM instabilities. It needs to be same to have consistant allocation."
    },
    "correctAnswers": [
      "A",
      "B",
      "C"
    ],
    "explanation": "A) If the requested heap size exceeds the available physical memory, the OS will be unable to allocate the memory, leading to crashes. B) If garbage collection is not tuned for larger heaps, it can lead to frequent pauses and 00M errors despite the increased memory. C) ulimit’ settings can restrict the maximum memory a process can use, even if the JVM is configured with a larger heap size.D) Although ‘hive- site.xmr corruption could be a reason, is not directly related to 00M and increased Heap Size. E) Initial and Maximum heap size can be different, it's about dynamically alloacting memory, it's not a direct cause of OOM."
  },
  {
    "id": 190,
    "question": "You are using the Cloudera Manager REST API to retrieve the configuration history for the HDFS service.\nYou need to filter the results to only show changes made by a specific user, 'admin', within the last 24\nhours. How can you achieve this?",
    "options": {
      "A": "Option A",
      "B": "Option B",
      "C": "Option C",
      "D": "Option D",
      "E": "Option E"
    },
    "correctAnswers": [
      "B"
    ],
    "explanation": "The endpoint does not support filtering by user or time range directly using query parameters. You need to retrieve all the configuration history and then filter the results programmatically in your application code."
  },
  {
    "id": 191,
    "question": "You are programmatically deploying a new Cloudera DataFlow (CDF) service using the Cloudera Manager\nREST API. After submitting the creation request, the API returns a success code (200 0K), but the service\nfails to start. Upon inspecting the Cloudera Manager IJI, you see an error indicating a missing license key.\nHow can you use the REST API to upload a valid license key to resolve this issue?",
    "options": {
      "A": "Use a POST request to '/api/v32/license’ with the license key in the request body.",
      "B": "Use a PUT request to with the license key as a file attachment.",
      "C": "Use a POST request to '/api/v32/cm/config’ to update 'license_key’ property to the key value.",
      "D": "Use a POST request to '/api/v32/license’ endpoint with a multipart/form-data content type to upload the license file.",
      "E": "License keys cannot be uploaded programmatically via the Cloudera Manager REST API and must be uploaded through the IJI."
    },
    "correctAnswers": [
      "D"
    ],
    "explanation": "The correct way to upload a license key using the Cloudera Manager REST API is to use a POST request to the S/api/v32/license’ endpoint with a multipart/form-data content type to upload the license file. This allows you to provide the license key as a file attachment within the request."
  },
  {
    "id": 192,
    "question": "You are using the Cloudera Manager REST API to perform a rolling restart of the HDFS service. You want\nto monitor the progress of the restart operation in real-time. Which of the following approaches is the\nmost efficient and reliable way to track the restart progress?",
    "options": {
      "A": "Poll the endpoint repeatedly to check the service's overall health and status.",
      "B": "Poll the '/api/v32/commands/{commandldY endpoint repeatedly to check the command's status. This endpoint will provide details on the progress and any errors encountered.",
      "C": "Use the '/api/v32/eventS endpoint and filter for events related to the HDFS service restart.",
      "D": "Use the endpoint and filter by HDFS service and restart operation to see the operation events.",
      "E": "Implement a WebSocket connection to receive real-time updates on the command's progress. This requires a custom client that supports WebSocket communication."
    },
    "correctAnswers": [
      "B"
    ],
    "explanation": "The most efficient and reliable way is B) to poll the '/api/v32/commands/{commandldY endpoint. This endpoint provides detailed information about the command's progress, including its current state (e.g., RUNNING, SUCCESS, FAILED), start and end times, and any errors encountered. While A) can give a general service health, it doesn't provide granular restart progress. C) is event based so may not be real time. D) is less specific on the actual command progress. E) provides real-time progress is the most complex and not always needed."
  },
  {
    "id": 193,
    "question": "You're tasked with automating the creation of a new Cloudera Manager user with specific roles and\npermissions using the REST API. You need to create a user 'data_analyst' with 'Read-Only' access to the\n'MyCluster' cluster and 'Operator' access to the 'Hive' service. Which of the following API call(s) and\nbody structures would accomplish this? Note that a single user creation might require multiple API calls\nto set all the desired permissions.",
    "options": {
      "A": "Option A",
      "B": "Option B",
      "C": "Option C",
      "D": "Option D",
      "E": "Option E"
    },
    "correctAnswers": [
      "B"
    ],
    "explanation": ""
  },
  {
    "id": 194,
    "question": "You are configuring Auto-TLS for a CDP on-premise cluster. After enabling Auto-TLS, you notice some\nservices are failing to start, and the Cloudera Manager UIreports certificate-related errors. What is the\nMOST likely cause of this issue?",
    "options": {
      "A": "The DNS configuration is incorrect, and services cannot resolve each other's hostnames.",
      "B": "The Cloudera Manager Agent on some nodes is not running with root privileges.",
      "C": "The Java Cryptography Extension (JCE) Unlimited Strength Jurisdiction Policy Files are not installed on all nodes.",
      "D": "The property in the Cloudera Manager database is incorrect.",
      "E": "The certificate authority (CA) certificate used by Auto-TLS has expired."
    },
    "correctAnswers": [
      "A"
    ],
    "explanation": "Incorrect DNS configuration is a common cause of Auto-TLS failures. Services rely on DNS to resolve hostnames and validate certificates. If DNS resolution is failing, certificate validation will also fail, preventing services from starting."
  },
  {
    "id": 195,
    "question": "During the Auto-TLS configuration, you need to specify the certificate validity period. Which Cloudera\nManager property controls this validity period?",
    "options": {
      "A": "tls.certificate.validity.days",
      "B": "cm.auto_tls.certificate_validity",
      "C": "auto_tls.certificate.validity_days",
      "D": "cm_auto_tls_certificate_validity_days",
      "E": "ssl_certificate_validity"
    },
    "correctAnswers": [
      "D"
    ],
    "explanation": "The property in ‘cm_auto_tls_certificate_validity_days’ in Cloudera Manager controls the validity period of the certificates generated by Auto-TLS. This property determines how long the certificates will be valid before they need to be renewed."
  },
  {
    "id": 196,
    "question": "You have enabled Auto-TLS in your CDP cluster. You want to verify that all services are using the Auto-\nTLS generated certificates. How can you confirm this?",
    "options": {
      "A": "Examine the service configuration files (e.g., core-site.xml, hdfs-site.xml) for references to the Auto- TLS generated certificate paths.",
      "B": "Use the ‘openssl s_client’ command to connect to each service and inspect the presented certificate chain.",
      "C": "Check the Cloudera Manager audit logs for certificate-related events.",
      "D": "Review the Cloudera Manager UIfor each service instance to ensure that the TLS/SSL settings are configured to use Auto-TLS.",
      "E": "Check the Cloudera Manager server logs for any errors or warnings related to certificate loading."
    },
    "correctAnswers": [
      "A",
      "B",
      "D"
    ],
    "explanation": "All of the options are valid ways to confirm that services are using Auto-TLS certificates. A, B, and D are direct methods to verify configuration and certificate usage. C and E can provide indirect evidence or help troubleshoot issues."
  },
  {
    "id": 197,
    "question": "You are configuring Auto-TLS for a multi-tenant CDP clusten Which of the following statements is TRUE\nregarding Auto-TLS and multi- tenancy?",
    "options": {
      "A": "Auto-TLS automatically creates separate certificate authorities (CAs) for each tenant.",
      "B": "Auto-TLS uses a single CA for the entire cluster, regardless of the number of tenants.",
      "C": "Auto-TLS cannot be used in a multi-tenant environment.",
      "D": "Auto-TLS requires manual configuration of certificates for each tenant.",
      "E": "Auto-TLS creates separate namespaces within a single CA for each tenant."
    },
    "correctAnswers": [
      "B"
    ],
    "explanation": "Auto-TLS in CDP on-premise uses a single CA for the entire cluster, regardless of the number of tenants. While separate CAS might be desirable for stronger isolation, Auto-TLS simplifies the process by using a single CA."
  },
  {
    "id": 198,
    "question": "You need to configure Auto-TLS to use a custom certificate authority (CA) instead of the default\nCloudera Manager CA. What steps are required to achieve this?",
    "options": {
      "A": "Import the custom CA certificate into the Cloudera Manager truststore and configure the property to ‘true'.",
      "B": "Replace the default Cloudera Manager CA certificate with the custom CA certificate in the 'Ivar/lib/cloudera-scm-server/truststore.jks’ file.",
      "C": "Configure the and 'cm_auto_tls_custom_ca_private_key’ properties with the paths to the custom CA certificate and private key files, respectively, and set to ‘true'.",
      "D": "There is no option to use custom CA with Auto-TLS, only internal Cloudera Manager CA is supported.",
      "E": "Import the custom CA certificate into the Java truststore of each node in the cluster."
    },
    "correctAnswers": [
      "C"
    ],
    "explanation": ""
  },
  {
    "id": 199,
    "question": "After enabling Auto-TLS, you notice that the Hive Metastore service is unable to connect to the\nHiveServer2 service. The error message indicates a certificate validation failure. You have verified that\nDNS resolution is working correctly. What is the MOST likely cause and how can you resolve it?",
    "options": {
      "A": "The Hive Metastore truststore is not properly configured with the Auto-TLS CA certificate. You need to update the ‘javax.net.ssl.truststore’ and ‘javax.net.ssl.trustStorePassword’ properties in the Hive Metastore configuration.",
      "B": "The Hive Metastore service principal is not correctly configured. You need to ensure that the principal includes the correct hostname.",
      "C": "The HiveServer2 SSL settings are incorrect. You need to verify the 'hive.server2.use.SSL4 and Shive.server2.keystore.patW properties.",
      "D": "The JCE Unlimited Strength Jurisdiction Policy Files are not installed on the Hive Metastore host. Install the JCE files and restart the Hive Metastore service.",
      "E": "The Hive Metastore Kerberos delegation token is expired. Renew the delegation token using ‘hive — service metastore –hiveconf hive.metastore.kerberos.principal=‘."
    },
    "correctAnswers": [
      "A"
    ],
    "explanation": "When Auto-TLS is enabled, the Hive Metastore needs to trust the CA that signed the HiveServer2 certificate. This is achieved by configuring the ‘javax.net.ssl.truststore’ and ‘javax.net.ssl.trustStorePassword' properties in the Hive Metastore configuration to point to the correct truststore containing the Auto-TLS CA certificate."
  },
  {
    "id": 200,
    "question": "After enabling Auto-TLS on your cluster, the LDAP authentication for Hue stopped working. Which of the\nfollowing steps are necessary to fix the problem?",
    "options": {
      "A": "Update Hue's LDAP configuration to use the Auto-TLS CA certificate.",
      "B": "Disable Auto-TLS for Hue, as it is not compatible with LDAP authentication.",
      "C": "Reconfigure the LDAP server to use a non-SSL connection.",
      "D": "Import the LDAP server's certificate into Hue's truststore.",
      "E": "Regenerate the Hue's Kerberos keytab."
    },
    "correctAnswers": [
      "A"
    ],
    "explanation": "When Auto-TLS is enabled, all services, including Hue, must be configured to trust the Auto-TLS CA. This involves updating Hue's LDAP configuration to use the Auto-TLS CA certificate for secure communication with the LDAP server."
  },
  {
    "id": 201,
    "question": "You have enabled Auto-TLS and are using a custom CA. During a rolling restart of the cluster, you\nencounter errors related to certificate validation for newly created nodes. The existing nodes are\nworking fine. What could be the cause?",
    "options": {
      "A": "The custom CA certificate is not installed on the newly added nodes.",
      "B": "The DNS configuration for the new nodes is incorrect.",
      "C": "The Cloudera Manager Agent on the new nodes is using an outdated version.",
      "D": "The property is not set to 'true' on the new nodes.",
      "E": "The time on the new nodes is not synchronized with the rest of the cluster."
    },
    "correctAnswers": [
      "A",
      "E"
    ],
    "explanation": "If a custom CA is being used, it's crucial to ensure that the custom CA certificate is installed on all nodes, including the newly added ones. If the CA is not installed, the new nodes will not trust the certificates issued by that CA. Time synchronization is critical for certificate validation, incorrect time causes validation to fail."
  },
  {
    "id": 202,
    "question": "Which of the following actions will trigger a certificate renewal in a CDP cluster configured with Auto-\nTLS?",
    "options": {
      "A": "Restarting the Cloudera Manager Server.",
      "B": "When certificate reaches its expiry date",
      "C": "Changing the hostname of a node in the cluster.",
      "D": "Adding a new service to the cluster.",
      "E": "Adding a new role to the cluster."
    },
    "correctAnswers": [
      "B",
      "C"
    ],
    "explanation": "Auto-TLS certificates are automatically renewed when they are close to expiring, ensuring continuous security. Changing the hostname of a node invalidates the existing certificate, requiring a renewal for the affected service. Adding a new service also cause new certificate to be issued."
  },
  {
    "id": 203,
    "question": "Your organization requires that all certificates used in the CDP cluster have a specific Subject Alternative\nName (SAN) format. How can you customize the SANs generated by Auto-TLS?",
    "options": {
      "A": "Auto-TLS does not support customization of SANs. The SANs are automatically generated based on the hostname and IP address of the nodes.",
      "B": "Use the property in Cloudera Manager to specify the desired SAN format using a regular expression.",
      "C": "Modify the Auto-TLS script located in S/opt/cloudera/cm/scripts/auto_tls.sh' to generate SANs according to the required format.",
      "D": "Customize the ‘ssl_client.cnf and ‘ssl_server.cnf files in 'letc/pki/tls/openssl.cnf to define the SAN format.",
      "E": "Configure the Kerberos realm to match the desired SAN format."
    },
    "correctAnswers": [
      "A"
    ],
    "explanation": "Auto-TLS does not support the customization of SANs. The SANs are automatically generated based on the hostname and IP address of the nodes. Altering the generation of SANs would require custom scripting and could break the functionality of Auto-TLS."
  },
  {
    "id": 204,
    "question": "After enabling Auto-TLS, you observe that the Namenode is failing to start with a 'java.io.lOException:\nIncompatible clusterlD' error.\nWhat could be the reason and how to fix it?",
    "options": {
      "A": "The format of the Namenode has changed during the Auto-TLS configuration process. You need to reformat the Namenode using 'hdfs namenode -format.",
      "B": "The 'dfs.namenode.shared.edits.dir’ property is not correctly configured. Verify that it points to a valid shared directory accessible by all Namenodes.",
      "C": "The Kerberos keytab for the Namenode is corrupted. Regenerate the keytab using 'kinit -kt and update the 'hadoop.security.key.provider.path' property.",
      "D": "The Auto-TLS configuration has inadvertently altered the Namenode's cluster ID. You need to revert the configuration changes or manually set the property to the correct value in 'hdfs-site.xml' on all nodes.",
      "E": "The HDFS client configuration is not up-to-date. Refresh the client configuration using ‘hdfs dfsadmin - refreshServiceAcl'."
    },
    "correctAnswers": [
      "D"
    ],
    "explanation": "Auto-TLS configuration can sometimes lead to an 'Incompatible clusterlD error if it inadvertently modifies the Namenode's cluster ID. The cluster ID is a unique identifier for the HDFS cluster, and if it's inconsistent across Namenodes, the service will fail to start. You need to identify and revert the configuration changes or manually set the 'dfs.cluster.id' property to the correct value in ‘hdfs-site.xmr on all nodes."
  },
  {
    "id": 205,
    "question": "After enabling Auto-TLS in CDP, you want to use external clients (e.g., beeline, spark-submit) to securely\naccess the cluster services.\nWhat steps are required to ensure these clients can connect successfully?",
    "options": {
      "A": "Distribute the Auto-TLS CA certificate to the client machines and configure the client applications to trust the CA certificate.",
      "B": "Configure the clients to use Kerberos authentication with the appropriate service principals.",
      "C": "Disable TLS/SSL verification on the client applications.",
      "D": "Use the —truststore’ option to configure the client to point to the correct location.",
      "E": "Configure the clients to use the hostnames in letc/hosts file to use secure communication."
    },
    "correctAnswers": [
      "A"
    ],
    "explanation": "For external clients to connect securely to services secured by Auto-TLS, they need to trust the CA that signed the service certificates. This requires distributing the Auto-TLS CA certificate to the client machines and configuring the client applications to trust the CA certificate by specifying the path of trustore. The client applications use the trusted certificates to validate connections to server."
  },
  {
    "id": 206,
    "question": "After enabling Kerberos on your CDP on-premises cluster, users are unable to access HDFS through the\ncommand line, even though their Kerberos tickets are valid. The error message suggests an\nauthentication failure. What is the MOST likely cause?",
    "options": {
      "A": "The 'hadoop.security.authentication' property in ‘core-site.xmr is not set to 'kerberoS.",
      "B": "The user's principal is not correctly mapped to their OS user in HDFS.",
      "C": "The 'hdfs-site.xmr file is missing.",
      "D": "The NameNode is not running.",
      "E": "The user does not have a Kerberos ticket."
    },
    "correctAnswers": [
      "A"
    ],
    "explanation": "The ‘hadoopsecurity.authentication’ property in ‘core-site.xmr MUST be set to 'kerberos’ for Hadoop services to authenticate using Kerberos. If it's missing or set to 'simple' , authentication will fail even with valid Kerberos tickets."
  },
  {
    "id": 207,
    "question": "You are configuring Kerberos for your CDP cluster and need to create a principal for the Hive Metastore\nservice. Which command is the correct way to create a principal with the service keytab in KDC?",
    "options": {
      "A": "B.",
      "C": "D.",
      "E": "Answer: E"
    },
    "correctAnswers": [
      "E"
    ],
    "explanation": "The correct command is ‘kadmin -p admin/admin -w password addprinc hive/ _HOST@YOUR.REALM && ktadd –k letc/security/keytabs/hive.service.keytab This creates the principal with the fully qualified hostname C_HOST is replaced by the hostname) and then uses Sktadd' to add the principal to the service keytab."
  },
  {
    "id": 208,
    "question": "After Kerberizing your CDP cluster, you notice that some YARN applications are failing with\n'Authentication failed' errors. You suspect it's related to delegation tokens. What configuration property\nin ‘yarn-site.xml' directly controls the lifecycle of YARN delegation tokens?",
    "options": {
      "A": "yarn.resourcemanager.keytab'",
      "B": "yarn.resourcemanager.principal'",
      "C": "yarn.resourcemanager.delegation.key.update-intervar",
      "D": "‘yarn.resourcemanager.delegation-token.max-lifetime’",
      "E": "yarn.resourcemanager.delegation-token.renew-intervar"
    },
    "correctAnswers": [
      "D",
      "E"
    ],
    "explanation": "'yarn.resourcemanager.delegation-token.max-lifetime’ controls the maximum lifetime of a delegation token, and yarn.resourcemanager.delegation-token.renew-intervar sets the interval at which tokens can be renewed. These properties are crucial for managing the validity of delegation tokens used for authentication in YARN applications."
  },
  {
    "id": 209,
    "question": "You've configured Kerberos authentication for Hive, and users can successfully query tables. However,\nwhen they attempt to use User\nDefined Functions (UDFs) that access external resources, they encounter permission errors. What is a\npotential cause of this issue related to Kerberos?",
    "options": {
      "A": "The UDFs are not properly configured to use Kerberos delegation tokens.",
      "B": "The UDFs are running with the Hive service principal, which does not have permissions to access the external resources.",
      "C": "The HiveServer2 is not configured to pass the user's Kerberos ticket to the IJDF process.",
      "D": "The external resources are not Kerberized.",
      "E": "The Kerberos ticket cache on the Hive client is outdated."
    },
    "correctAnswers": [
      "A",
      "B"
    ],
    "explanation": "UDFs often need to access resources on behalf of the user. If Kerberos delegation isn't configured correctly, the IJDF might attempt to access the resources using the Hive service principal's credentials, which may not have the necessary permissions. Alternatively, UDFs may not be setup to use delegation tokens."
  },
  {
    "id": 210,
    "question": "What is the PRIMARY role of the 'kinit' command in a Kerberized CDP environment?",
    "options": {
      "A": "To create Kerberos principals in the KDC.",
      "B": "To destroy existing Kerberos tickets.",
      "C": "To obtain and cache a Kerberos ticket-granting ticket (TGT) for a user or service.",
      "D": "To list all available Kerberos principals.",
      "E": "To modify the Kerberos configuration file (krb5.conf)."
    },
    "correctAnswers": [
      "C"
    ],
    "explanation": "The ‘kinit command is used to obtain a Kerberos ticket-granting ticket (TGT) from the KDC. This TGT is then cached and used to request service tickets for accessing Kerberized services."
  },
  {
    "id": 211,
    "question": "You are troubleshooting Kerberos authentication issues with Oozie. You've verified that the Oozie\nservice principal and keytab are correctly configured. However, Oozie workflows that access HDFS are\nstill failing with authentication errors. Which of the following steps are MOST likely to resolve the issue?",
    "options": {
      "A": "Ensure that the 'hadoop.security.authenticatiorf property is set to 'kerberoS in the Oozie server's ‘core-site.xmr file.",
      "B": "Configure Oozie to use delegation tokens for HDFS access.",
      "C": "Ensure the 'oozie.service.HadoopAccessorService.kerberos.principal' and 'oozie.service.HadoopAccessorService.keytab.file’ properties are set correctly in oozie-site.xmr.",
      "D": "Restart the NameNode.",
      "E": "Ensure that the Oozie client has a valid Kerberos ticket."
    },
    "correctAnswers": [
      "B"
    ],
    "explanation": "Oozie needs to be configured to use delegation tokens for HDFS access. Delegation tokens allow Oozie to act on behalf of the user without requiring the user's Kerberos credentials to be passed to the Oozie server."
  },
  {
    "id": 212,
    "question": "You are planning to enable Kerberos on a CDP cluster that includes Kafka’. Which of the following is the\nMOST important consideration regarding Kafka and Kerberos?",
    "options": {
      "A": "Kafka does not support Kerberos authentication.",
      "B": "You must configure Kafka brokers to use SASL/GSSAPI for authentication with Kerberos.",
      "C": "You only need to Kerberize Zookeeper; Kafka will automatically authenticate through Zookeeper.",
      "D": "The Kerberos realm name must match the Kafka cluster ID.",
      "E": "Kafka Connect does not support Kerberos."
    },
    "correctAnswers": [
      "B"
    ],
    "explanation": "Kafka uses SASL/GSSAPI for Kerberos authentication. You need to configure the Kafka brokers to use this mechanism and create appropriate Kerberos principals for the brokers and clients."
  },
  {
    "id": 213,
    "question": "After enabling Kerberos, users complain that Impala queries are slow You suspect the issue is related to\nKerberos overhead. What can you configure to mitigate Kerberos overhead in Impala? Choose two.",
    "options": {
      "A": "Enable short-circuit reads in HDFS.",
      "B": "Configure Impala to use delegation tokens for HDFS access.",
      "C": "Disable Kerberos authentication for Impala.",
      "D": "Increase the Kerberos ticket lifetime.",
      "E": "Enable TLS/SSL encryption for Impala."
    },
    "correctAnswers": [
      "A",
      "B"
    ],
    "explanation": "Delegation tokens allow Impala to access HDFS on behalf of the user without repeatedly authenticating. Short-circuit reads allow Impala to bypass the NameNode for some data access, reducing network traffic and authentication overhead. Increasing the ticket lifetime is not recommanded."
  },
  {
    "id": 214,
    "question": "Which of the following configuration files is used to configure Kerberos client settings on a Linux\nsystem?",
    "options": {
      "A": "/etc/hadoop/conf/core-site.xml",
      "B": "/etc/krb5.conf",
      "C": "/etc/security/limits.conf",
      "D": "/etc/nsswitch.conf",
      "E": "/etc/resolv.conf"
    },
    "correctAnswers": [
      "B"
    ],
    "explanation": "The 'letc/krb5.conf file is the standard configuration file for Kerberos clients. It specifies the KDC, realm, and other settings needed for Kerberos authentication."
  },
  {
    "id": 215,
    "question": "You have a CDP cluster with Kerberos enabled. A user is running a Spark application that needs to access\ndata in Hive. The application fails with an authentication error related to Hive metastore. The user has a\nvalid Kerberos ticket. What additional configuration might be needed for the Spark application to\nsuccessfully authenticate with Hive Metastore?",
    "options": {
      "A": "No additional configuration is needed; a valid Kerberos ticket should be sufficient.",
      "B": "The Spark application needs to be configured to use Hive's delegation token mechanism.",
      "C": "The ‘hive-site.xmr file must be copied to the Spark application's classpath.",
      "D": "The user's principal needs to be granted explicit permissions on the Hive metastore database.",
      "E": "The application must use a Kerberos-enabled Hive JDBC driver and specify the Hive service principal."
    },
    "correctAnswers": [
      "B",
      "C",
      "E"
    ],
    "explanation": "Spark applications often need to be explicitly configured to use Hive's delegation token mechanism. Copying hive-site.xml to Spark's classpath makes Hive configuration available to Spark. Using a Kerberos- enabled Hive JDBC driver configured with service principal ensures Spark can properly authenticate to the Metastore."
  },
  {
    "id": 216,
    "question": "After enabling Kerberos, users report that they can no longer submit jobs through the command line to\na secured YARN cluster, even though they have valid Kerberos tickets. The error message indicates a\nfailure to obtain a delegation token. Which of the following properties should you examine or configure\nin 'yarn-site.xmP to address this issue? Select all that apply.",
    "options": {
      "A": "yarn.resourcemanager.principar",
      "B": "‘yarn.resourcemanager.keytab'",
      "C": "yarn.nodemanager.aux-services’",
      "D": "‘yarn.resourcemanager.delegation.key.update-intervar",
      "E": "yarn.nodemanager.container-executor.class'"
    },
    "correctAnswers": [
      "A",
      "B",
      "D"
    ],
    "explanation": "‘yarn.resourcemanager.principar and ‘yarn.resourcemanager.keytab' define the Kerberos identity for the ResourceManager. yarn.resourcemanager.delegation.key.update-intervar controls how frequently the ResourceManager updates its delegation key, influencing the availability of delegation tokens. If the principal or keytab are incorrect, or the update interval is too long, delegation tokens may not be properly generated or available. ‘yarn.nodemanager.aux-services’ and 'yarn.nodemanager.container- executor.class' are related to other aspects of YARN and don't directly impact delegation token generation."
  },
  {
    "id": 217,
    "question": "You are configuring Kerberos for a CDP cluster and wish to automate the keytab generation and\nprincipal creation process. You have access to a configuration management tool. What is the most\nsecure and recommended approach for storing the Kerberos administrator password used by the\nautomation script?",
    "options": {
      "A": "Store the password in plain text in the automation script.",
      "B": "Store the password in an environment variable.",
      "C": "Store the password encrypted within the configuration management tool's secret management system (e.g., Vault, HashiCorp Vault, Ansible Vault).",
      "D": "Store the password in a file with restricted permissions (e.g., 400).",
      "E": "Store the password hashed using MD5."
    },
    "correctAnswers": [
      "C"
    ],
    "explanation": "The most secure approach is to store the Kerberos administrator password encrypted within a dedicated secret management system like Vault or Ansible Vault. These systems provide secure storage, access control, and auditing for sensitive information. Storing the password in plain text, an environment variable, or a file with restricted permissions is insecure. MD5 is an outdated and insecure hashing algorithm."
  },
  {
    "id": 218,
    "question": "You are tasked with implementing Kerberos authentication for your CDP on-premises cluster. Which of\nthe following steps are essential for configuring users and groups within Cloudera Manager after you\nhave successfully installed and configured the Kerberos Key Distribution Center (KDC)?",
    "options": {
      "A": "Create principals for Cloudera Manager, each service, and user accounts using 'kadmin.locar or a similar tool. Ensure each principal has a corresponding keytab file.",
      "B": "Configure the 'krb5.conf file on all nodes in the cluster to point to the KDC server.",
      "C": "Import users and groups from an existing LDAP directory to Cloudera Manager.",
      "D": "Manually create local Linux user accounts on all nodes in the cluster that match the Kerberos principals.",
      "E": "Enable Kerberos authentication via the Cloudera Manager Admin Console and provide the necessary KDC details (Realm, KDC host)."
    },
    "correctAnswers": [
      "A",
      "B",
      "E"
    ],
    "explanation": "Creating Kerberos principals and keytabs (A), configuring krb5.conf (B), and enabling Kerberos via Cloudera Manager (E) are the core steps. While LDAP import can be used (C), it's not always essential, particularly if the KDC is the primary source of user authentication. Manual user creation on each node (D) is generally not necessary when using Kerberos."
  },
  {
    "id": 219,
    "question": "You want to grant a specific user, 'data analyst', read-only access to data in a specific Hive table, 'sales\ndata', within the 'marketing' database. Assuming Kerberos is enabled, what is the most secure and\nappropriate way to achieve this using Hive authorization (grant privileges)?",
    "options": {
      "A": "GRANT SELECT ON TABLE marketing.sales_data TO USER data_analyst;",
      "B": "GRANT ALL ON TABLE marketing.sales_data TO USER data_analyst;",
      "C": "GRANT READ ON TABLE marketing.sales_data TO USER data_analyst;",
      "D": "GRANT SELECT ON DATABASE marketing TO USER data_analyst;",
      "E": "GRANT SELECT ON TABLE sales_data TO USER data_analyst;"
    },
    "correctAnswers": [
      "A"
    ],
    "explanation": "The correct syntax for granting read-only access (SELECT privilege) on a specific table in Hive is 'GRANT SELECT ON TABLE database_name.table_name TO USER user_name;'. Option B grants ALL privileges, which is not read-only. Option C is invalid Hive syntax. Option D grants SELECT on the entire database, which is broader than necessary. Option E misses the database name, so Hive won't know from which database to get table"
  },
  {
    "id": 220,
    "question": "You are auditing user access within your CDP on-premises cluster. You need to identify all users\nbelonging to the 'finance' group who have 'ADMIN' role assigned in Ranger for HDFS. How can you\nefficiently retrieve this information? Assume you have access to the Ranger Admin UIand the command-\nline interface.",
    "options": {
      "A": "Use the Ranger Admin UI to filter users by group 'finance' and then manually inspect each user's role assignments for HDFS policies.",
      "B": "Use the Ranger REST API to retrieve all users and their group memberships, then filter the results programmatically to identify users in the 'finance' group and their HDFS roles.",
      "C": "Use the Ranger Admin UIto search for the 'ADMIN' role assignment in the HDFS policies and then filter the results to identify users belonging to the 'finance' group.",
      "D": "Query the Ranger database directly using SQL to retrieve user-group mappings and role assignments for HDFS.",
      "E": "Manually review the HDFS ACLs to determine effective permissions for the 'finance' group."
    },
    "correctAnswers": [
      "B",
      "D"
    ],
    "explanation": "The Ranger REST API (B) allows for programmatic retrieval and filtering of user and group information, providing efficiency. Directly querying the Ranger database (D) allows a DBA or admin to perform complex queries, joining tables to find user-group mappings and role assignments related to HDFS. The Ranger UI is possible (A, C) but less efficient for auditing, and HDFS ACLs (E) don't directly correlate with Ranger roles."
  },
  {
    "id": 221,
    "question": "After integrating LDAP with Cloudera Manager, you notice that new users added to LDAP are not\nautomatically recognized in Cloudera Manager. What configuration setting is most likely causing this\nissue, and where would you configure it?",
    "options": {
      "A": "The 'Sync Interval' for LDAP synchronization in Cloudera Manager's LDAP configuration is set too high.",
      "B": "The 'User Base DN' in Cloudera Manager's LDAP configuration is incorrect, preventing the discovery of new user entries.",
      "C": "The 'Group Membership Attributes' in the LDAP configuration are not correctly mapped to the corresponding LDAP attributes.",
      "D": "Kerberos authentication is enabled and conflicting with the LDAP configuration.",
      "E": "The 'External Account Mapping' section is misconfigured, so new accounts are unable to be created."
    },
    "correctAnswers": [
      "A",
      "B"
    ],
    "explanation": "The 'Sync Interval' (A) determines how frequently Cloudera Manager synchronizes with the LDAP server. If set too high, new users won't be recognized promptly. The 'User Base DN' (B) specifies the starting point for searching for user entries in the LDAP directory. An incorrect Base DN will prevent the discovery of new users. Group membership (C) impacts group synchronization but not necessarily user discovery. Kerberos and LDAP can coexist (D). External account mapping isn't related to syncing(E)."
  },
  {
    "id": 222,
    "question": "You are configuring Ranger policies for a new data lake in CDP on-premises. You want to implement a\nrow-level filtering policy on a Hive table 'customer_data' based on the 'country' column. Only users in\nthe 'eu_analysts' group should be able to see data where 'country' is 'Germany', 'France', or 'Spain'. How\nwould you define the Ranger policy condition?",
    "options": {
      "A": "country IN ('Germany', 'France', 'Spain') AND usergroup='eu_analysts'",
      "B": "country IN ('Germany', 'France', 'Spain')",
      "C": "ROW FILTER = 'country IN ('Germany', 'France', 'Spain') AND inGroup(\"eu_analysts\")'",
      "D": "ROW FILTER = 'country IN (\"Germany\", \"France\", \"Spain\")' && usergroup(\"eu_analysts\")'",
      "E": "ROW FILTER = 'country IN (\"Germany\", \"France\", \"Spain\") AND in_group(\"eu_analysts\")'"
    },
    "correctAnswers": [
      "E"
    ],
    "explanation": "The correct syntax for row-level filtering in Ranger uses the property, and incorporates the function to check group membership. Also, the strings need to be quoted correctly with double quotes (E). Option A is missing the ROW FILTER syntax. Options B do not take the group consideration. The other Options has incorrect syntaxes and the quotations are incorrect as well."
  },
  {
    "id": 223,
    "question": "A user is complaining that they cannot access HDFS, even though they belong to a group that has been\ngranted access via Ranger policies. You have confirmed that the user is indeed a member of the correct\ngroup and that the Ranger policy is active. What is the most likely reason for this access denial?",
    "options": {
      "A": "The user's Kerberos ticket has expired.",
      "B": "The HDFS NameNode is in safemode.",
      "C": "The user's group membership information has not been synchronized with the HDFS NameNode.",
      "D": "The Ranger plugin for HDFS is disabled or misconfigured.",
      "E": "The user's account is locked out in the KDC"
    },
    "correctAnswers": [
      "C"
    ],
    "explanation": "Even with correct Ranger policies, HDFS needs to be aware of the user's group memberships. If this information is out of sync, access will be denied. Kerberos ticket expiration (A) would prevent any Kerberos-authenticated access, not just HDFS. Safemode (B) would affect all users. A disabled Ranger plugin (D) would likely prevent any Ranger-controlled access. KDC lockout (E) prevent any Kerberos- authenticated access, not just HDFS"
  },
  {
    "id": 224,
    "question": "You have a requirement to limit the resources (CPU, memory) that a specific group of users, 'etl_users',\ncan consume when submitting Spark jobs on YARN. How would you configure this resource limitation\nwithin Cloudera CDP on-premises?",
    "options": {
      "A": "Configure YARN queues with resource limits and assign the 'etl_users' group to a specific queue with restricted resources.",
      "B": "Configure Linux cgroups to limit the CPU and memory usage for processes owned by members of the 'etl_users' group.",
      "C": "Implement Ranger policies to restrict the maximum CPU and memory allocated to Spark applications submitted by 'etl_users'.",
      "D": "Modify the Spark configuration files (e.g., spark-defaults.conf) to set default resource limits for jobs submitted by 'etl_users'.",
      "E": "Implement resource quotas for HDFS directories owned by the 'etl_users' group."
    },
    "correctAnswers": [
      "A"
    ],
    "explanation": "YARN queues (A) are the standard mechanism for resource management in Hadoop. By configuring queues with limits and assigning users to those queues, you can control their resource consumption. Linux cgroups (B) are a lower-level system and not directly integrated with YARN. Ranger (C) is primarily for authorization, not resource management. Spark configuration (D) can set defaults, but this is not group-specific resource limitation. HDFS quotas (E) limit storage space, not CPU or memory."
  },
  {
    "id": 225,
    "question": "You are setting up a new CDP on-premises cluster with Kerberos enabled. You need to create a service\nprincipal and keytab for the HTTP endpoint of the YARN Timeline Service v2. What is the correct\ncommand to create this principal using 'kadmin.locar , assuming the hostname of the node is\n'yarn.example.com' and the Kerberos realm is 'EXAMPLE.COM'?",
    "options": {
      "A": "addprinc -randkey HTTP/yarn.example.com@EXAMPLE.COM",
      "B": "addprinc HTTP/yarn.example.com@EXAMPLE.COM",
      "C": "createprinc -randkey HTTP@yarn.example.com",
      "D": "add_principal HTTP/yarn.example.com",
      "E": "create -principal HTTP/yarn.example.com@EXAMPLE.COM -random_key"
    },
    "correctAnswers": [
      "A"
    ],
    "explanation": "The correct command is ‘addprinc -randkey HTTP/yarn.example.com@EXAMPLE.COMS. The ‘addprinc' command creates a new Kerberos principal. '-randkey' generates a random key for the principal. The principal name follows the format ‘service/hostname@REALM' Option B is missing the -randkey. All other options used wrong commands"
  },
  {
    "id": 226,
    "question": "You need to configure access control for Kafka topics using Ranger. Specifically, you want to allow users\nin the 'kafka_producers' group to produce messages to topics prefixed with 'events-', and users in the\n'kafka_consumers' group to consume messages from those same topics. Which Ranger policies would\nyou create?",
    "options": {
      "A": "Option A",
      "B": "Option B",
      "C": "Option C",
      "D": "Option D",
      "E": "Option E"
    },
    "correctAnswers": [
      "A"
    ],
    "explanation": "Ranger for Kafka uses specific access types: 'produce' for writing messages and 'consume' for reading messages. You need separate policies (A) to grant these specific permissions to the respective groups. 'write' and 'read' (B) are not the correct access types. 'publish' and 'subscribe' (C) are commonly related to messaging but are not the specific terms Ranger uses for Kafka. Granting broad access and then revoking (D) is less secure and harder to manage. Combining permissions in a single policy (E) is generally not recommended for separation of concems."
  },
  {
    "id": 227,
    "question": "You are trying to troubleshoot why a specific user, 'analystl', cannot access a table in Impala even\nthough you've granted them SELECT privileges via Ranger. You suspect that the user's privileges are not\nbeing properly propagated to Impala’. Which steps can you take to verify the Ranger-Impala integration\nand refresh the user's privileges?",
    "options": {
      "A": "Restart the Impala catalog server to force a refresh of Ranger policies.",
      "B": "Run the 'INVALIDATE METADATA' command in Impala to refresh the metadata cache.",
      "C": "Run the 'REFRESH AUTHORIZATION' command in Impala to reload Ranger policies.",
      "D": "Verify that the Ranger Impala plugin is enabled and configured correctly in Cloudera Manager.",
      "E": "Ensure that the Impala user is synced in the Ranger plugin by running 'SYNC USER analystl command in Impala shell."
    },
    "correctAnswers": [
      "C",
      "D"
    ],
    "explanation": "'REFRESH AUTHORIZATION' (C) in Impala specifically reloads Ranger policies, ensuring Impala has the latest privileges. Verifying the Ranger plugin configuration (D) ensures the integration is functioning correctly. Restarting the catalog server (A) might help in some cases, but it's not the most direct solution. 'INVALIDATE METADATA (B) refreshes metadata, not authorization. Impala does not have a ‘SYNC USER command (E)."
  },
  {
    "id": 228,
    "question": "After implementing Kerberos authentication, you notice that some legacy applications, which are not\nKerberos-aware, need to access HDFS. How can you provide access to these applications without\ncompromising the overall security of the cluster?",
    "options": {
      "A": "Disable Kerberos authentication temporarily to allow the legacy applications to access HDFS.",
      "B": "Create a dedicated user account with simplified authentication (e.g., password-based) for the legacy applications to use.",
      "C": "Use Knox to provide a secure gateway for the legacy applications to access HDFS via REST APIs with authentication.",
      "D": "Configure the legacy applications to directly access HDFS using the HDFS client libraries without Kerberos.",
      "E": "Implement Delegation Tokens. The token can be acquired through Kerberos and then used to authenticate non-kerberized applications."
    },
    "correctAnswers": [
      "C",
      "E"
    ],
    "explanation": "Knox (C) provides a secure gateway, allowing authentication and authorization before accessing HDFS resources via REST. Using delegation tokens (E) offers a secure alternative by allowing non-Kerberos applications to authenticate with a token obtained via Kerberos. Disabling Kerberos (A) is a major security risk. Simplified authentication (B) is also less secure. Direct access without Kerberos (D) circumvents the security setup."
  },
  {
    "id": 229,
    "question": "You have a situation where several users are inadvertently locking Hive tables, causing performance\nissues. You want to implement a mechanism to automatically kill Hive queries that have been holding\nlocks for an excessive period. How can you achieve this using configuration settings?",
    "options": {
      "A": "Set the ‘hive.lock.expiration’ property in 'hive-site.xmr to a short duration (e.g., 60 seconds) to automatically release locks after that time.",
      "B": "Configure the Hive Metastore to use an external database that supports lock timeouts, and then configure the database's lock timeout settings.",
      "C": "Write a custom script that periodically queries the Hive Metastore database to identify long-held locks and then kills the corresponding Hive queries using the Hive CLI.",
      "D": "Set the \"hive.txn.timeout property in 'hive-site.xml' to control how long a transaction can remain open before being automatically aborted.",
      "E": "Set the Ranger policies that have time-based restrictions of access to the data, so no user can accidentally lock Hive tables for a long time."
    },
    "correctAnswers": [
      "B",
      "D"
    ],
    "explanation": "Hive's lock management relies on the underlying Metastore database. Configuring the database to use lock timeouts (B) is the most effective approach. Set the 'hive.txn.timeout' property in 'hive-site.xmr to control how long a transaction can remain open before being automatically aborted.(D) 'hive.lock.expiration’ (A) is deprecated. A custom script (C) is complex and unnecessary. Ranger (E) can control access but doesn't directly manage lock timeouts."
  },
  {
    "id": 230,
    "question": "You have enabled TLS for all services in your Cloudera CDP on-premise cluster. After the cluster restart,\nthe Hue service fails to start, and the logs show SSL handshake errors. What is the most likely cause?",
    "options": {
      "A": "The Hue service does not support TLS encryption.",
      "B": "The Hue service is configured to use a different truststore or keystore than the other services.",
      "C": "The TLS certificates used for the Hue service have expired.",
      "D": "The Hue service is configured to use an unsupported TLS protocol version.",
      "E": "The TLS encryption was not enabled for all nodes where Hue runs."
    },
    "correctAnswers": [
      "B"
    ],
    "explanation": "If Hue fails with SSL handshake errors after enabling TLS, it often indicates that it's using a different truststore/keystore, or that these are incorrectly configured. Ensure Hue is using the correct, updated truststore and keystore with valid certificates. Incorrect TLS protocol version or incomplete TLS enablement would typically affect other services as well."
  },
  {
    "id": 231,
    "question": "You are configuring encryption in transit for your Cloudera Data Platform (CDP) cluster using Cloudera\nManager. Which of the following components are essential for a successful TLS configuration?",
    "options": {
      "A": "A valid Certificate Authority (CA) certificate.",
      "B": "Correctly configured Keystores and Truststores on all nodes.",
      "C": "The correct TLS/SSL protocol version enabled for each service.",
      "D": "Firewall rules allowing communication on the encrypted ports.",
      "E": "Kerberos authentication configured for all services"
    },
    "correctAnswers": [
      "A",
      "B",
      "C",
      "D"
    ],
    "explanation": "All options are crucial for a successful TLS configuration. A CA certificate is needed to sign the service certificates. Correct Keystore and Truststore setup ensures identity and trust. Proper TLS/SSL protocol version management (e.g., TLSv1.2 or higher) is crucial for security, and correctly configured firewall rules allow communication on encrypted ports. Kerberos is for authentication, not encryption."
  },
  {
    "id": 232,
    "question": "You have enabled TLS for all services, but now MapReduce jobs are failing. The error logs indicate a\nproblem with hostname verification. How would you address this issue, assuming the certificates are\nvalid and correctly configured?",
    "options": {
      "A": "Disable hostname verification globally in the cluster.",
      "B": "Update the 'hadoop.ssl.hostname.verifier’ property in the ‘core-site.xml' to allow wildcard matching of hostnames.",
      "C": "Ensure the hostname in the certificate matches the hostname used by the MapReduce client when connecting to the ResourceManager.",
      "D": "Add the following to ‘yarn-site.xmr: ‘yarn.nodemanager.ssl.enabled=false’ .",
      "E": "Modify '/etc/hosts’ to include all possible names each server might be known by."
    },
    "correctAnswers": [
      "C"
    ],
    "explanation": "Hostname verification failure suggests that the hostname used by the client doesn't match the hostname in the server's certificate. The correct solution is to ensure consistency between the hostname in the certificate and the hostname used by the client during connection. Disabling hostname verification (A) is insecure. Option (B) is a poor practice, though it could technically work if absolutely necessary. Option (D) would disable SSL on the Nodemanager (not the correct answer). Adding to '/etc/hosts’ might work, but isn't reliable in all environments."
  },
  {
    "id": 233,
    "question": "Which of the following configuration parameters is most directly responsible for enabling TLS encryption\nfor communication between the DataNodes in HDFS?",
    "options": {
      "A": "dfs.http.policy",
      "B": "dfs.namenode.https-address",
      "C": "dfs.datanode.address",
      "D": "dfs.datanode.https.address",
      "E": "hadoop.security.authentication"
    },
    "correctAnswers": [
      "D"
    ],
    "explanation": "'dfs.datanode.https.address’ is the configuration parameter that specifies the HTTPS address that the DataNodes use to communicate with each other and the NameNode for secure data transfer. ‘dfs.http.policy’ enforces HTTP or HTTPS policies, 'dfs.namenode.https-address’ defines the NameNode's HTTPS address, 'dfs.datanode.address’ is for unencrypted communication, and 'hadoop.security.authentication’ is related to Kerberos."
  },
  {
    "id": 234,
    "question": "You need to rotate the TLS certificates in your Cloudera CDP cluster without causing any downtime.\nWhat steps should you take?",
    "options": {
      "A": "Replace the certificates and restart the entire cluster at once.",
      "B": "Replace the certificates and restart each service role one at a time in a rolling fashion, using Cloudera Manager.",
      "C": "Update the certificates in Cloudera Manager, and the changes will automatically propagate without any restarts.",
      "D": "Replace the certificates on each host manually and restart the Cloudera Manager Agent on each host.",
      "E": "Place the new certificates in the HDFS and point all services to that shared location, and then restart the services"
    },
    "correctAnswers": [
      "B"
    ],
    "explanation": "To rotate certificates with minimal downtime, you should replace the certificates in Cloudera Manager, then perform a rolling restart of each service role. This allows services to continue operating while others are being updated. Restarting the entire cluster would cause downtime. Cloudera Manager doesn't automatically propagate certificates without service restarts. Manually replacing certificates and restarting agents is less efficient and error-prone. Placing the certificates in HDFS is not a common or recommended practice."
  },
  {
    "id": 235,
    "question": "You are setting up TLS encryption for a new Kafka cluster in CDR You have generated the necessary\ncertificates and configured the listeners. However, the Kafka brokers fail to communicate with each\nother. Upon inspection, you find that the broker.id is not included as a Subject Alternative Name (SAN)\nin the broker certificates. What is the consequence and how should you solve this?",
    "options": {
      "A": "The Kafka brokers will use the internal IP addresses for communication instead, which may cause connectivity issues. Regenerate the certificates including the broker.id as a SAN.",
      "B": "This is not an issue as long as the Common Name (CN) matches the hostname. No action is needed.",
      "C": "The Kafka brokers will be vulnerable to man-in-the-middle attacks. Disable TLS to solve this problem.",
      "D": "This will cause high CPU usage on the brokers due to constant certificate verification failures. Regenerate the certificates without the broker.id as a SAN.",
      "E": "The communication will be unencrypted and vulnerable."
    },
    "correctAnswers": [
      "A"
    ],
    "explanation": "Kafka requires the broker.id to be included as a SAN (Subject Alternative Name) in the broker certificates to ensure proper TLS communication between brokers. If the broker.id is missing, the brokers might attempt to use internal IP addresses, leading to connectivity problems, especially in multi-homed environments. You must regenerate the certificates to include the broker.id as a SAN. Disabling TLS or ignoring the issue are not viable solutions."
  },
  {
    "id": 236,
    "question": "Your organization mandates that all network communication must use TLS 1.3. How do you ensure that\nall components in your CDP cluster adhere to this requirement, given that some older components\nmight support older TLS versions?",
    "options": {
      "A": "Configure Cloudera Manager to only allow TLS 1.3 in its settings. Cloudera Manager will then enforce this setting across all components.",
      "B": "Configure the ‘ssl_protocols' parameter in the relevant configuration files (e.g., 'ssl-server.xml', 'core- site.xml') for each service to only include TLSv1.3. Example: ‘ssl_protocolsTLSv1.3'.",
      "C": "Upgrade all components to the latest versions. These should automatically use TLS 1.3.",
      "D": "Disable all older TLS versions (TLS 1.0, TLS 1.1, TLS 1.2) at the operating system level by modifying the system-wide SSL configuration.",
      "E": "Configure the Java Cryptography Extension (JCE) policy files to disable older TLS versions."
    },
    "correctAnswers": [
      "B",
      "E"
    ],
    "explanation": "To enforce TLS 1.3, you need to explicitly configure each service to only use that version. This is typically done by modifying the service-specific configuration files (B). Additionally, you could configure the JCE policy files to disable older TLS versions, which offers a more systemic approach and ensures no older versions are used by any Java application. While upgrading components (C) can help, it doesn't guarantee that older TLS versions are disabled. Disabling at the OS level (D) might impact other applications. Cloudera Manager (A) doesn't inherently provide a global setting to force TLS versions for all components."
  },
  {
    "id": 237,
    "question": "After enabling TLS, you notice that performance has degraded significantly in your HDFS cluster. What\nare the potential causes of this performance degradation related to TLS encryption?",
    "options": {
      "A": "Increased CPU overhead due to encryption and decryption.",
      "B": "Increased network latency due to larger packet sizes with TLS headers.",
      "C": "Insufficient memory allocated for the Java Cryptography Extension (JCE).",
      "D": "Inefficient configuration of the cipher suites used for TLS.",
      "E": "Lack of hardware acceleration for cryptographic operations."
    },
    "correctAnswers": [
      "A",
      "D",
      "E"
    ],
    "explanation": "TLS encryption introduces CPU overhead for encryption and decryption (A). Inefficiently configured cipher suites can also contribute to performance problems (D). Lack of hardware acceleration for cryptographic operations means that the CPU bears the entire burden of encryption, which can significantly slow down performance (E). While increased packet sizes and memory issues might have a small impact, the primary reasons are CPU overhead, inefficient cipher suites, and lack of hardware acceleration."
  },
  {
    "id": 238,
    "question": "You are tasked with configuring TLS for your Cloudera CDP cluster. You have a central Certificate\nAuthority (CA). What is the recommended approach for managing certificates across the cluster?",
    "options": {
      "A": "Manually generate and distribute certificates to each service and node.",
      "B": "Use Cloudera Manager's built-in certificate generation and management tools to generate and deploy certificates signed by the central CA.",
      "C": "Configure each service to trust the central CA's root certificate and then request certificates directly from the CA.",
      "D": "Create a self-signed certificate for each service, then import each of these into a central truststore.",
      "E": "Copy the same certificate and key pair to all services."
    },
    "correctAnswers": [
      "B"
    ],
    "explanation": "Cloudera Manager provides built-in tools for generating and managing certificates, including signing them with a central CA. This simplifies the process and ensures consistency. Manually managing certificates (A) is error-prone and difficult to maintain. While configuring each service to trust the CA and request certificates directly (C) is possible, it's more complex and not directly supported by Cloudera Manager's tooling. Self-signed certificates (D) are not ideal for production environments. Using the same certificate for all services (E) is a security risk."
  },
  {
    "id": 239,
    "question": "After enabling encryption in transit on a CDP cluster, you are observing connection timeout issues. The\nclients are unable to connect to the services. What are the possible causes for these timeouts related to\nthe new encryption?",
    "options": {
      "A": "Firewall rules are blocking the encrypted traffic ports.",
      "B": "The TLS handshake process is taking longer than the client's timeout settings.",
      "C": "The client and server are using incompatible cipher suites.",
      "D": "The client's truststore does not contain the CA certificate used to sign the server's certificate.",
      "E": "Kerberos authentication is conflicting with the SSL settings"
    },
    "correctAnswers": [
      "A",
      "B",
      "C",
      "D"
    ],
    "explanation": "Several factors can contribute to connection timeout issues after enabling TLS. Firewall rules (A) might be blocking the new ports used for encrypted traffic. The TLS handshake (B) can take longer than the client's timeout if resources are limited or the network is slow. Incompatible cipher suites (C) between the client and server will prevent a successful connection. If the client doesn't trust the CA that signed the server's certificate (D), the handshake will fail. Option E (Kerberos) is not correct, since timeouts would not be impacted by Kerberos."
  },
  {
    "id": 240,
    "question": "You want to verify that TLS encryption is working correctly for HDFS data transfer. Which of the\nfollowing methods can you use?",
    "options": {
      "A": "Use ‘tcpdump’ or 'Wireshark’ to capture network traffic and analyze the packets to ensure they are encrypted.",
      "B": "Check the HDFS service logs for messages indicating successful TLS handshake and data transfer.",
      "C": "Use the ‘openssl s_client’ command to connect to the DataNodes and NameNode and verify the certificate information.",
      "D": "Monitor the CPU usage on DataNodes, an increase in CPU implies an active encryption process.",
      "E": "Review the audit logs in Ranger for encrypted connections."
    },
    "correctAnswers": [
      "A",
      "B",
      "C"
    ],
    "explanation": "You can use 'tcpdump’ or 'Wireshark' (A) to capture and analyze network traffic, confirming that the packets are encrypted. Checking the HDFS service logs (B) can reveal messages about successful TLS handshakes and data transfers. ‘openssl s_client’ (C) allows you to connect to the DataNodes and NameNode and inspect the certificate information, ensuring it is valid. While increased CPU usage (D) might indicate encryption, it's not a direct verification method, it could be due to other processes. While Ranger does perform access control for secure services, TLS isn't logged or audited in the same way."
  },
  {
    "id": 241,
    "question": "A user reports that they are unable to connect to the HiveServer2 JDBC endpoint using a client\napplication. You suspect that TLS configuration on the client side may be incorrect. Which of the\nfollowing JDBC connection string parameters are most relevant for troubleshooting TLS connectivity?",
    "options": {
      "A": "‘ssl=true’",
      "B": "‘sslTrustStore=‘",
      "C": "‘trustStorePassword=‘",
      "D": "‘principal=‘",
      "E": "‘useSSL=trues"
    },
    "correctAnswers": [
      "A",
      "B",
      "C",
      "E"
    ],
    "explanation": "For TLS connectivity in JDBC, ‘ssl=true’ or ‘useSSL=true' (A, E) enables SSL/TLS. 'sslTrustStore=' (B) specifies the location of the truststore file containing the CA certificate, and ‘trustStorePassword=' (C) provides the password for the truststore. The ‘principal=‘ parameter is related to Kerberos authentication, not TLS encryption."
  },
  {
    "id": 242,
    "question": "You are tasked with installing Atlas in a Cloudera Data Platform (CDP) on-premises environment. Which\nof the following statements regarding the installation process are accurate?",
    "options": {
      "A": "Atlas requires a dedicated ZooKeeper ensemble, separate from other CDP services.",
      "B": "Atlas metadata is stored within the Hive Metastore by default and cannot be changed during installation.",
      "C": "The Atlas service can be co-located on the same nodes as other services like Hive or Impala, as long as resource contention is carefully managed.",
      "D": "During installation, you must specify a fully qualified domain name (FQDN) for each Atlas server node.",
      "E": "Atlas installation requires that a supported database (e.g., PostgreSQL, MySQL) is pre-configured and accessible."
    },
    "correctAnswers": [
      "C",
      "D",
      "E"
    ],
    "explanation": "Atlas can be co-located with other services if resources are managed properly. FQDNs are necessary for proper inter-service communication. Atlas requires a backend database to store its metadata."
  },
  {
    "id": 243,
    "question": "After installing Atlas, you need to verify its functionality. Which command-line tool is typically used to\ninteract with the Atlas API and perform basic metadata operations?",
    "options": {
      "A": "cmctl",
      "B": "atlas-cli",
      "C": "curl",
      "D": "hdfs dfs",
      "E": "beeline"
    },
    "correctAnswers": [
      "C"
    ],
    "explanation": "While there isn't a dedicated 'atlas-cli' tool, 'curl' is commonly used to interact with Atlas's REST API, allowing for metadata retrieval, creation, and updates."
  },
  {
    "id": 244,
    "question": "You want to create a new custom classification in Atlas to tag sensitive data’. Which Atlas API endpoint\nshould you use to define this classification?",
    "options": {
      "A": "/api/atlas/v2/types/typedefs",
      "B": "/api/atlas/v2/entity",
      "C": "/api/atlas/v2/lineage",
      "D": "/api/atlas/v2/glossary",
      "E": "/api/atlas/v2/types"
    },
    "correctAnswers": [
      "A"
    ],
    "explanation": "The 'lapi/atlas/v2/types/typedefs' endpoint is used to create and manage type definitions, including classifications, entity types, and enums in Atlas."
  },
  {
    "id": 245,
    "question": "Given the following JSON payload, which represents a custom classification, what is missing to make it a\nvalid Atlas classification?",
    "options": {
      "A": "The’super Types’ array is missing, specifying inheritance.",
      "B": "The ‘attributeDefs’ array is missing, defining attributes for the classification.",
      "C": "The ‘name’ field is improperly formatted.",
      "D": "The ‘serviceType’ field is missing.",
      "E": "The ‘category' field is missing or invalid."
    },
    "correctAnswers": [
      "B"
    ],
    "explanation": "A valid Atlas classification requires the 'attributeDefs' array to define any attributes associated with the classification. Without this, the classification is not properly defined and cannot be used effectively."
  },
  {
    "id": 246,
    "question": "You have defined a classification named 'PII' in Atlas. How can you apply this classification to a Hive\ntable named 'customers' in the 'sales' database using HiveQL?",
    "options": {
      "A": "COMMENT ON TABLE sales.customers IS 'CLASSIFIED: Pill;",
      "B": "ALTER TABLE sales.customers SET TBLPROPERTIES ('atlas.classification'",
      "C": "ALTER TABLE sales.customers ADD CLASSIFICATION PII;",
      "D": "CREATE CLASSIFICATION PII ON TABLE sales.customers;",
      "E": "ALTER TABLE sales.customers SET SERDEPROPERTIES ('atlas.classification' = 'Pill);"
    },
    "correctAnswers": [
      "B"
    ],
    "explanation": "The correct HiveQL command to apply an Atlas classification is using ALTER TABLE SET TBLPROPERTIES ('atlas.classification' = 'classification_name');’ This sets a table property that Atlas recognizes and uses to associate the classification with the table."
  },
  {
    "id": 247,
    "question": "Which of the following configurations directly impacts Atlas's ability to ingest metadata from Hive?",
    "options": {
      "A": "hive.hook.proto.validate.fully.materialized=true",
      "B": "hive.metastore.uris",
      "C": "atlas.authentication.method=kerberos",
      "D": "atlas.jaas.KafkaClient.option.serviceName",
      "E": "atlas.enable.hive.hook=true"
    },
    "correctAnswers": [
      "B",
      "E"
    ],
    "explanation": "'hive.metastore.uriS specifies the location of the Hive Metastore, which Atlas needs to connect to. ‘atlas.enable.hive.hook=true' enables the Hive hook, allowing Hive to send metadata updates to Atlas."
  },
  {
    "id": 248,
    "question": "You have enabled Atlas integration with Ranger. What is the primary benefit of this integration in a CDP\non-premises environment?",
    "options": {
      "A": "Automatically encrypting data at rest in HDFS.",
      "B": "Synchronizing Ranger policies with Atlas classifications, enabling policy enforcement based on metadata tags.",
      "C": "Automatically backing up Atlas metadata to a remote cloud storage service.",
      "D": "Creating audit logs for all data access events directly within Atlas.",
      "E": "Allowing Atlas to directly manage user authentication and authorization."
    },
    "correctAnswers": [
      "B"
    ],
    "explanation": "The key benefit of Ranger and Atlas integration is the ability to create Ranger policies based on Atlas classifications. This allows for dynamic policy enforcement based on metadata."
  },
  {
    "id": 249,
    "question": "After a recent upgrade, the Atlas IJI is not accessible. You suspect a problem with the Atlas server. How\nwould you check the status of the Atlas service using Cloudera Manager?",
    "options": {
      "A": "Use the ‘atlas-stop' and ‘atlas-start’ commands to restart the service.",
      "B": "Navigate to the Atlas service in Cloudera Manager, check the service status, and review any recent alerts or logs.",
      "C": "Execute ‘yarn application -list to see if the Atlas application is running.",
      "D": "Check the ZooKeeper nodes for the Atlas service to verify its availability.",
      "E": "Review the 'atlas. log' file in the /var/log/atlas directory for error messages."
    },
    "correctAnswers": [
      "B",
      "E"
    ],
    "explanation": "Cloudera Manager provides a centralized view of service status, alerts, and logs. Checking the specific Atlas log files can provide granular details on any errors or issues."
  },
  {
    "id": 250,
    "question": "You need to configure Atlas to ingest metadata from a custom application that is not natively supported.\nWhat is the recommended approach?",
    "options": {
      "A": "Directly modify the Atlas codebase to add support for the custom application.",
      "B": "Develop a custom Atlas bridge that uses the Atlas API to push metadata from the application.",
      "C": "Configure the application to write metadata directly to the Atlas database.",
      "D": "Use Flume to collect metadata from the application and send it to Atlas.",
      "E": "Utilize the GenericJson hook to send metadata as a JSON to Kafka topic which Atlas can consume."
    },
    "correctAnswers": [
      "B",
      "E"
    ],
    "explanation": "Developing a custom bridge using the Atlas API is the recommended and supported way to integrate with custom applications. GenericJson hook provides a standardized method for sending custom metadata to Atlas via Kafka."
  },
  {
    "id": 251,
    "question": "An analyst reports that lineage information is missing for a newly created Impala table in Atlas. What\nsteps should you take to troubleshoot this issue?",
    "options": {
      "A": "Verify that the Impala audit logs are being correctly ingested by Atlas.",
      "B": "Ensure that the 'impala.atlas.hook.enabled' property is set to 'true' in the Impala configuration.",
      "C": "Restart the Atlas service to refresh its metadata cache.",
      "D": "Manually create the lineage information in Atlas using the API.",
      "E": "Check if Impala is configured to use the same Hive Metastore instance as Atlas."
    },
    "correctAnswers": [
      "A",
      "B",
      "E"
    ],
    "explanation": "Impala lineage depends on audit logs being ingested and the Impala Atlas hook being enabled. Both Atlas and Impala using the same Hive Metastore is essential for metadata consistency and proper lineage tracking. Verify property also."
  },
  {
    "id": 252,
    "question": "You are planning to implement data masking based on Atlas classifications. Which component(s) within\nthe CDP ecosystem would you primarily use to enforce these masking policies?",
    "options": {
      "A": "Atlas",
      "B": "Ranger",
      "C": "Sentry",
      "D": "HDFS ACLs",
      "E": "Cloudera Navigator"
    },
    "correctAnswers": [
      "B"
    ],
    "explanation": "Ranger is the primary component for enforcing data masking policies in CDP. By integrating with Atlas classifications, you can create dynamic masking policies based on metadata."
  },
  {
    "id": 253,
    "question": "Consider you have installed Atlas on your CDP on-premise cluster and want to classify all tables in Hive\ndatabases that contain credit card information. Which of the following steps would be most effective to\naccomplish this using automated discovery and classification?",
    "options": {
      "A": "Manually inspect each Hive table and apply the 'CreditCardlnfo' classification in Atlas.",
      "B": "Write a custom script that connects to the Hive Metastore, analyzes column names and data samples, and then uses the Atlas API to apply the 'CreditCardlnfo' classification based on pattern matching.",
      "C": "Create a Ranger policy that automatically applies the 'CreditCardlnfo' classification to all Hive tables.",
      "D": "Configure the Atlas Hive hook to automatically classify tables based on column names that match specific patterns (e.g., 'credit_card', 'cc_number').",
      "E": "Use the Apache Griffin project to profile the data quality and automatically classify tables based on predefined data quality rules for credit card information."
    },
    "correctAnswers": [
      "B",
      "D"
    ],
    "explanation": "While manual inspection is an option, it's not scalable. Writing a custom script allows for automated pattern matching and classification via the Atlas API. Configuring the Atlas Hive hook with column name pattern matching also provides automated classification. Although other options might seem viable, the custom script provides the most flexibility and direct control over the classification process. Option D is also valid, but not as precise."
  },
  {
    "id": 254,
    "question": "You are configuring Ranger to authorize access to Hive. After enabling Ranger plugin in Hive, you notice\nthat users are still able to access Hive tables without Ranger policies being applied. Which of the\nfollowing is the most likely cause?",
    "options": {
      "A": "The Hive service in Ranger Admin is not properly configured with the correct JDBC connection string.",
      "B": "The Ranger plugin for Hive is not enabled in the Hive Metastore configuration.",
      "C": "The Ranger Admin service is down.",
      "D": "The Hive user does not have the appropriate Kerberos principal associated.",
      "E": "The HiveServer2 advanced configuration property ‘hive.security.authorization.manager’ is not set to ‘org.apache.ranger.authorization.hive.authorizer.RangerHiveAuthorizer’."
    },
    "correctAnswers": [
      "E"
    ],
    "explanation": "Ranger authorization for Hive relies on the ‘hive.security.authorization.manager' property being correctly set to the Ranger authorizer. If this is missing or incorrect, Hive will bypass Ranger policies."
  },
  {
    "id": 255,
    "question": "Which Ranger component is responsible for enforcing access policies at the resource level for various\nHadoop components?",
    "options": {
      "A": "Ranger Admin",
      "B": "Ranger UserSync",
      "C": "Ranger Plugins",
      "D": "Ranger KMS",
      "E": "Ranger TagSync"
    },
    "correctAnswers": [
      "C"
    ],
    "explanation": "Ranger Plugins are embedded within the Hadoop components (e.g., Hive, HDFS, YARN) and intercept access requests to enforce the policies defined in Ranger Admin."
  },
  {
    "id": 256,
    "question": "You want to grant read access to a specific column in a Hive table to a group of users using Ranger.\nWhich of the following policy structures would you use?",
    "options": {
      "A": "Database -> Table -> Column",
      "B": "Table -> Database -> Column",
      "C": "Column -> Table -> Database",
      "D": "Database -> Column-> Table",
      "E": "Namespace-> Database -> Table"
    },
    "correctAnswers": [
      "A"
    ],
    "explanation": "Ranger policies for Hive follow a hierarchical structure: Database -> Table -> Column. This allows you to define granular access controls at the column level within a specific table in a database."
  },
  {
    "id": 257,
    "question": "Which of the following is the primary purpose of Ranger UserSync?",
    "options": {
      "A": "Synchronizing Ranger policies across multiple Hadoop clusters.",
      "B": "Synchronizing user and group information from external identity providers (e.g., LDAP, Active Directory) into Ranger.",
      "C": "Synchronizing data masking policies across different Hadoop components.",
      "D": "Synchronizing audit logs from Hadoop components to Ranger Admin.",
      "E": "Synchronizing Ranger roles and permissions."
    },
    "correctAnswers": [
      "B"
    ],
    "explanation": "Ranger UserSync is responsible for fetching user and group information from external identity providers and keeping Ranger's internal user and group database synchronized. This ensures that Ranger can properly authenticate and authorize users based on their existing identities."
  },
  {
    "id": 258,
    "question": "You are tasked with auditing all access to HDFS files through Ranger. What configuration setting needs\nto be enabled on HDFS for successful auditing?",
    "options": {
      "A": "Enable HDFS encryption.",
      "B": "Enable HDFS quota management.",
      "C": "Enable Ranger audit logging in HDFS configuration.",
      "D": "Enable HDFS ACLs.",
      "E": "Enable HDFS delegation tokens."
    },
    "correctAnswers": [
      "C"
    ],
    "explanation": "To audit access to HDFS files through Ranger, you need to enable Ranger audit logging in the HDFS configuration. This ensures that all access attempts are logged and sent to Ranger for auditing purposes."
  },
  {
    "id": 259,
    "question": "You are configuring Ranger to provide centralized auditing for Hive, HDFS, and YARN. However, you\nnotice that the audit logs are not appearing in Ranger Admin for YARN. Which of the following steps is\nmost likely to resolve this issue?",
    "options": {
      "A": "Restart the YARN Resource Manager.",
      "B": "Verify that the Ranger plugin is enabled for YARN in the YARN configuration and that the yarn-site.xml is correctly configured to point to the Ranger Admin server.",
      "C": "Restart the Ranger Admin server.",
      "D": "Enable audit logging in the YARN application master configuration.",
      "E": "Disable and re-enable the YARN service in Cloudera Manager."
    },
    "correctAnswers": [
      "B"
    ],
    "explanation": "For Ranger to audit YARN access, the Ranger plugin must be correctly enabled in the YARN configuration (yarn-site.xml). This includes specifying the Ranger Admin server details so that YARN can send audit logs to Ranger."
  },
  {
    "id": 260,
    "question": "What is the correct order of steps to install and configure Ranger for Hive access control in a Cloudera\nData Platform (CDP) on-premises cluster?",
    "options": {
      "A": "Install Ranger Admin -> Configure Ranger Usersync -> Enable Ranger Plugin for Hive -> Define Ranger Policies.",
      "B": "Enable Ranger Plugin for Hive -> Install Ranger Admin -> Configure Ranger Usersync -> Define Ranger Policies.",
      "C": "Install Ranger Admin Define Ranger Policies -> Configure Ranger Usersync Enable Ranger Plugin for Hive.",
      "D": "Configure Ranger Usersync -> Install Ranger Admin -> Enable Ranger Plugin for Hive -> Define Ranger Policies.",
      "E": "Install Ranger Admin -> Configure Ranger Usersync Define Ranger Policies -> Enable Ranger Plugin for Hive."
    },
    "correctAnswers": [
      "A"
    ],
    "explanation": "The correct order is: Install Ranger Admin, configure IJserSync to populate users/groups, enable the Ranger plugin for Hive so that Hive requests are intercepted and checked against Ranger, and finally, define Ranger policies to grant or deny access."
  },
  {
    "id": 261,
    "question": "You need to implement row-level filtering in Hive using Ranger. Which of the following conditions must\nbe met to achieve this functionality?",
    "options": {
      "A": "The Hive table must be stored in ORC format.",
      "B": "The Ranger plugin for Hive must be configured with a row-level filtering policy that includes a SQL WHERE clause condition.",
      "C": "The user accessing the Hive table must have the 'SELECT' privilege on the table.",
      "D": "The Hive table must be ACID-compliant.",
      "E": "All of the above."
    },
    "correctAnswers": [
      "E"
    ],
    "explanation": "All the conditions must be met. The Hive table needs to be in ORC format, the Ranger plugin needs to be configured with a SQL WHERE clause to filter rows, the user needs SELECT privilege, and the Hive table needs to be ACID-compliant (although strictly speaking Ranger masking/filtering can be applied to non- ACID tables, ACID is needed for update/delete policies)."
  },
  {
    "id": 262,
    "question": "You are troubleshooting an issue where Ranger policies are not being enforced for a specific HDFS path.\nYou have confirmed that the Ranger plugin is enabled for HDFS and that the policies are correctly\ndefined in Ranger Admin. However, users can still access the path without the policies being applied.\nWhich of the following are potential causes for this issue? (Select all that apply)",
    "options": {
      "A": "The HDFS user's Kerberos principal is not properly configured.",
      "B": "The HDFS namenode is not properly configured to communicate with Ranger Admin.",
      "C": "The HDFS path is not included in any Ranger policies.",
      "D": "The Ranger Admin service is experiencing high latency, causing delays in policy retrieval.",
      "E": "HDFS ACLs are conflicting with the Ranger policies."
    },
    "correctAnswers": [
      "B",
      "E"
    ],
    "explanation": "If policies are not being enforced despite the plugin being enabled and policies defined, the most likely causes are: HDFS namenode cannot properly communicate with Ranger Admin to retrieve policies, and there is a potential conflict with pre-existing HDFS ACLs which might override Ranger policies."
  },
  {
    "id": 263,
    "question": "You want to configure Ranger to mask a specific column in a Hive table (e.g., email address) so that only\nauthorized users can see the actual data’. Other users should see a masked version (e.g., 'XXX'). Which\nof the following masking options available in Ranger would be most suitable for this scenario?",
    "options": {
      "A": "Partial mask: show first 4, last 3",
      "B": "Redact",
      "C": "Hash",
      "D": "Custom",
      "E": "Date: show only year"
    },
    "correctAnswers": [
      "D"
    ],
    "explanation": "The 'Custom' masking option is the most suitable because it allows you to define a specific masking format (e.g., replacing the entire email address with IXXX') for unauthorized users. This provides the required level of data protection while allowing authorized users to see the actual email address."
  },
  {
    "id": 264,
    "question": "Consider the following Ranger policy for HDFS: Resource: /data/sensitive User: analytics_team\nPermissions: read, write, execute Conditions: Time-based restriction (9 AM - 5 PM) If a user from the\n'analytics_team' attempts to access the '/data/sensitive' directory at 7 PM, what will happen?",
    "options": {
      "A": "The user will be granted read, write, and execute access because they belong to the ‘analytics_team’ .",
      "B": "The user will be denied access because the access attempt is outside the allowed time window (9 AM - 5 PM).",
      "C": "The user will be prompted to enter a password to verify their identity.",
      "D": "The user will be granted read access only.",
      "E": "The behavior is unpredictable; it depends on the load on the Ranger Admin server."
    },
    "correctAnswers": [
      "B"
    ],
    "explanation": "Ranger policies are strictly enforced based on the defined conditions. In this case, the time-based restriction will deny access to the 'Idata/sensitive\" directory outside the 9 AM - 5 PM window, regardless of the user's group membership."
  },
  {
    "id": 265,
    "question": "You are configuring Ranger auditing to store audit events in HDFS. However, the Ranger Admin server is\ngenerating a large number of audit events, which are filling up the HDFS storage. Which of the following\nstrategies can you implement to manage the audit event storage effectively? (Select all that apply)",
    "options": {
      "A": "Reduce the audit log level to only capture critical events.",
      "B": "Implement a data lifecycle management policy in HDFS to archive or delete older audit events.",
      "C": "Increase the replication factor for the audit event directory in HDFS.",
      "D": "Configure Ranger to store audit events in a different storage system (e.g., Kafka).",
      "E": "Disable auditing for less critical services."
    },
    "correctAnswers": [
      "A",
      "B",
      "D",
      "E"
    ],
    "explanation": "To manage audit event storage effectively, you can: reduce the audit log level to capture only critical events, implement data lifecycle management policies to archive/delete old events, configure Ranger to use a different storage system (like Kafka which allows for easier scaling), and disable auditing for less critical services. Increasing the replication factor will increase storage use."
  },
  {
    "id": 266,
    "question": "You are tasked with implementing column-level masking on the 'employees’ table in Hive. The\nrequirement is to mask the ‘salary' column for users in the 'analytics' group while allowing full access to\nthe 'hr' group. Which of the following Apache Ranger policies would best achieve this?",
    "options": {
      "A": "Create a Ranger policy with the 'analytics' group, 'salary' column, and 'mask' permission, selecting the 'MASKED' mask type. Create another policy with 'hr' group, 'salary' column, and 'select' permission, allowing full access.",
      "B": "Create a Ranger policy with the 'analytics' group, 'salary' column, and 'select' permission, selecting the 'NULLIFY' mask type. Create another policy with 'hr' group, 'salary' column, and 'select' permission, allowing full access.",
      "C": "Create a single Ranger policy with the 'analytics' group, 'salary' column, and 'mask' permission, selecting the 'PARTIAL' mask type. Add an exclusion condition for the 'hr' group with 'select' permission.",
      "D": "Create a Ranger policy with the 'analytics' group, 'salary' column, and 'select' permission, selecting the 'MASKED' mask type. No separate policy is needed for the 'hr' group.",
      "E": "Create a single Ranger policy with the 'analytics' group, 'salary' column, and 'mask' permission, selecting the 'MASKED' mask type. Create another policy with 'hr' group, 'salary' column, and 'select' permission, allowing full access. Ensure the 'hr' policy is ordered higher in the policy list."
    },
    "correctAnswers": [
      "E"
    ],
    "explanation": "The correct approach is to create two separate policies: one for masking the salary for the 'analytics' group and another explicitly granting select access to the 'hr' group. The 'hr' policy needs to be higher in order because Ranger uses a first-match approach. If a single policy with exclusions is used, it can become complex to manage and might not be as clear in its intent."
  },
  {
    "id": 267,
    "question": "You have configured Ranger to audit access to Hive tables. You notice a significant increase in audit logs,\nmaking it difficult to analyze relevant events. Which of the following actions could you take to reduce\nthe volume of audit logs while still maintaining adequate security monitoring?",
    "options": {
      "A": "Disable auditing for all Hive policies except those related to sensitive data, such as columns containing Personally Identifiable Information (PII).",
      "B": "Reduce the audit frequency by increasing the ‘ranger.audit.async.max.queue.size’ property in Ranger's configuration.",
      "C": "Implement a Ranger policy to exclude certain users or groups from audit logging while still auditing their data access.",
      "D": "Decrease the ‘ranger.audit.max.flusher.threads’ property to reduce the number of threads writing audit logs.",
      "E": "Configure Ranger to only audit 'SELECT' statements and disable auditing for 'INSERT', 'UPDATE', and 'DELETE' statements."
    },
    "correctAnswers": [
      "A"
    ],
    "explanation": "Disabling auditing for policies not related to sensitive data will significantly reduce the volume of audit logs without compromising security monitoring for critical resources. Increasing queue size (B) only postpones the writing. Excluding users or groups (C) could miss important audit events. Decreasing flusher threads (D) could cause performance issues or dropped audits. Auditing only SELECT statements (E) would miss data modification activities."
  },
  {
    "id": 268,
    "question": "You are configuring Apache Ranger to manage access to data in Hive. You need to define a policy that\ngrants access to a specific set of columns in a table only when the user is accessing the data through a\nspecific application. How can you achieve this?",
    "options": {
      "A": "Use the 'Context Enforcer' feature in Ranger to define a policy that restricts access based on the application's name.",
      "B": "Use the 'Resource-based Authorization' feature in Hive to define access control lists (ACLs) based on the application's user ID.",
      "C": "Use the 'Application ID' condition in Ranger policies to restrict access to specific columns based on the application's identifier. This needs Ranger version 2.0+.",
      "D": "Use the 'Custom Conditions' feature in Ranger to write a Java plugin that verifies the application's identity and grants access accordingly.",
      "E": "It is not possible to restrict access based on the application accessing the data using Ranger's built-in features. Custom code or external authentication mechanisms are required."
    },
    "correctAnswers": [
      "C"
    ],
    "explanation": "Ranger policies can use the 'Application ID' condition (available in Ranger 2.0 and later) to restrict access based on the application accessing the data. This allows you to define policies that grant access to specific columns only when the user is accessing the data through the specified application. Context Enforcer is not a valid Ranger feature, and resource-based authorization in Hive does not directly integrate with application identity in this way."
  },
  {
    "id": 269,
    "question": "Which of the following are valid methods for ensuring data masking policies are consistently enforced\nacross multiple Cloudera Data Platform (CDP) environments (e.g., development, staging, production)?\n(Select TWO)",
    "options": {
      "A": "Manually re-creating the Ranger policies in each environment, ensuring identical configurations.",
      "B": "Using the Ranger Import/Export functionality to export policies from one environment and import them into another.",
      "C": "Configuring Ranger replication across environments using a shared metastore.",
      "D": "Using Cloudera Manager to automatically synchronize Ranger policies across all configured environments.",
      "E": "Employing infrastructure-as-code (laC) tools, such as Terraform or Ansible, to automate the deployment and configuration of Ranger policies."
    },
    "correctAnswers": [
      "B",
      "E"
    ],
    "explanation": "Ranger's import/export functionality provides a structured way to move policies between environments. Infrastructure-as-code (laC) allows for automated and repeatable deployment of Ranger configurations, including policies, ensuring consistency across environments. Manually re-creating policies is error- prone and not scalable. While CM manages the Ranger service itself, it does not directly manage Ranger policies. Replication using a shared metastore would only replicate metadata, not Ranger policies."
  },
  {
    "id": 270,
    "question": "You've configured Apache Ranger with Atlas integration. After making some changes to Ranger policies,\nyou notice that the Atlas metadata is not being updated with the new access control information. Which\nof the following steps would you take to troubleshoot this issue?",
    "options": {
      "A": "Restart the Ranger Admin service to force a synchronization with Atlas.",
      "B": "Verify that the ‘atlas.hook.ranger.sync' property is set to ‘true' in the Ranger Admin's configuration.",
      "C": "Check the Ranger audit logs for any errors related to Atlas integration.",
      "D": "Manually trigger a full import of Ranger policies into Atlas using the Atlas API.",
      "E": "Ensure the user running the Ranger to Atlas synchronization has 'atlas_admin' role."
    },
    "correctAnswers": [
      "B",
      "C"
    ],
    "explanation": "The ‘atlas.hook.ranger.sync' property must be set to 'true' to enable automatic synchronization between Ranger and Atlas. Checking the Ranger audit logs can reveal errors or warnings related to the Atlas integration, providing clues about the cause of the synchronization failure. Restarting the Ranger Admin service might help if the process has stalled, but doesn't address the root cause if synchronization is disabled or failing. There is no specific 'manual import' from Ranger to Atlas using the Atlas API. The Atlas admin role is important to check as well."
  },
  {
    "id": 271,
    "question": "A data analyst reports that they are unable to see the data in a specific column, even though they\nbelieve they have the necessary permissions. After investigating, you discover that a masking policy is\napplied to the column, but the analyst is supposed to be exempt. Assuming Ranger is configured\ncorrectly, what is the MOST likely cause of this issue?",
    "options": {
      "A": "The analyst's user account is not properly synchronized between Ranger and the underlying data source (e.g., Hive, Impala).",
      "B": "The Ranger policy granting the analyst access is not high enough in the policy order compared to the masking policy.",
      "C": "The masking policy is configured with a higher priority than the grant access policy and there is a conflict.",
      "D": "The masking policy has not been properly replicated to the data source nodes, and a restart is required.",
      "E": "The Ranger audit log is full and masking policies are not being applied correctly due to write errors."
    },
    "correctAnswers": [
      "B"
    ],
    "explanation": "Ranger policies are evaluated in order, with the first matching policy taking effect. If a masking policy is applied before a policy granting access to the analyst, the masking will override the access, even if the analyst is supposed to be exempt. Account synchronization (A) is important, but a separate access policy would still be needed. Replication of policies (D) and audit log issues (E) would likely affect all users, not just a specific analyst. Policy conflict and priority (C) is similar to the correct answer, but policies are evaluated based on order, not priority."
  },
  {
    "id": 272,
    "question": "You are responsible for implementing data lineage tracking in your Cloudera CDP environment. You\nwant to use Apache Atlas to capture the lineage of data transformations performed by Spark jobs.\nWhich of the following configurations are necessary to enable Spark-Atlas integration for data lineage?",
    "options": {
      "A": "Install the Atlas client libraries on all Spark driver nodes and configure the 'spark.extraListenerS property to point to the Atlas listener class.",
      "B": "Configure the Spark thrift server to publish metadata events to Atlas via the Atlas REST API.",
      "C": "Enable the ‘spark.sql.queryExecutionListenerS property and specify the Atlas lineage listener class. Ensure the Atlas client libraries are included in the Spark classpath.",
      "D": "No specific configuration is needed. Spark automatically integrates with Atlas to capture data lineage information.",
      "E": "You must write a custom Spark listener that extracts data lineage information from Spark events and publishes it to Atlas using the Atlas API."
    },
    "correctAnswers": [
      "C"
    ],
    "explanation": "To enable Spark-Atlas integration, you need to configure the 'spark.sql.queryExecutionListenerS property to point to the Atlas lineage listener class. This listener captures Spark SQL query execution events and publishes them to Atlas, creating data lineage relationships. The Atlas client libraries must also be included in the Spark classpath for the listener to function correctly. spark.extraListeners is used for general spark listeners, but spark.sql.queryExecutionListeners is specifically used to track SQL query execution. There is no built in thrift server publish to Atlas function. While you could write a custom listener, the default configuration is the intended practice."
  },
  {
    "id": 273,
    "question": "A security auditor has reported that sensitive data is being stored in a Hive table without any masking or\naccess controls. As a Cloudera CDP administrator, what steps would you take to remediate this issue\nusing Apache Ranger? Select all that apply",
    "options": {
      "A": "Implement column-level masking on the sensitive columns in the Hive table to prevent unauthorized access.",
      "B": "Configure Ranger policies to restrict access to the Hive table based on user roles and groups.",
      "C": "Encrypt the entire Hive table using Hive's built-in encryption features to protect the sensitive data at rest.",
      "D": "Implement row-level filtering in Ranger to restrict access to specific rows based on user attributes or data values.",
      "E": "Purge the sensitive data from the Hive table and store it in a separate, more secure location."
    },
    "correctAnswers": [
      "A",
      "B",
      "D"
    ],
    "explanation": "Implementing column-level masking (A) protects sensitive data by replacing it with masked values for unauthorized users. Configuring Ranger policies (B) restricts access to the table based on roles and groups. Implementing row-level filtering (D) allows access to rows based on some condition. Encryption(C) is generally recommended, but is more focused on data at rest and doesn't protect access in the same way that Ranger policies and masking do. Purging the data(E) might be a valid option in some cases, but is not a general solution for managing sensitive data."
  },
  {
    "id": 274,
    "question": "You are using Apache Atlas to manage the metadata of your data assets in Cloudera CDP. You want to\ncreate a custom entity type in Atlas to represent a specific type of data transformation process in your\norganization. What steps would you take to define this custom entity type?",
    "options": {
      "A": "Create a new Java class that extends the ‘org.apache.atlas.model.instance.AtlasEntity’ class and define the custom attributes and relationships for the entity type.",
      "B": "Define a new entity definition in a JSON file and import it into Atlas using the Atlas REST API.",
      "C": "Create a new table in the Atlas metadata repository and define the columns for the custom entity type.",
      "D": "Use the Atlas IJI to create a new entity type and define its attributes and relationships.",
      "E": "Define a new entity type in a YAML file and use the 'atlas-cli' command-line tool to import it into Atlas."
    },
    "correctAnswers": [
      "B"
    ],
    "explanation": "Custom entity types in Atlas are defined using JSON files that specify the attributes, relationships, and other properties of the entity type. These JSON files are then imported into Atlas using the Atlas REST API. Creating a Java class (A) is not necessary for defining custom entity types. Creating a table in the Atlas metadata repository (C) is incorrect, as Atlas manages its metadata internally. While the Atlas IJI (D) can be used to browse and manage existing entities, it does not support creating custom entity types. YAML isn't native to Atlas."
  },
  {
    "id": 275,
    "question": "You are configuring Ranger to manage access to Impala data’. A user reports that they are able to see\ndata in a table via Impala-shell but are unable to see the same data when accessing the table through a\nBl tool connected via JDBC. What is the MOST likely reason for this discrepancy?",
    "options": {
      "A": "The user's Kerberos principal is different when connecting via Impala-shell versus JDBC.",
      "B": "The JDBC connection is using a different Ranger plugin configuration than Impala-shell.",
      "C": "The Impala service is configured to bypass Ranger authorization for JDBC connections.",
      "D": "The Ranger policy evaluation is case-sensitive, and the table name is being passed differently via JDBC.",
      "E": "The BI tool is caching the Impala metadata, and the Ranger policies have not been updated in the cache."
    },
    "correctAnswers": [
      "A"
    ],
    "explanation": "When connecting via Impala-shell, the user's Kerberos principal is typically used for authentication and authorization. However, when connecting via JDBC, the BI tool might be using a different Kerberos principal or a different authentication mechanism altogether. If the Ranger policies are configured based on the Kerberos principal, a mismatch can cause access discrepancies. The JDBC connection typically uses the same Ranger plugin as other Impala access methods. Impala is not configured to bypass Ranger, and policy evaluation is not case-sensitive."
  },
  {
    "id": 276,
    "question": "You need to implement a data retention policy for audit logs generated by Apache Rangen The policy\nrequires that audit logs be retained for one year and then automatically archived. What steps should\nyou take to configure this data retention policy?",
    "options": {
      "A": "Configure the ‘ranger.audit.max.age’ property in Ranger's configuration file to 365 days.",
      "B": "Implement a custom script that periodically moves audit logs older than one year to an archive directory.",
      "C": "Configure a log rotation policy in the operating system to rotate audit logs on a daily basis and retain logs for one year.",
      "D": "Use the Ranger API to query audit logs older than one year and delete them from the audit log repository.",
      "E": "Configure Ranger's archiver feature to automatically move audit logs to a separate storage location after one year."
    },
    "correctAnswers": [
      "B"
    ],
    "explanation": "Currently, Cloudera CDP does not have built in features for automatic archival. The best solution involves implementing custom scripts. The ‘ranger.audit.max.age' (A) only affects how long the logs are stored in the current system, not archival. Configuring a log rotation policy (C) is a good practice for managing log file size, but doesn't provide a mechanism for archiving. The Ranger API could be used, but the logs would be deleted, not archived."
  },
  {
    "id": 277,
    "question": "You are using Ranger to manage access to Hive tables, and you want to create a policy that grants access\nto a table only during specific business hours (e.g., 9 AM to 5 PM on weekdays). How can you achieve\nthis?",
    "options": {
      "A": "Use the 'Time-based Access Control' feature in Ranger to define a schedule for the policy to be active.",
      "B": "Use the 'Custom Condition' feature in Ranger to write a Java plugin that checks the current time and grants access accordingly.",
      "C": "Schedule a script to enable and disable the Ranger policy based on the business hours.",
      "D": "Time-based access control is not directly supported in Ranger. You must implement it using external authentication mechanisms or custom code within the application accessing the data.",
      "E": "Use the 'REST API' feature in Ranger to enable and disable the Ranger policy based on the business hours."
    },
    "correctAnswers": [
      "B"
    ],
    "explanation": "Ranger does not have a built-in 'Time-based Access Control' feature. The most straightforward way to implement time-based access control is to use the 'Custom Condition' feature in Ranger and write a Java plugin that checks the current time and grants access accordingly. The Ranger REST API can be used to enable and disable policies, but would require a separate scheduling mechanism. Building it directly into a Java plugin allows for a more complete solution."
  },
  {
    "id": 278,
    "question": "Your Cloudera CDP on-premises cluster is experiencing frequent HDFS block replication delays,\nimpacting write performance. Which of the following actions would MOST directly address this issue\nfrom a capacity management perspective?",
    "options": {
      "A": "Increase the value of ‘dfs.namenode.handler.count’ in the NameNode's ‘hdfs-site.xmr configuration file.",
      "B": "Reduce the 'dfs.replication’ factor for newly created HDFS files.",
      "C": "Add more DataNodes to the cluster to increase overall storage capacity and network bandwidth.",
      "D": "Increase the ‘dfs.datanode.max.locked.memory' on all DataNodes.",
      "E": "Enable HDFS Federation with another existing cluster to balance the load."
    },
    "correctAnswers": [
      "C"
    ],
    "explanation": "Adding more DataNodes directly addresses the capacity issue by increasing storage capacity and, critically, network bandwidth for replication. While A might help the NameNode, it doesn't solve the underlying resource shortage. B reduces data durability. D focuses on memory usage, not bandwidth or storage. E is a complex solution and not the most direct."
  },
  {
    "id": 279,
    "question": "You notice that your YARN ResourceManager is consistently operating near its memory limit, causing\napplication failures. You want to increase the ResourceManager's heap size. Which Cloudera Manager\nproperty should you modify to achieve this goal?",
    "options": {
      "A": "resourcemanager_java_heapsize",
      "B": "yarn.resourcemanager.resource-tracker.expiry-interval",
      "C": "yarn_resourcemanager_heapsize",
      "D": "rm.java.opts",
      "E": "resourcemanager_java_opts"
    },
    "correctAnswers": [
      "E"
    ],
    "explanation": "is the correct Cloudera Manager property to modify Java options for the ResourceManager, including heap size. You would set this property to a value like '-Xmx8g’ to allocate 8GB of heap memory. \"yarn.resourcemanager.resource-tracker.expiry- interval' is related to node manager heartbeats. The other options are incorrect."
  },
  {
    "id": 280,
    "question": "A Cloudera CDP cluster is experiencing a high volume of small file writes to HDFS. This is causing\nexcessive NameNode load. What are TWO effective strategies to mitigate this issue from a capacity\nmanagement perspective, assuming you cannot change the application writing the files?",
    "options": {
      "A": "Increase the 'dfs.blocksize’ configuration parameter in 'hdfs-site.xmr.",
      "B": "Implement HDFS Federation with a dedicated namespace for small files.",
      "C": "Enable HDFS caching for the directories containing the small files.",
      "D": "Implement a small file archival strategy using tools like Apache NiFi or Flume to periodically consolidate small files into larger ones.",
      "E": "Reduce the number of DataNodes in the cluster to consolidate storage."
    },
    "correctAnswers": [
      "B",
      "D"
    ],
    "explanation": "HDFS Federation allows you to isolate the small files to a dedicated namespace, reducing the load on the primary NameNode. Archiving small files into larger ones reduces the overall number of files, thereby reducing NameNode metadata load. Increasing the block size doesn't directly address the number of files. Caching can improve read performance, but doesn't reduce NameNode load from writes. Reducing the number of DataNodes decreases overall capacity."
  },
  {
    "id": 281,
    "question": "You are tasked with monitoring the disk utilization of your HDFS DataNodes. Which command provides\nthe MOST comprehensive information about disk space usage across all DataNodes in the cluster?",
    "options": {
      "A": "hdfs dfsadmin -report",
      "B": "hdfs fsck /",
      "C": "df -h",
      "D": "yarn application -list",
      "E": "hdfs storagepolicies -getStoragePolicy I"
    },
    "correctAnswers": [
      "A"
    ],
    "explanation": "'hdfs dfsadmin -report' provides a detailed report on the overall HDFS status, including disk space utilization for each DataNode. 'hdfs fsck P checks file system integrity. 'df -h' shows local disk usage on the server it's executed on. 'yarn application -list lists YARN applications. ‘hdfs storagepolicies - getStoragePolicy P lists the storage policy applied to the root directory."
  },
  {
    "id": 282,
    "question": "A critical MapReduce job is consistently failing due to insufficient container memory. You want to\nincrease the maximum memory allocation for containers in a specific YARN queue named 'production'.\nWhich configuration property and location should you modify?",
    "options": {
      "A": "Modify yarn.scheduler.maximum-allocation-mb' in ‘yarn-site.xmr for the entire cluster.",
      "B": "Modify ‘yarn.scheduler.maximum-allocation-mb' specifically for the 'production' queue in the Capacity Scheduler configuration within Cloudera Manager.",
      "C": "Modify mapreduce.map.memory.mb' in ‘mapred-site.xmr for the entire cluster.",
      "D": "Modify ‘yarn.nodemanager.resource.memory-mb' on each NodeManager.",
      "E": "Modify yarn.scheduler.maximum-allocation-mb' in the mapred-site.xml file for the entire cluster."
    },
    "correctAnswers": [
      "B"
    ],
    "explanation": "The correct approach is to modify 'yarn.scheduler.maximum-allocation-mb' specifically for the 'production' queue within the Capacity Scheduler configuration in Cloudera Manager. This allows you to increase the maximum memory allocation for containers within that specific queue without impacting other queues. Modifying it in ‘yarn-site.xmr affects the entire cluster. controls memory requested by map tasks, not the overall maximum. \"yarn.nodemanager.resource.memory-mb' defines the total memory available on each NodeManager."
  },
  {
    "id": 283,
    "question": "You need to determine the total available memory resources in your YARN cluster. Which of the\nfollowing commands or Uls would provide you with the MOST accurate and up-to-date information?",
    "options": {
      "A": "Summing the ‘yarn.nodemanager.resource.memory-mb' values from the ‘yarn-site.xml' files on each NodeManager.",
      "B": "Using the Cloudera Manager YARN service IJI to view the 'Total Available Memory' metric.",
      "C": "Running 'free -m' on the ResourceManager node.",
      "D": "Parsing the output of 'yarn node -list' and summing the 'memory' attribute for each node.",
      "E": "Consulting the hardware specifications of the servers in the cluster."
    },
    "correctAnswers": [
      "B"
    ],
    "explanation": "The Cloudera Manager YARN service IJI provides the most accurate and up-to-date view of the total available memory in the YARN cluster. It dynamically reflects the actual resources available, taking into account node health and configuration changes. Summing values from ‘yarn-site.xml' is static and doesn't reflect the current state. Tree -m' shows memory on a single server. ‘yarn node -list' can be used, but parsing is cumbersome. Hardware specs don't reflect what's allocated to YARN."
  },
  {
    "id": 284,
    "question": "Which statement about the impact of HDFS block size ('dfs.blocksize') on cluster capacity and\nperformance is MOST accurate ?",
    "options": {
      "A": "Increasing the block size always improves write performance for all types of workloads.",
      "B": "Decreasing the block size reduces the memory footprint of the NameNode but can lead to increased disk I/O for large file reads.",
      "C": "Increasing the block size reduces the number of metadata objects the NameNode must manage, potentially improving NameNode performance, but may lead to wasted space if many files are significantly smaller than the block size.",
      "D": "The block size has no impact on the overall storage capacity of the cluster.",
      "E": "Decreasing the block size always increases write throughput and reduces latency."
    },
    "correctAnswers": [
      "C"
    ],
    "explanation": "Increasing the block size reduces the number of metadata objects (blocks) that the NameNode needs to track, which can improve its performance. However, it can also lead to wasted space if many files are smaller than the block size, as each file will occupy at least one full block. The other options are inaccurate or oversimplified."
  },
  {
    "id": 285,
    "question": "You have a Cloudera CDP cluster where several applications are competing for resources in the default\nYARN queue. You want to implement fair sharing of resources among these applications. What is the\nMOST appropriate YARN scheduler to use and how would you configure it within Cloudera Manager?",
    "options": {
      "A": "Use the FIFO scheduler and configure the ‘yarn.scheduler.fifo.max-capacity’ property.",
      "B": "Use the Capacity scheduler and configure the ‘yarn.scheduler.capacity.maximum-am-resource- percent property for the default queue.",
      "C": "Use the Fair scheduler and create allocation policies in the 'fair-scheduler.xmr file, managed through the Fair Scheduler Advanced Configuration Snippet (Safety Valve) for 'Fair Scheduler Configuration Directory' in Cloudera Manager.",
      "D": "Use the Capacity scheduler and enable preemption.",
      "E": "Use the FIFO scheduler and configure ‘yarn.scheduler.fair.preemption’ to True."
    },
    "correctAnswers": [
      "C"
    ],
    "explanation": "The Fair scheduler is designed for fair resource sharing among applications. It's configured using allocation policies defined in ‘fair- scheduler.xmr. Cloudera Manager provides a Safety Valve for managing this file. The FIFO scheduler is not designed for fair sharing. The Capacity scheduler can be configured for capacity guarantees, but Fair scheduler is better for fair sharing. While the Capacity scheduler with preemption can improve faimess, it's not as direct as using the Fair scheduler with appropriate policies."
  },
  {
    "id": 286,
    "question": "Your company is implementing a data lake using Cloudera CDP on-premises. You anticipate a significant\nincrease in data volume over the next year. What is the MOST proactive approach to capacity planning\nfor HDFS?",
    "options": {
      "A": "Wait until HDFS utilization reaches 80% before adding more DataNodes.",
      "B": "Regularly monitor HDFS utilization metrics (e.g., remaining capacity, disk 1/0) and extrapolate future growth based on historical trends, factoring in planned data ingestion and retention policies. Establish thresholds for triggering capacity expansion.",
      "C": "Periodically run 'hdfs fsck r to identify under-replicated blocks and allocate more space accordingly.",
      "D": "Assume a linear growth rate of 10% per month and add DataNodes accordingly, regardless of actual utilization.",
      "E": "Add as many datanodes as the budget allows."
    },
    "correctAnswers": [
      "B"
    ],
    "explanation": "Proactive capacity planning involves monitoring key metrics, analyzing historical trends, and considering future data ingestion plans and retention policies. Establishing thresholds allows for timely capacity expansion before performance is significantly impacted. Waiting until 80% utilization is reactive. 'hdfs fsck' checks file system integrity, not capacity. Assuming a fixed growth rate is often inaccurate. Adding nodes without any planning is not cost-effective."
  },
  {
    "id": 287,
    "question": "Which of the following are valid reasons for using HDFS Storage Policies (e.g., COLD, WARM, HOT) in a\nCloudera CDP on-premises environment?",
    "options": {
      "A": "To automatically tier data based on access frequency, moving less frequently accessed data to lower- cost storage.",
      "B": "To improve the reliability of data stored in HDFS by replicating data across multiple data centers.",
      "C": "To optimize disk I/O by placing frequently accessed data on faster storage devices (e.g., SSDs).",
      "D": "To reduce the overall storage capacity required for HDFS by compressing data more aggressively.",
      "E": "To isolate data based on security classifications, storing sensitive data on encrypted storage volumes."
    },
    "correctAnswers": [
      "A",
      "C"
    ],
    "explanation": "HDFS Storage Policies are primarily used for data tiering based on access frequency (A) and for optimizing disk I/O by placing data on appropriate storage devices (C). They don't inherently improve reliability through cross-DC replication, reduce storage capacity through compression, or provide security isolation (although those capabilities can be combined with storage policies)."
  },
  {
    "id": 288,
    "question": "You are investigating slow query performance in Impala, and suspect that insufficient memory is a\ncontributing factor. You want to increase the memory available to Impala queries. Which configuration\nsetting should you adjust, and where can you find it in Cloudera Manager?",
    "options": {
      "A": "Increase ‘mem_limit under the Impala Daemon Advanced Configuration Snippet (Safety Valve) for 'impala-site.xml'.",
      "B": "Increase ‘yarn.scheduler.maximum-allocation-mb' in the YARN service configuration in Cloudera Manager.",
      "C": "Increase in the Impala Catalog Server Advanced Configuration Snippet (Safety Valve) for 'impala- site.xml'.",
      "D": "Increase under the Impala Daemon Service-Wide Advanced Configuration Snippet (Safety Valve).",
      "E": "Increase the in Cloudera Manager."
    },
    "correctAnswers": [
      "A"
    ],
    "explanation": "The ‘mem_limit setting in the 'impala-site.xmr file controls the maximum memory that an Impala query can use. You can adjust this through the Impala Daemon Advanced Configuration Snippet (Safety Valve). ‘yarn.scheduler.maximum-allocation-mb’ affects YARN container memory. controls the Catalog Server memory. configures default query options but is not the primary setting for memory limits. impala_server_memory_percent is a setting that governs the proportion of server memory to be used but not the maximum limit."
  },
  {
    "id": 289,
    "question": "Your organization uses Ranger for fine-grained access control in Cloudera CDP A new business unit\nneeds access to specific tables in Hive. However, you are running out of available memory on Ranger\nAdmin server. What are the best ways to address Ranger Admin memory capacity and performance?\nChoose two options.",
    "options": {
      "A": "Increase Ranger Admin heap size via Ranger Admin Java Heap Size property in Cloudera Manager.",
      "B": "Reduce the number of Ranger policies defined within Ranger IJI.",
      "C": "Reduce the value of ‘ranger.audit.db.batch.size' in ranger-admin-site.xml",
      "D": "Scale out the number of Ranger Admin servers and load balance the traffic.",
      "E": "Tune the Ranger policy retrieval queries and add appropriate indexes on Ranger database."
    },
    "correctAnswers": [
      "A",
      "E"
    ],
    "explanation": "Increasing the Ranger Admin heap size is the first line of defense in increasing the Ranger Admin memory capacity. Ranger policies are stored in database and tuning the database and indexes can improve performance. Number of Ranger policies should be reduced only if there are overlapping policies. Ranger audit database batch size change is a configuration level tuning and is only relevant if audit logging is causing issue. Currently Ranger Admin does not support scale out and so load balancing is not an option."
  },
  {
    "id": 290,
    "question": "You are tasked with decommissioning a host node in a CDP on-premises cluster. After stopping all\nservices and roles on the node, you attempt to remove it from the cluster using Cloudera Manager.\nHowever, the removal process fails with the error 'Insufficient replicas found for HDFS blocks on the\nnode'. What is the MOST appropriate action to take?",
    "options": {
      "A": "Immediately force the node removal from Cloudera Manager, ignoring the replication warning.",
      "B": "Run an HDFS balancer to redistribute blocks from the node to other nodes in the cluster before attempting the removal again.",
      "C": "Increase the replication factor for all HDFS files to ensure sufficient replicas before removing the node.",
      "D": "Manually move all HDFS blocks from the node to other nodes using the 'hdfs dfs -mv’ command.",
      "E": "Wait for the HDFS replication to complete automatically; no further action is required."
    },
    "correctAnswers": [
      "B"
    ],
    "explanation": "Running the HDFS balancer is the most appropriate action. It intelligently redistributes the data from the decommissioning node to other nodes in the cluster, ensuring data availability and preventing data loss. Forcing the removal ignores the data replication issue and could lead to data loss. Increasing replication factor increases storage utilization and doesn't solve the immediate problem. Manually moving blocks is inefficient and prone to errors. Waiting for replication to complete might not happen if the node is already considered dead by the cluster."
  },
  {
    "id": 291,
    "question": "You are observing high CPU utilization on a NameNode in your CDP cluster. Which of the following\nactions can help alleviate the CPU load on the NameNode and improve cluster performance? (Select\nTWO)",
    "options": {
      "A": "Increase the number of DataNodes in the cluster.",
      "B": "Enable HDFS caching for frequently accessed metadata.",
      "C": "Reduce the number of files and directories in the HDFS namespace.",
      "D": "Increase the block size of HDFS files.",
      "E": "Increase the heap size allocated to the NameNode."
    },
    "correctAnswers": [
      "B",
      "C"
    ],
    "explanation": "Enabling HDFS caching stores frequently accessed metadata in memory, reducing disk I/O and CPU load. Reducing the number of files and directories lessens the metadata the NameNode needs to manage, directly reducing CPU utilization. Increasing DataNodes does not directly alleviate NameNode CPU load. Increasing block size can improve I/O performance but doesn't directly affect CPU load significantly. Increasing heap size can help if the NameNode is running out of memory, but if CPU is the bottleneck, it won't be as effective as caching or reducing the namespace size."
  },
  {
    "id": 292,
    "question": "A DataNode in your CDP cluster is consistently reporting disk failures. You want to gracefully\ndecommission the node and prevent future data loss. What is the correct sequence of steps to achieve\nthis?",
    "options": {
      "A": "1. Stop the DataNode. 2. Remove the DataNode from Cloudera Manager.",
      "B": "1. In Cloudera Manager, initiate the decommission process for the DataNode. 2. Monitor the decommissioning progress until completion. 3. Stop the DataNode. 4. Remove the DataNode from Cloudera Manager.",
      "C": "1. Physically remove the disk drives from the DataNode. 2. Stop the DataNode. 3. Remove the DataNode from Cloudera Manager.",
      "D": "1. Stop the DataNode. 2. In Cloudera Manager, initiate the decommission process for the DataNode. 3. Monitor the decommissioning progress until completion. 4. Remove the DataNode from Cloudera Manager.",
      "E": "1. Remove the DataNode from Cloudera Manager. 2. Stop the DataNode."
    },
    "correctAnswers": [
      "B"
    ],
    "explanation": "The correct sequence is to first initiate the decommissioning process from Cloudera Manager, which allows HDFS to replicate the data to other nodes. Then, monitor the progress until the decommissioning is complete. After that, the DataNode can be stopped and removed from Cloudera Manager."
  },
  {
    "id": 293,
    "question": "You want to add a new DataNode to your existing CDP on-premises cluster. Which of the following\nconfiguration changes are REQUIRED on the new DataNode before starting the DataNode process?",
    "options": {
      "A": "Install the Cloudera Manager Agent and configure it to point to the Cloudera Manager server.",
      "B": "Create the HDFS data directories as specified in the DataNode's configuration.",
      "C": "Configure the DataNode's hostname in the /etc/hosts file of all other nodes in the cluster.",
      "D": "Install the Hadoop client libraries.",
      "E": "Install the Hive client libraries."
    },
    "correctAnswers": [
      "A",
      "B"
    ],
    "explanation": "The Cloudera Manager Agent is essential for managing the DataNode. Creating the HDFS data directories ensures the DataNode has storage space. While hostname resolution is important, updating /etc/hosts on every node isn't scalable; DNS is preferred. Hadoop client libraries are typically installed as part of the CDP installation. Hive libraries are not directly required for a DataNode."
  },
  {
    "id": 294,
    "question": "A new application writes a large number of small files to HDFS, causing significant performance\ndegradation. What configuration change can you implement to mitigate this issue and improve write\nperformance? Assume no code changes are possible in the application itself. (Single choice)",
    "options": {
      "A": "Increase the number of DataNodes in the cluster.",
      "B": "Enable HDFS Federation.",
      "C": "Enable HDFS caching.",
      "D": "Use the HDFS Archive (HAR) tool to combine the small files into larger archive files.",
      "E": "Decrease the block size of HDFS."
    },
    "correctAnswers": [
      "D"
    ],
    "explanation": "The HDFS Archive (HAR) tool is designed specifically for addressing the small files problem. It combines small files into larger archive files, reducing the load on the NameNode and improving performance. Increasing DataNodes won't directly address the small file issue. HDFS Federation is for scaling the namespace, not directly for small files. HDFS caching helps with read performance. Decreasing block size would exacerbate the problem. Since application change is not permitted, HAR is the only solution."
  },
  {
    "id": 295,
    "question": "You are troubleshooting a DataNode that is failing to start. The DataNode logs contain the following\nerror: 'java.io.IOException: Incompatible clusterlD. What is the MOST likely cause of this error?",
    "options": {
      "A": "The DataNode's clock is not synchronized with the NameNode's clock.",
      "B": "The DataNode is using an incorrect core-site.xml configuration file.",
      "C": "The DataNode's disk space is full.",
      "D": "The DataNode is attempting to join a different HDFS cluster.",
      "E": "The DataNode's Java version is incompatible with the NameNode's Java version."
    },
    "correctAnswers": [
      "D"
    ],
    "explanation": "The ‘ Incompatible clusterlD error indicates that the DataNode is configured to connect to a different HDFS cluster than the NameNode. The ‘clusterlD’ is a unique identifier for an HDFS cluster. The DataNode's 'dfs.cluster.id' property in ‘hdfs-site.xmr must match the NameNode's 'dfs.cluster.id'."
  },
  {
    "id": 296,
    "question": "You have configured automatic decommissioning of unhealthy DataNodes in your CDP cluster. However,\nyou notice that DataNodes are being decommissioned too aggressively, even when the issues are\ntransient (e.g., network blips). Which configuration parameter can you adjust to make the\ndecommissioning process less sensitive?",
    "options": {
      "A": "'dfs.namenode.decommission.intervar",
      "B": "'dfs.namenode.decommission.nodes.per.interval'",
      "C": "'dfs.health.check.intervar",
      "D": "'dfs.datanode.failed.volumes.tolerated'",
      "E": "'dfs.namenode.decommission.replicas.required'"
    },
    "correctAnswers": [
      "D"
    ],
    "explanation": "'dfs.datanode.failed.volumes.tolerated' determines how many disk failures a DataNode can tolerate before being considered unhealthy. Increasing this value will make the decommissioning process less sensitive, as DataNodes will be allowed to have more failed disks before being automatically decommissioned. The other parameters control the decommissioning interval, the number of nodes decommissioned per interval, the health check interval and replica availability respectively but don't directly address the sensitivity to transient issues."
  },
  {
    "id": 297,
    "question": "You need to perform a rolling restart of your DataNodes to apply a security patch. While doing so, you\nwant to ensure minimal impact on the running applications. Which of the following approaches is the\nMOST suitable and ensures high availability during the process?",
    "options": {
      "A": "Stop all DataNodes simultaneously, apply the patch, and then start all DataNodes.",
      "B": "Decommission each DataNode one at a time, apply the patch, and then recommission them.",
      "C": "Use Cloudera Manager's rolling restart feature, ensuring the minimum number of replicas are available throughout the process.",
      "D": "Restart each DataNode without any prior action, assuming HDFS will handle the temporary unavailability.",
      "E": "Manually move all the data from each DataNode to other nodes, patch the node, and then move the data back."
    },
    "correctAnswers": [
      "C"
    ],
    "explanation": "Using Cloudera Manager's rolling restart feature is the best approach. It handles the restart process gracefully, ensuring that the minimum number of replicas are available throughout the process, minimizing impact on running applications and maintaining high availability. Stopping all DataNodes simultaneously will cause a cluster outage. Decommissioning and recommissioning each node is time- consuming and unnecessary. Restarting without any prior action can lead to data unavailability. Manually moving data is extremely time consuming and prone to errors."
  },
  {
    "id": 298,
    "question": "You are monitoring your CDP cluster and notice that one of your DataNodes has a consistently high disk\nI/O wait time. The disk is almost full. What are the MOST effective actions you can take to address this\nissue and improve performance without adding new hardware immediately? (Select TWO)",
    "options": {
      "A": "Increase the HDFS block size.",
      "B": "Run the HDFS balancer to redistribute data to other DataNodes with more free space.",
      "C": "Delete unused or unnecessary data from the DataNode's disks.",
      "D": "Reduce the replication factor of the HDFS files.",
      "E": "Increase the number of NameNodes in the cluster."
    },
    "correctAnswers": [
      "B",
      "C"
    ],
    "explanation": "Running the HDFS balancer redistributes data to other DataNodes, alleviating the I/O bottleneck on the full disk. Deleting unused data frees up space, directly reducing disk I/O wait time. Increasing block size might help in some cases, but it's not a guaranteed solution and might not be feasible without data migration. Reducing the replication factor reduces data redundancy. Increasing the number of NameNodes does not address the DataNode disk I/O issue."
  },
  {
    "id": 299,
    "question": "A MapReduce job is failing consistently with ‘OutOfMemoryError: Java heap space' on the TaskTrackers\n(running on DataNodes). You suspect the problem is related to the amount of memory available to the\nTaskTrackers. Which configuration parameters should you adjust to increase the memory allocated to\nMapReduce tasks without affecting other services running on the DataNode? (Select TWO)",
    "options": {
      "A": "'mapreduce.map.java.opts",
      "B": "‘mapreduce.reduce.java.opts’",
      "C": "yarn.nodemanager.resource.memory-mb'",
      "D": "‘mapreduce.task.io.sort.mb'",
      "E": "mapreduce.map.memory.mb’ and ‘mapreduce.reduce.memory.mb'"
    },
    "correctAnswers": [
      "A",
      "B"
    ],
    "explanation": "‘mapreduce.map.java.opts’ and ‘mapreduce.reduce.java.opts’ are the correct parameters to adjust. These allow you to increase heap size for map and reduce tasks respectively without affecting other services. Although ‘yarn.nodemanager.resource.memory-mb’ controls the total memory available to YARN containers on the NodeManager, it impacts all containers, not just MapReduce tasks. ‘mapreduce.task.io.sort.mb' controls the amount of memory used for sorting intermediate data, not the overall heap size. ‘mapreduce.map.memory.mb’ and ‘mapreduce.reduce.memory.mb' define the memory requested by the containers, not the java heap options. Java heap options are controlled via the java opts parameter."
  },
  {
    "id": 300,
    "question": "You are setting up a new CDP cluster and want to optimize DataNode disk usage. You have a mix of fast\nSSDs and slower HDDs. How can you configure HDFS to leverage the faster SSDs for frequently accessed\ndata and the slower HDDs for less frequently accessed data? Show Configuration as code snippet. (Single\nAnswer)",
    "options": {
      "A": "Configure HDFS caching using the 'hdfs dfsadmin -storagePolicies’ command, creating a policy that moves data to SSDs based on access frequency.",
      "B": "C. Manually move frequently accessed files to the SSDs using the ‘hdfs dfs -mv’ command.",
      "D": "Create separate HDFS clusters for SSDs and HDDs.",
      "E": "Configure Hadoop Distributed Cache to use SSDs."
    },
    "correctAnswers": [
      "B"
    ],
    "explanation": "The correct answer is to configure 'dfs.datanode.data.diff to specify different storage types (SSD and HDD) and enable storage policies. The configuration allows HDFS to automatically manage data placement based on storage policies. Option A is close, but it doesn't show configuration and it's more of a management command. Manually moving files is not scalable or efficient. Creating separate clusters is unnecessary complexity. Hadoop Distributed Cache is for distributing application-specific files, not general HDFS data."
  },
  {
    "id": 301,
    "question": "You've configured HDFS to utilize erasure coding to reduce storage overhead. However, you observe\nthat read performance for erasure- coded files is significantly slower than for replicated files. What are\nthe PRIMARY factors contributing to this performance difference and what steps can you take to\nmitigate this issue? (Select TWO)",
    "options": {
      "A": "Erasure coding requires reconstruction of data blocks during reads, which involves computation and network I/O.",
      "B": "Erasure coded files are stored in a compressed format, requiring decompression during reads.",
      "C": "Increase the replication factor for erasure-coded files.",
      "D": "Enable short-circuit reads on the DataNodes.",
      "E": "Ensure sufficient network bandwidth and CPU resources on the DataNodes to handle the reconstruction process."
    },
    "correctAnswers": [
      "A",
      "E"
    ],
    "explanation": "Erasure coding inherently introduces overhead due to the need to reconstruct data blocks from parity blocks during reads. This reconstruction involves computation (decoding) and network 1/0 to retrieve the necessary blocks. Ensuring sufficient network bandwidth and CPU resources on the DataNodes is crucial for efficiently handling the reconstruction process. While compression can be used with erasure coding, it's not the primary factor contributing to the performance difference. Increasing the replication factor defeats the purpose of using erasure coding. Short-circuit reads can improve read performance in general but doesn't specifically address the overhead of erasure coding reconstruction."
  },
  {
    "id": 302,
    "question": "You are managing a CDP cluster with a large number of DataNodes. You need to automate the process\nof updating the 'hdfs-site.xml' configuration file on all DataNodes whenever there's a change. What is\nthe BEST approach to achieve this in a scalable and maintainable manner within the Cloudera Manager\nenvironment?",
    "options": {
      "A": "Manually edit the 'hdfs-site.xmr file on each DataNode.",
      "B": "Use a script to copy the 'hdfs-site.xmr file to all DataNodes.",
      "C": "Use Cloudera Manager's configuration management features to modify the *hdfs-site.xmr template and redeploy the DataNode configurations.",
      "D": "Create a symbolic link from each DataNode's 'hdfs-site.xmr file to a central location.",
      "E": "Use Ansible to push the configuration changes to all DataNodes."
    },
    "correctAnswers": [
      "C"
    ],
    "explanation": "Cloudera Manager's configuration management features are designed for managing configuration files across the cluster. Modifying the 'hdfs-site.xmr template in Cloudera Manager and redeploying the configurations ensures that the changes are propagated to all DataNodes in a controlled and auditable manner. It also handles the restart of the DataNodes if necessary. Manual editing and scripting are not scalable or maintainable. Symbolic links can lead to inconsistencies. While Ansible can be used, Cloudera Manager provides a more integrated and manageable solution within the CDP environment."
  },
  {
    "id": 303,
    "question": "You are performing a rolling upgrade of a CDP Private Cloud Base cluster from 7.1.7 to 7.2.17. During the\nupgrade process, the NameNode service fails to start after being restarted. You check the NameNode\nlogs and find the following error: 'java.lang.lllegalArgumentException: Required attribute\n'dfs.namenode.rpc-address' not defined'. What is the most likely cause of this issue?",
    "options": {
      "A": "The HDFS site configuration file was not properly propagated to the NameNode host.",
      "B": "The CM agent on the NameNode host is not running or is unable to communicate with the CM server.",
      "C": "The upgrade process did not properly migrate the NameNode metadata to the new version.",
      "D": "A custom property defined in a safety valve for the NameNode overwrites the required configuration.",
      "E": "The Kerberos principal for the NameNode is not properly configured."
    },
    "correctAnswers": [
      "D"
    ],
    "explanation": "The error message ‘java.lang.lllegalArgumentException: Required attribute 'dfs.namenode.rpc-address' not defined' indicates that a required configuration property is missing or has been unintentionally overridden. A likely cause is a custom property defined in a safety valve that is inadvertently overwriting the necessary configuration. While other options might seem plausible, the error points directly to a configuration problem."
  },
  {
    "id": 304,
    "question": "After upgrading your CDP cluster's Data Hub runtime, you notice that Spark applications are failing with\n'java.lang.NoSuchMethodError: org.apache.spark.sql.SparkSession.builder()'. What are the potential\nreasons for this error? (Select TWO)",
    "options": {
      "A": "The Spark client libraries used by the applications are incompatible with the new Spark version in the Data Hub runtime.",
      "B": "The YARN client libraries used by the applications are incompatible with the new YARN version.",
      "C": "The Spark application is using an older version of the Java Development Kit (JDK).",
      "D": "The Spark configuration properties in ‘spark-defaults.conf are incorrect.",
      "E": "The PATH environment variable on the client machine does not point to the correct Spark installation."
    },
    "correctAnswers": [
      "A",
      "E"
    ],
    "explanation": "The ‘java.lang.NoSuchMethodError’ suggests that the Spark client libraries used by the application are referencing a method ('builder()') that is either missing or has a different signature in the Spark version available on the cluster (A). Inconsistent client environment (E). The PATH envirnoment variable on the client machine does not point to the correct Spark installation can lead to client using old libraries."
  },
  {
    "id": 305,
    "question": "You've initiated an upgrade rollback procedure for the Hive service in your CDP cluster. During the\nrollback, you encounter the following error in the Hive metastore logs:\n'javax.jdo.JDODataStoreException: Error executing SQL query'. Which actions should you consider to\ntroubleshoot this issue?",
    "options": {
      "A": "Verify that the Hive metastore database is accessible and running.",
      "B": "Check for any database schema changes or inconsistencies that might have occurred during the upgrade.",
      "C": "Review the Hive metastore logs for more detailed error messages and stack traces.",
      "D": "Restart the Hive metastore service to clear any transient errors.",
      "E": "All of the above."
    },
    "correctAnswers": [
      "E"
    ],
    "explanation": "The error ‘javax.jdo.JDODataStoreException: Error executing SQL query' during a Hive rollback indicates a problem with the Hive metastore database. All the options provided are valid troubleshooting steps. Verifying database accessibility (A), checking for schema inconsistencies (B), reviewing logs for more details (C), and restarting the service (D) are all important to diagnose and resolve the issue."
  },
  {
    "id": 306,
    "question": "You are using Cloudera Manager to upgrade your CDP cluster You are in the pre-upgrade tasks phase.\nWhich of the following actions is MOST critical to perform before proceeding with the actual upgrade?",
    "options": {
      "A": "Back up all configuration files for each service in the cluster.",
      "B": "Run the Cloudera Manager pre-upgrade health checks and resolve all critical alerts.",
      "C": "Stop all client applications that are accessing the cluster.",
      "D": "Disable all scheduled jobs and workflows.",
      "E": "Clear all temporary directories on the cluster nodes."
    },
    "correctAnswers": [
      "B"
    ],
    "explanation": "Running the Cloudera Manager pre-upgrade health checks (B) is the most critical step. These checks identify potential issues that could cause the upgrade to fail or lead to data loss. Resolving the critical alerts ensures that the cluster is in a stable state before the upgrade begins. While other actions are important, resolving the pre-upgrade health checks is the most crucial."
  },
  {
    "id": 307,
    "question": "During an upgrade of your Cloudera DataFlow (CDF) cluster using Cloudera Manager, the NiFi services\nfail to start after the upgrade is complete. The NiFi logs show a persistent error related to\n'javax.net.ssl.SSLHandshakeException'. What is the most probable cause of this issue, and how would\nyou address it?",
    "options": {
      "A": "The Keystore/Truststore configuration in NiFi is corrupted or misconfigured. Regenerate the Keystore/Truststore using the NiFi toolkit, and update the NiFi properties.",
      "B": "The NiFi Zookeeper quorum is unstable. Restart the Zookeeper services and then try starting the NiFi services.",
      "C": "The NiFi database is corrupted. Restore the NiFi database from a backup taken before the upgrade.",
      "D": "The Java version used by NiFi is incompatible with the upgraded cluster. Update the JAVA_HOME environment variable to point to a compatible JDK.",
      "E": "There are conflicting dependencies in NiFi's lib directory. Clean the lib directory and redeploy the required dependencies."
    },
    "correctAnswers": [
      "A"
    ],
    "explanation": "An 'SSLHandshakeException’ strongly suggests a problem with the SSL/TLS configuration, specifically related to Keystores and Truststores. During an upgrade, these configurations can become invalidated due to changes in security protocols or certificate updates. Regenerating and updating these stores (A) ensures that NiFi can establish secure communication within the cluster."
  },
  {
    "id": 308,
    "question": "You are upgrading a CDP cluster and using the Express Upgrade option. After the upgrade completes,\nyou notice that the Hue service is not functioning correctly and shows '502 Bad Gateway' errors. What is\nthe MOST likely reason for this and how would you quickly verify the cause?",
    "options": {
      "A": "Hue is not compatible with the new version of the cluster. Revert the upgrade and upgrade Hue separately after the core cluster upgrade.",
      "B": "The Hue service did not restart successfully. Check the Hue service logs for errors and restart the service via Cloudera Manager.",
      "C": "The Hue load balancer configuration is incorrect. Verify the load balancer settings and update them if necessary.",
      "D": "The Hue metastore connection is broken. Check the Hue configuration and ensure that the metastore is accessible.",
      "E": "The Hue service depends on other services that did not upgrade correctly. Check all dependent service logs for errors."
    },
    "correctAnswers": [
      "B"
    ],
    "explanation": "A '502 Bad Gateway' error commonly indicates that the upstream server (in this case, the Hue service) is not responding. Checking the Hue service logs is the quickest way to determine if the service failed to start or is experiencing errors preventing it from serving requests. Restarting the service might resolve temporary issues."
  },
  {
    "id": 309,
    "question": "You are preparing to upgrade a CDP 7.1 .x cluster to CDP 7.2.x. Your current cluster heavily utilizes\ncustom YARN queue configurations.\nWhat considerations must you make regarding these custom YARN queue configurations during the\nupgrade process?",
    "options": {
      "A": "Custom YARN queue configurations are automatically migrated during the upgrade. No special action is required.",
      "B": "You must manually back up the ‘yarn-site.xmr configuration file before the upgrade, and restore it after the upgrade is complete.",
      "C": "You should review the YARN queue configurations after the upgrade to ensure they are compatible with the new YARN version and make any necessary adjustments using the YARN Resource Manager U or API.",
      "D": "You need to migrate the existing YARN queue configuration to use the Capacity Scheduler's configuration API before upgrading.",
      "E": "All custom YARN queues will be deleted during the upgrade. You must recreate them after the upgrade."
    },
    "correctAnswers": [
      "C"
    ],
    "explanation": "While upgrades aim to preserve configurations, compatibility issues can arise with custom YARN queues. It's important to review the configurations after the upgrade (C) to ensure they are still valid and functioning correctly with the newer YARN version. This often involves checking queue capacities, access controls, and other parameters."
  },
  {
    "id": 310,
    "question": "You have a CDP Private Cloud Base cluster running on-premise. You are planning to upgrade to a newer\nversion of CDP. Your organization mandates a zero-downtime upgrade. Which upgrade strategy can you\nuse?",
    "options": {
      "A": "Express Upgrade",
      "B": "Rolling Upgrade",
      "C": "Offline Upgrade",
      "D": "Blue/Green Deployment Upgrade",
      "E": "Upgrade via Lift and Shift"
    },
    "correctAnswers": [
      "D"
    ],
    "explanation": "A Blue/Green Deployment Upgrade (D) is the best strategy for zero-downtime upgrades. It involves creating a new, upgraded cluster (Green) alongside the existing cluster (Blue). Once the new cluster is validated, traffic is switched over to it, providing a seamless transition without downtime. Rolling upgrades minimize downtime but may not fully eliminate it, and other methods involve significant downtime."
  },
  {
    "id": 311,
    "question": "You are upgrading your CDP cluster's Data Hub runtime. During the upgrade, the upgrade process fails\nwith a message indicating a conflict in the versions of Hadoop client libraries. What steps can you take to\nresolve this?",
    "options": {
      "A": "Manually remove the older Hadoop client libraries from the nodes and restart the upgrade.",
      "B": "Update the 'HADOOP_CLASSPATH' environment variable to point to the new Hadoop client libraries and retry the upgrade.",
      "C": "Use Cloudera Manager to identify and resolve the version conflicts automatically. Cloudera Manager has built-in conflict resolution mechanisms.",
      "D": "Rollback to the previous version and perform a full cluster restart before attempting the upgrade again.",
      "E": "Ignore the warning and continue the upgrade. Hadoop client library conflicts are usually benign."
    },
    "correctAnswers": [
      "C"
    ],
    "explanation": "Cloudera Manager is designed to manage and resolve dependency conflicts during upgrades. Option C leverages Cloudera Manager's built-in capabilities to handle these types of issues automatically, which is the best and safest approach. The other approaches are not recommended."
  },
  {
    "id": 312,
    "question": "You've upgraded your CDP cluster to a new version. After the upgrade, you observe that the\nperformance of Hive queries has significantly degraded. What are the potential causes of this\nperformance degradation? (Select TWO)",
    "options": {
      "A": "The Hive metastore schema was not upgraded properly.",
      "B": "The Hive configuration properties are not optimized for the new version of Hive.",
      "C": "The underlying storage format of the Hive tables is incompatible with the new Hive version.",
      "D": "The YARN queue configurations are not properly configured for the Hive workload.",
      "E": "The hardware resources allocated to the Hive service are insufficient."
    },
    "correctAnswers": [
      "B",
      "D"
    ],
    "explanation": "Upgrading to a new version of Hive can introduce changes in configuration requirements and YARN resource allocation. The default settings may not be optimized for your specific workload in the new version (B), and changes to YARN configurations can affect the resources available to Hive queries (D). The hardware may also become a bottleneck, but that wouldn't be directly related to the upgrade itself."
  },
  {
    "id": 313,
    "question": "After upgrading your CDP cluster using a rolling upgrade, some of your Spark applications are failing\nintermittently with a 'Container killed by YARN for exceeding memory limits' error. These applications\nwere running successfully before the upgrade. What are the possible root causes? (Select TWO)",
    "options": {
      "A": "The default memory allocation for Spark containers has decreased after the upgrade.",
      "B": "YARN node labels are not correctly configured after the upgrade, causing applications to be scheduled on nodes with insufficient resources.",
      "C": "The Spark application code contains a memory leak that was not previously triggered but is now exposed by changes in the upgraded environment.",
      "D": "The YARN queue used by the Spark applications has a lower memory limit than before the upgrade.",
      "E": "The Spark dynamic allocation feature is disabled."
    },
    "correctAnswers": [
      "C",
      "D"
    ],
    "explanation": "The error indicates that containers are exceeding memory limits set by YARN. While various factors can contribute, a memory leak in the Spark application code (C) would cause it to consume more memory over time, leading to the container being killed. Changes in YARN queue configurations could also reduce the memory available to applications (D), causing them to exceed the new limits. Incorrect node label configuration is less likely to suddenly cause 00M errors after an upgrade."
  },
  {
    "id": 314,
    "question": "During an upgrade of your CDP cluster, you encounter an issue where the HBase upgrade process gets\nstuck in the 'Pre-Upgrade Tasks' state within Cloudera Manager. The logs show errors related to HBase\nregion server connectivity. What steps should you take to diagnose and resolve this?",
    "options": {
      "A": "Force the upgrade to proceed to the next step. Connectivity issues are often transient and resolve themselves.",
      "B": "Check the network connectivity between the Cloudera Manager server and the HBase region servers. Verify DNS resolution and firewall rules.",
      "C": "Restart all HBase region servers simultaneously to resolve any potential temporary connectivity issues.",
      "D": "Increase the timeout values for HBase region server connectivity checks in the Cloudera Manager configuration.",
      "E": "Check if the HBase root directory in HDFS is accessible and has the correct permissions."
    },
    "correctAnswers": [
      "B"
    ],
    "explanation": "The 'Pre-Upgrade Tasks' state getting stuck with connectivity errors strongly suggests a network issue. Option B addresses the most fundamental issue: ensuring the Cloudera Manager server can communicate with the HBase region servers. Verifying network connectivity, DNS resolution, and firewall rules are essential steps to resolve this problem before proceeding with the upgrade."
  },
  {
    "id": 315,
    "question": "You are tasked with patching a Cloudera Data Platform (CDP) on-premises cluster using Cloudera\nManager. Before proceeding, what is the most crucial pre-patching task you should perform to ensure\nminimal downtime and data integrity?",
    "options": {
      "A": "Immediately halt all cluster services to ensure no data corruption during the patch application.",
      "B": "Download the latest Cloudera Manager agent packages from the Cloudera website.",
      "C": "Perform a full backup of the Cloudera Manager Server and all service metadata databases.",
      "D": "Ensure all users are logged out of the cluster to prevent any concurrent modifications.",
      "E": "Disable automatic failover for all highly available services."
    },
    "correctAnswers": [
      "C"
    ],
    "explanation": "Backing up the Cloudera Manager Server and service metadata databases is crucial. This allows for a complete rollback in case the patch application fails or introduces unexpected issues. While other options might seem relevant in specific scenarios, a full backup provides the most comprehensive protection against data loss and cluster instability."
  },
  {
    "id": 316,
    "question": "While applying a patch via Cloudera Manager, you encounter the following error message during the\n'Deploy Client Configuration' step for the Hive service: 'java.net.ConnectException: Connection refused'.\nWhich of the following is the most likely cause of this error?",
    "options": {
      "A": "The Hive Metastore service is unavailable or not responding.",
      "B": "There is a firewall blocking communication between Cloudera Manager Server and the Hive client host.",
      "C": "The Hive service user does not have sufficient permissions to write to the Hive configuration directory.",
      "D": "The Hadoop Distributed File System (HDFS) is in safe mode.",
      "E": "The Cloudera Manager Agent on the Hive client host is not running."
    },
    "correctAnswers": [
      "B"
    ],
    "explanation": "A ‘java.net.ConnectException: Connection refused' typically indicates a network connectivity issue. In this context, it suggests that Cloudera Manager Server is unable to connect to the Hive client host, likely due to a firewall rule preventing the communication."
  },
  {
    "id": 317,
    "question": "After successfully applying a patch to your CDP on-premises cluster, you observe that a custom IJDF\n(User Defined Function) in Hive is no longer working. Which of the following steps are necessary to\ntroubleshoot and resolve this issue? (Select all that apply)",
    "options": {
      "A": "Verify that the IJDF JAR file is still present in the HDFS location specified by ‘hive.aux.jars.patW.",
      "B": "Restart the Hive Metastore service to reload the IJDF definitions.",
      "C": "Recompile the IJDF code against the updated Hive libraries after the patch.",
      "D": "Update the Hive catalog to reflect the new patch version using 'ALTER DATABASE SET DBPROPERTIES ('cdp.version'=\");’",
      "E": "Rollback the entire cluster to the previous version."
    },
    "correctAnswers": [
      "A",
      "B",
      "C"
    ],
    "explanation": "A, B and C are the most appropriate actions. Patches can sometimes affect dependencies or classpaths, so these steps help ensure the UDF is correctly loaded and compatible. The UDF JAR needs to exist in the specified location (A), the metastore needs to be refreshed (B), and recompilation might be required if the underlying Hive libraries have changed (C). D isn’t directly related to UDF functionality, and E is an extreme measure if targeted troubleshooting can fix the problem."
  },
  {
    "id": 318,
    "question": "Which of the following Cloudera Manager roles is required to initiate and execute a patch upgrade\nprocess within a CDP on-premises cluster?",
    "options": {
      "A": "Read-Only User",
      "B": "Operator",
      "C": "Limited Operator",
      "D": "Cluster Administrator",
      "E": "Navigator Administrator"
    },
    "correctAnswers": [
      "D"
    ],
    "explanation": "The Cluster Administrator role has the necessary privileges to manage all aspects of the cluster, including initiating and executing patch upgrades, which involve significant system-level changes."
  },
  {
    "id": 319,
    "question": "You are applying a patch to your Cloudera Data Engineering (CDE) service using Cloudera Manager. After\nthe patch is applied, some Spark applications are failing with 'java.lang.ClassNotFoundException'. What\ncould be the possible reason and how can you address this? (Select all that apply)",
    "options": {
      "A": "The patch introduced a breaking change in the Spark API, requiring you to update the Spark application code.",
      "B": "The Spark driver and executors are not picking up the updated classpath, requiring a restart of the CDE service.",
      "C": "The dependent JAR files for the Spark application are missing from the executor nodes, requiring you to redeploy the application with the necessary dependencies.",
      "D": "The application requires a different version of Java than that on the CDE nodes after the patch.",
      "E": "Disable the CDE service, manually download the missing class files, and copy them to the appropriate lib directory."
    },
    "correctAnswers": [
      "A",
      "B",
      "C"
    ],
    "explanation": "A, B, and C are valid reasons. A patch might introduce incompatible API changes (A), the updated classpath might not be propagating correctly (B), or dependencies might be missing on the executor nodes (C). D is also possible but less frequent than A, B, and C. E is not a recommended approach as it involves manual changes outside the scope of Cloudera Manager and can cause instability."
  },
  {
    "id": 320,
    "question": "During a patch application process, Cloudera Manager displays a warning: 'Rolling restart is required for\nImpala to fully apply the patch'.\nWhat does this warning signify, and what steps should you take to ensure a smooth restart?",
    "options": {
      "A": "A rolling restart means stopping and starting all Impala daemons simultaneously, which can cause significant downtime. You should schedule a full cluster outage.",
      "B": "A rolling restart involves restarting Impala daemons one at a time (or in small groups) to minimize service interruption. Ensure sufficient resources are available and monitor query performance during the restart.",
      "C": "The warning is benign and can be ignored, as Impala will automatically update itself without requiring any manual intervention.",
      "D": "A rolling restart means that only the Impala catalog server needs to be restarted.",
      "E": "You need to manually update the Impala metadata in the Hive metastore before restarting Impala."
    },
    "correctAnswers": [
      "B"
    ],
    "explanation": "A rolling restart is designed to minimize service interruption. Restarting daemons one at a time (or in small groups) allows the service to remain available while the update is applied. Monitoring query performance is critical to ensure the restart doesn't negatively impact users."
  },
  {
    "id": 321,
    "question": "You are preparing to patch a Cloudera Manager deployment that is running on RHEL 7. However, you\nnoticed that the Cloudera Manager agent on one of the host nodes is showing as 'unresponsive' in the\nCloudera Manager UI. What are the most likely causes and the steps to resolve this before initiating the\npatching process? (Select all that apply)",
    "options": {
      "A": "The Cloudera Manager agent process is not running on the host. Use 'systemctl status cloudera-scm- agent' and ‘systemctl start cloudera-scm-agent’ to verify and restart the agent.",
      "B": "There is a network connectivity issue preventing the agent from communicating with the Cloudera Manager server. Verify firewall rules and DNS resolution.",
      "C": "The host's clock is significantly out of sync with the Cloudera Manager server. Use ‘ntpdate’ or ‘chrony’ to synchronize the clocks.",
      "D": "The host's Cloudera Manager agent is using an outdated version of the agent package. Manually upgrade the agent package using ‘yum update cloudera-manager-agent.",
      "E": "The Cloudera Manager server has run out of disk space. Clean up the Cloudera Manager server's logs and database files."
    },
    "correctAnswers": [
      "A",
      "B",
      "C"
    ],
    "explanation": "A, B, and C are the most likely causes. An unresponsive agent can be due to the agent not running (A), network connectivity issues (B), or clock synchronization problems (C). An outdated agent could be a cause, but it's less likely if the cluster has been running smoothly before. A full Cloudera Manager server could impact agent responsiveness indirectly, but the primary focus should be on the agent's health and connectivity."
  },
  {
    "id": 322,
    "question": "You have scheduled a patch upgrade for your CDP on-premises cluster using Cloudera Manager. During\nthe patch process, you want to monitor the progress of the upgrade, including the status of each service\nand host. Which of the following methods or tools can provide the most detailed and real-time insights\ninto the patch upgrade process?",
    "options": {
      "A": "The Cloudera Manager Activity Monitor, which provides a visual representation of the upgrade process and the status of each step.",
      "B": "The ‘cm_api' command-line interface, which allows you to query the Cloudera Manager API for detailed information about the upgrade process.",
      "C": "The system logs on each host in the cluster, which contain detailed information about the upgrade process for each service.",
      "D": "The Hadoop YARN ResourceManager U, which provides information about the resources being used by the upgrade process.",
      "E": "Nagios or other external monitoring tool using metrics from the Cloudera Manager Metrics API."
    },
    "correctAnswers": [
      "A"
    ],
    "explanation": "The Cloudera Manager Activity Monitor provides the most comprehensive and real-time view of the patch upgrade process. It shows the status of each step, any errors encountered, and the overall progress of the upgrade. The can also provide detailed information, but the Activity Monitor is generally more user-friendly for monitoring progress. Other options provide some insight, but not as directly related to the CM managed process of the patching activity itself."
  },
  {
    "id": 323,
    "question": "After patching your CDP cluster, you notice that the Hue service is unable to connect to the Hive\nMetastore. After reviewing the Hue logs, you see a message indicating a Kerberos authentication failure.\nWhat steps are most appropriate to resolve this Kerberos-related connectivity issue? (Select all that\napply)",
    "options": {
      "A": "Ensure the Hue service principal (e.g., 'hue/hostname@REALM') is properly created and the keytab file is correctly configured and accessible by the Hue service account.",
      "B": "Verify that the 'hive.metastore.kerberos.principal' property in the Hive Metastore configuration is correctly set to the Hive Metastore service principal.",
      "C": "Renew the Kerberos ticket for the Hue service account using 'kinit -kt Ipath/to/hue.keytab hue/hostname@REALM' and then restart the Hue service.",
      "D": "Disable Kerberos authentication for the Hive Metastore and rely on simple authentication.",
      "E": "Restart the entire Kerberos Key Distribution Center (KDC)."
    },
    "correctAnswers": [
      "A",
      "B",
      "C"
    ],
    "explanation": "A, B, and C are the correct approaches to troubleshoot Kerberos authentication issues. Ensure the Hue service principal and keytab are correctly configured (A), verify the Hive Metastore principal is set correctly (B), and renew the Kerberos ticket for the Hue service (C). Disabling Kerberos (D) is not a recommended solution in a secure environment. Restarting the entire KDC (E) is overkill and should only be considered as a last resort if other troubleshooting steps fail."
  },
  {
    "id": 324,
    "question": "You are using Cloudera Manager to upgrade a cluster from CDP 7.1.7 to CDP 7.2.16. The upgrade\nprocess fails during the ‘Pre- Upgrade Taskse step with the following error message: 'Incompatible\nschema version detected for table.. Expected version: , Actual version: What are the most effective\nstrategies to address this issue and continue the upgrade process?",
    "options": {
      "A": "Manually upgrade the schema version for the affected table using the SALTER TABLE . COMPACT 'major\" command in Hive.",
      "B": "Revert the cluster back to CDP 7.1.7, drop the affected table, and then recreate it before attempting the upgrade again.",
      "C": "Use the 'hive —service metastore -hiveconf \"datanucleus.autoCreateSchema=true\" -hiveconf \"datanucleus.fixedDatastore=false\" –hiveconf \"datanucleus.autoCreateTables=true\" -updateSchema’ command to automatically update the metastore schema.",
      "D": "Run the 'Upgrade Hive Metastore Schema' command from Cloudera Manager for the Hive service, specifically targeting the database containing the incompatible table.",
      "E": "Ignore the error message and proceed with the upgrade. The schema version will be automatically updated during the post-upgrade tasks."
    },
    "correctAnswers": [
      "D"
    ],
    "explanation": "Running the 'Upgrade Hive Metastore Schema' command from Cloudera Manager is the recommended approach. This command is designed to handle schema upgrades in a managed and controlled manner. Option A might work but is a manual workaround and not the intended upgrade path. Option B is destructive. Option C is closer, but the Cloudera Manager command is preferred and option E is not advisable as that can lead to failures."
  },
  {
    "id": 325,
    "question": "A security vulnerability has been identified in a specific version of Apache Kafka running within your CDP\ncluster. Cloudera has released a patch to address this vulnerability. You have downloaded the patch and\nare preparing to apply it. However, your Kafka cluster is configured with custom interceptors for data\nauditing. What are the most important considerations and steps to take before applying the Kafka patch\nto ensure the custom interceptors continue to function correctly?",
    "options": {
      "A": "Back up the Kafka broker configuration files (e.g., ‘server.properties') before applying the patch. After the patch, manually re-apply any custom configurations related to the interceptors.",
      "B": "Test the custom interceptors in a non-production environment (e.g., a staging cluster) after applying the patch to verify their functionality.",
      "C": "Recompile the custom interceptor code against the updated Kafka libraries included in the patch to ensure compatibility.",
      "D": "Disable the custom interceptors temporarily during the patching process and re-enable them after the patch is complete.",
      "E": "Download the newer version of interceptor jar files."
    },
    "correctAnswers": [
      "B",
      "C"
    ],
    "explanation": "B and C are the most critical steps. Thorough testing in a non-production environment (B) is crucial to identify any issues with the custom interceptors. Recompiling the interceptor code against the updated Kafka libraries (C) is necessary to ensure compatibility with the new Kafka version. A is helpful, but not required. D and E don't guarantee they continue to function as expected after patching."
  },
  {
    "id": 326,
    "question": "You are planning to perform a rolling restart of the HDFS DataNodes after applying a patch. Before\ninitiating the rolling restart, you want to ensure that there is sufficient disk space available on each\nDataNode to handle the data replication that occurs during the restart process. Which of the following\nmethods provides the most accurate and reliable way to assess the available disk space on each\nDataNode before the restart?",
    "options": {
      "A": "Use the command on each DataNode host to check the available disk space. This provides a quick overview of the disk usage.",
      "B": "Use the HDFS Disk Balancer tool to redistribute data evenly across the DataNodes and check the disk utilization after the balancing process.",
      "C": "Use the ‘hdfs dfsadmin -report’ command to generate a report showing the disk space utilization for each DataNode in the cluster.",
      "D": "Monitor the 'HDFS Disk Free' metric in Cloudera Manager for each DataNode role instance. This provides a real-time view of the available disk space.",
      "E": "Consult the NameNode logs for information about free disk space on the DataNodes."
    },
    "correctAnswers": [
      "D"
    ],
    "explanation": "Cloudera Manager’s built-in monitoring provides the most accurate and reliable real-time view of the free disk space on each HDFS DataNode. While options A and C offer insights into disk utilization, Cloudera Manager's metrics provide continuous monitoring and historical data that's essential for proactive management during operations such as rolling restarts."
  },
  {
    "id": 327,
    "question": "You are administering an HDFS cluster with several DataNodes reporting high disk utilization. You need\nto add more storage capacity.\nWhich of the following is the MOST efficient and non-disruptive way to accomplish this?",
    "options": {
      "A": "Add more disks to the existing DataNodes and rebalance the cluster.",
      "B": "Format and add new DataNodes to the cluster.",
      "C": "Decommission existing DataNodes, add disks, then recommission them.",
      "D": "Add a new service role instance of NameNode to balance storage.",
      "E": "Increase the replication factor of existing data."
    },
    "correctAnswers": [
      "A"
    ],
    "explanation": "Adding disks to existing DataNodes and then rebalancing the cluster is the most efficient because it avoids downtime associated with formatting new nodes or decommissioning existing ones. Rebalancing redistributes data evenly across all DataNodes, including the newly added storage."
  },
  {
    "id": 328,
    "question": "You are troubleshooting an HDFS issue where certain files are consistently unavailable. You suspect a\nproblem with block placement. Which of the following HDFS commands would be MOST useful to\nidentify the location of the blocks for a specific file?",
    "options": {
      "A": "hdfs fsck /path/to/file",
      "B": "hdfs dfs -du -h /path/to/file",
      "C": "hdfs dfs -Is /path/to/file",
      "D": "hdfs getconf -confKey fs.defaultFS",
      "E": "hdfs balancer -threshold 5"
    },
    "correctAnswers": [
      "A"
    ],
    "explanation": "The 'hdfs fsck’ command with the path to the file provides detailed information about the health and location of all blocks associated with that file. It allows you to identify if blocks are missing, corrupted, or under-replicated."
  },
  {
    "id": 329,
    "question": "Which of the following parameters in the 'hdfs-site.xmr file directly impacts the replication factor of\nnewly created files in HDFS?",
    "options": {
      "A": "dfs.replication",
      "B": "dfs.blocksize",
      "C": "dfs.namenode.name.dir",
      "D": "dfs.datanode.data.dir",
      "E": "dfs.namenode.rpc-address"
    },
    "correctAnswers": [
      "A"
    ],
    "explanation": "The 'dfs.replication’ parameter in ‘hdfs-site.xmr specifies the default replication factor for newly created files in HDFS. This parameter determines how many copies of each block will be stored across the cluster."
  },
  {
    "id": 330,
    "question": "You observe frequent 'Safe Mode' entries and exits in your HDFS cluster logs. Which of the following\nactions would be the MOST effective first step to diagnose the root cause?",
    "options": {
      "A": "Increase the heap size for the NameNode.",
      "B": "Check the DataNode logs for any errors or connectivity issues.",
      "C": "Run a full HDFS balance operation.",
      "D": "Format the NameNode.",
      "E": "Decommission and recommission all DataNodes."
    },
    "correctAnswers": [
      "B"
    ],
    "explanation": "Frequent Safe Mode entries typically indicate that the NameNode is not receiving enough block reports from the DataNodes. Checking the DataNode logs for errors is the most logical first step to identify connectivity problems, disk issues, or other factors preventing block reports."
  },
  {
    "id": 331,
    "question": "Which of the following statements are true regarding HDFS Federation? (Select TWO)",
    "options": {
      "A": "It allows for horizontal scalability of the NameNode namespace.",
      "B": "It eliminates the need for DataNodes.",
      "C": "It isolates failures to specific namespaces.",
      "D": "It automatically balances data across all DataNodes.",
      "E": "It combines all metadata into a single NameNode for faster access."
    },
    "correctAnswers": [
      "A",
      "C"
    ],
    "explanation": "HDFS Federation allows for horizontal scalability of the NameNode namespace by using multiple independent NameNodes (A). It also isolates failures to specific namespaces because each NameNode manages its own namespace independently (C)."
  },
  {
    "id": 332,
    "question": "You have configured HDFS access control lists (ACLs) for a directory '/data/sensitive’. A user 'alice' needs\nread and execute permissions. How would you grant these permissions using the command line?",
    "options": {
      "A": "hdfs dfs -setfacl -m /data/sensitive",
      "B": "hdfs dfs -setfacl -m user:alice:r-x /data/sensitive",
      "C": "hdfs dfs -chown alice /data/sensitive",
      "D": "hdfs dfs -chmod 755 /data/sensitive",
      "E": "hdfs dfs -setfacl -rwx user:alice /data/sensitive"
    },
    "correctAnswers": [
      "B"
    ],
    "explanation": "The correct command to grant read and execute permissions to user 'alice' is 'hdfs dfs -setfacl -m user:alice:r-x Idata/sensitive’ . The -m' option modifies the ACL, specifies the user, ‘r-x’ defines the permissions (read and execute), and '/data/sensitive’ is the target directory."
  },
  {
    "id": 333,
    "question": "A MapReduce job is failing with 'OutOfMemoryError’. You suspect that the amount of data being\nprocessed by the mappers is too large for the default configuration. Which HDFS setting could you\nadjust to reduce the amount of data read by each mapper?",
    "options": {
      "A": "Increase 'dfs.blocksize’",
      "B": "Decrease 'dfs.replication",
      "C": "Decrease 'dfs.blocksize’",
      "D": "Increase 'dfs.namenode.handler.count'",
      "E": "Decrease 'dfs.datanode.max.locked.memory’"
    },
    "correctAnswers": [
      "C"
    ],
    "explanation": "Decreasing 'dfs.blocksize’ will result in smaller blocks, and potentially more, smaller input splits for the mappers to process. This reduces the amount of data each mapper handles at one time, which may alleviate ‘OutOfMemoryError’ issues."
  },
  {
    "id": 334,
    "question": "You want to configure HDFS to use a custom trash retention policy where files in the trash are\nautomatically deleted after 48 hours. How would you configure this?",
    "options": {
      "A": "Set 'fs.trash.intervar to 2 in ‘core-site.xmr.",
      "B": "Set 'fs.trash.max.age' to 172800 in 'core-site.xml'.",
      "C": "Set ‘fs.trash.classname’ to a custom class implementing the retention policy.",
      "D": "Set 'hadoop.security.impersonation.provider.class’ to a custom class implementing trash policy",
      "E": "Set 'fs.trash.intervar to 48 and 'fs.trash.purge.interval' to 1 in ‘core-site.xml'."
    },
    "correctAnswers": [
      "A"
    ],
    "explanation": "To set the trash retention policy to 48 hours, set 'fs.trash.interval' to 2 in ‘core-site.xmr. The unit of fs.trash.interval is in minutes. 48 hours 60 minutes/hour = 2880 minutes. Then, setting fs.trash.interval=2 will purge the trash files every 2 minutes, for files older than 'fs.trash.interval checklntervar. 'fs.trash.interval checklnterval = 2 minutes 1440 checklnterval = 2880 minutes = 48 hours."
  },
  {
    "id": 335,
    "question": "You are tasked with setting up encryption at rest for HDFS. Which component is responsible for\nmanaging the encryption keys?",
    "options": {
      "A": "DataNode",
      "B": "NameNode",
      "C": "Key Management Server (KMS)",
      "D": "ResourceManager",
      "E": "Zookeeper"
    },
    "correctAnswers": [
      "C"
    ],
    "explanation": "The Key Management Server (KMS) is responsible for managing the encryption keys used for HDFS encryption at rest. The KMS provides an interface for generating, storing, and retrieving encryption keys."
  },
  {
    "id": 336,
    "question": "A critical HDFS directory '/data/important' was accidentally deleted. You have snapshots enabled. What\nsteps are required to restore the directory to its previous state? Assume the snapshot name is\n'pre_delete'.",
    "options": {
      "A": "hdfs dfs -cp /data/important/.snapshot/pre_delete/ Idata/important/",
      "B": "hdfs dfs -restoreSnapshot /data/important pre_delete",
      "C": "hdfs dfs -mv /data/important/.snapshot/pre_delete Idata/important",
      "D": "hdfs dfs -createSnapshot /data/important pre_delete",
      "E": "hdfs dfs -rmdir /data/important"
    },
    "correctAnswers": [
      "A"
    ],
    "explanation": "To restore the directory, copy the contents of the snapshot to the original location using: ‘hdfs dfs –cp /data/important/.snapshot/pre_delete/ /data/importantr. You need to first recreate the /data/important directory using 'hdfs dfs –mkdir /data/important'."
  },
  {
    "id": 337,
    "question": "You have a scenario where a user is complaining that his HDFS writes are extremely slow. After some\ninvestigation, you determine that the disk I/O on one of the DataNodes is saturated. What is the BEST\napproach to immediately alleviate this issue?",
    "options": {
      "A": "Decommission the slow DataNode, allowing the data to be replicated to other nodes.",
      "B": "Increase the replication factor of the files being written.",
      "C": "Rebalance the cluster.",
      "D": "Stop the NameNode and restart it.",
      "E": "Reduce the 'dfs.datanode.max.locked.memory’ on all datanodes"
    },
    "correctAnswers": [
      "A"
    ],
    "explanation": "Decommissioning the slow DataNode is the most immediate solution. This forces the data to be replicated to other DataNodes with available resources, relieving the pressure on the saturated node and improving write performance for the user. A rebalance will also help eventually, but decommission is faster as the data will migrate faster. A rebalance will try to make the data in the cluster balanced, meaning it will attempt to put some data on the node again"
  },
  {
    "id": 338,
    "question": "Assume that you have an HDFS cluster where NameNode HA is enabled. You observe that the Active\nNameNode is constantly switching to Standby and vice-vers\na. Which of the following could be the root causes for this frequent failover? (Select THREE)",
    "options": {
      "A": "ZooKeeper quorum issues.",
      "B": "Network connectivity problems between the Active and Standby NameNodes.",
      "C": "A bug in the operating system kernel on the NameNode servers.",
      "D": "Incorrect configuration of the Failover Controller.",
      "E": "High disk I/O on the DataNodes."
    },
    "correctAnswers": [
      "A",
      "B",
      "D"
    ],
    "explanation": "Frequent NameNode failover can be caused by issues with the ZooKeeper quorum (A), network connectivity problems (B), and incorrect configuration of the Failover Controller (D). The DataNode 1/0 and OS are unlikely."
  },
  {
    "id": 339,
    "question": "You are tasked with optimizing YARN resource allocation for a cluster running diverse workloads,\nincluding batch processing and interactive queries. You notice that interactive queries are frequently\ndelayed due to resource contention. Which of the following YARN features would be MOST effective in\nmitigating this issue?",
    "options": {
      "A": "Using only a single FIFO scheduler.",
      "B": "Implementing Capacity Scheduler with dedicated queues and resource guarantees for interactive queries.",
      "C": "Disabling preemption in the Fair Scheduler.",
      "D": "Increasing the yarn.scheduler.minimum-allocation-mb to a very high value.",
      "E": "Relying solely on the default resource settings for all applications."
    },
    "correctAnswers": [
      "B"
    ],
    "explanation": "The Capacity Scheduler allows you to create dedicated queues with guaranteed resources. This ensures that interactive queries always have access to the necessary resources, reducing delays. The FIFO scheduler doesn't provide prioritization. Disabling preemption can exacerbate contention. Increasing minimum allocation can waste resources. Relying on default settings rarely optimizes diverse workloads."
  },
  {
    "id": 340,
    "question": "A YARN application is consistently failing with an 'OutOfMemoryError'. After inspecting the application\nlogs, you suspect that the memory allocated to the YARN container is insufficient. Which YARN\nconfiguration parameter should you adjust to increase the maximum memory allocated to the\ncontainers managed by a specific queue in the Capacity Scheduler?",
    "options": {
      "A": "yarn.scheduler.capacity..maximum-allocation-mb",
      "B": "yarn.nodemanager.resource.memory-mb",
      "C": "yarn.scheduler.maximum-allocation-mb",
      "D": "yarn.scheduler.capacity..maximum-am-resource-percent",
      "E": "yarn.scheduler.capacity..capacity"
    },
    "correctAnswers": [
      "D"
    ],
    "explanation": "The ‘yarn.scheduler.capacity..maximum-allocation-mb' parameter controls the maximum memory that can be allocated to containers within a specific queue. \"yarn.nodemanager.resource.memory-mb' defines the total memory available on a NodeManager. yarn.scheduler.maximum-allocation-mb& represents the overall maximum for the entire scheduler, not per queue. ‘maximum-am-resource- percent’ relates to Application Master resources. Queue capacity deals with proportional resource allocation, not maximum memory."
  },
  {
    "id": 341,
    "question": "You are using the Fair Scheduler with preemption enabled. A large application starts and consumes most\nof the cluster resources. Later, a smaller, high-priority application is submitted. What will happen?",
    "options": {
      "A": "The high-priority application will be placed in a pending state indefinitely.",
      "B": "The large application will be immediately killed to free up resources for the high-priority application.",
      "C": "The Fair Scheduler will preempt containers from the large application to make resources available for the high-priority application, after a certain delay.",
      "D": "The Fair Scheduler will evenly distribute remaining resources among all running applications, regardless of priority.",
      "E": "The high-priority application will be assigned to a different cluster if one is available."
    },
    "correctAnswers": [
      "C"
    ],
    "explanation": "With preemption enabled, the Fair Scheduler will preempt (i.e., reclaim) containers from lower-priority applications to satisfy the resource demands of higher-priority applications. This happens after a configurable delay to avoid excessive preemption. The large application will not be immediately killed, but rather have containers taken away gradually. The Scheduler will allocate resources based on priority, not distribute them evenly."
  },
  {
    "id": 342,
    "question": "You need to configure YARN node labels for a specific set of nodes in your cluster. You want to ensure\nthat applications requesting a particular label only run on those nodes. Which of the following steps are\nnecessary to achieve this? (Select TWO)",
    "options": {
      "A": "Enable node labels globally in YARN configuration ('yarn.node-labels.enabled=true').",
      "B": "Configure the application to explicitly request the desired node label using ‘yarn.application.node- label-expression’ .",
      "C": "Disable all other queues in the Capacity Scheduler.",
      "D": "Assign the node labels to the desired nodes using the YARN CLI or REST API.",
      "E": "Remove the ‘yarn-site.xml' file from all NodeManagers."
    },
    "correctAnswers": [
      "A",
      "D"
    ],
    "explanation": "To use node labels, you must first enable them globally ('yarn.node-labels.enabled=true'). Then, you must assign the labels to specific nodes using the YARN CLI or REST API. The application must then request the label. Disabling queues or removing ‘yarn-site.xmr will break YARN functionality."
  },
  {
    "id": 343,
    "question": "A MapReduce job is consistently slow, and you suspect that data locality is a contributing factor. Which\nYARN parameter can you tune to improve data locality by influencing how YARN allocates containers to\nnodes?",
    "options": {
      "A": "yarn.nodemanager.resource.cpu-vcores",
      "B": "yarn.nodemanager.resource.memory-mb",
      "C": "yarn.resourcemanager.scheduler.locality.delay",
      "D": "yarn.nodemanager.aux-services",
      "E": "yarn.nodemanager.disk-health-checker.max-disk-utilization-per-disk-percentage"
    },
    "correctAnswers": [
      "C"
    ],
    "explanation": "'yarn.resourcemanager.scheduler.locality.delay’ controls how long the Resource Manager will wait for a node-local or rack-local container assignment before relaxing the locality constraint. Tuning this parameter can improve data locality by giving YARN more time to find a node with the data. The other options affect CPU cores, memory, auxiliary services, and disk utilization, respectively, but not data locality directly."
  },
  {
    "id": 344,
    "question": "Which of the following commands is used to submit a YARN application with specific resource\nrequirements using the command line?",
    "options": {
      "A": "yarn application -jar myapp.jar -memory 4G -vcores 2",
      "B": "yarn jar myapp.jar —resource memory=4096,vcores=2",
      "C": "hadoop jar myapp.jar —resource memory=4096,vcores=2",
      "D": "hadoop jar myapp.jar -D mapreduce.map.memory.mb=4096 -D mapreduce.map.cpu.vcores=2",
      "E": "yarn application -submit -jar myapp.jar -am memory:4096,vcores:2"
    },
    "correctAnswers": [
      "C"
    ],
    "explanation": "The 'hadoop jar’ command is used to submit applications to YARN. The '—resource' option allows specifying resource requirements like memory and vcores. The other options are incorrect because they use incorrect syntax or command names. Option D configures settings that are specific to MapReduce applications only, not general YARN applications."
  },
  {
    "id": 345,
    "question": "You have enabled the Fair Scheduler and configured a placement policy to automatically place\napplications into queues based on their user and group. However, some applications are not being\nplaced into the correct queues. What is the MOST likely cause?",
    "options": {
      "A": "The application is specifying a queue name explicitly, overriding the placement policy.",
      "B": "The Fair Scheduler is not compatible with user and group-based placement policies.",
      "C": "The Capacity Scheduler is also enabled and interfering with the Fair Scheduler.",
      "D": "The ‘yarn-site.xml' file is corrupted.",
      "E": "The placement policy is configured to use the FIFO scheduler."
    },
    "correctAnswers": [
      "A"
    ],
    "explanation": "If an application explicitly specifies a queue name (e.g., using ‘-D mapreduce.job.queuename’ or similar), it will override the Fair Scheduler's placement policy. You need to ensure that applications are not explicitly specifying queue names if you want the placement policy to take effect. The Fair Scheduler does support user and group-based placement. Having both schedulers active will cause problems, but the manual queue specification is more common. A corrupted ‘yarn-site.xmr file would likely cause more widespread issues."
  },
  {
    "id": 346,
    "question": "You are monitoring your YARN cluster and notice that some nodes are consistently underutilized, while\nothers are heavily loaded. You suspect that node labels might not be configured correctly. How can you\nverify the assigned node labels and resource usage on each node?",
    "options": {
      "A": "Using the 'hdfs dfsadmin -report' command.",
      "B": "Using the YARN Resource Manager web U and navigating to the Nodes section.",
      "C": "Inspecting the Vvar/log/hadoop-yarn/nodemanager/yarn-nodemanager- .log’ files on each node.",
      "D": "Using the \"yarn node -list' command and analyzing the output.",
      "E": "By logging into each NodeManager and running 'free -m' to check memory utilization."
    },
    "correctAnswers": [
      "B"
    ],
    "explanation": "The YARN Resource Manager web U provides a comprehensive view of the cluster, including node labels, resource usage, and other relevant information. The 'hdfs dfsadmin -report' command is for HDFS information. While the NodeManager logs might contain some information, they are not the most efficient way to get a cluster-wide view. ‘yarn node -list' is valid, but the web U provides more detailed information in a more accessible format. ‘free -m’ only shows memory utilization on a single node, not node labels or a cluster view."
  },
  {
    "id": 347,
    "question": "A YARN application requires a specific version of a library that is not available on all nodes in the cluster.\nHow can you ensure that this library is available to the application's containers running on any node?",
    "options": {
      "A": "Install the library globally on all nodes in the cluster.",
      "B": "Package the library along with the application's JAR file and configure YARN to distribute it to the containers.",
      "C": "Create a symbolic link to the library on the HDFS filesystem.",
      "D": "Configure YARN to download the library from a remote repository at runtime.",
      "E": "Use the ‘Idconfig’ command on all nodes to update the shared library cache."
    },
    "correctAnswers": [
      "B"
    ],
    "explanation": "The best practice is to package the required libraries along with the application's JAR file. YARN can then distribute these files to the containers' working directories. This ensures that the application has access to the specific version of the library it needs, without requiring global installation or relying on symbolic links. Global installation can lead to version conflicts. Downloading from a remote repository at runtime can be unreliable. Mdconfig’ updates the system's shared library cache, but it doesn't guarantee availability within YARN containers."
  },
  {
    "id": 348,
    "question": "You have a critical YARN application that needs to be restarted automatically if it fails. Which YARN\nfeature should you configure to achieve this?",
    "options": {
      "A": "YARN Federation.",
      "B": "ApplicationMaster Restart.",
      "C": "Container Preemption.",
      "D": "Resource Localization.",
      "E": "NodeManager Heartbeat."
    },
    "correctAnswers": [
      "B"
    ],
    "explanation": "ApplicationMaster Restart is specifically designed to automatically restart the ApplicationMaster (the main process coordinating the application) if it fails. This provides fault tolerance and ensures that critical applications can recover from failures. YARN Federation is for scaling across multiple clusters. Container preemption is for reclaiming resources. Resource localization is about making resources available to containers. NodeManager heartbeat is about monitoring node health."
  },
  {
    "id": 349,
    "question": "You're encountering 'Container killed by YARN for exceeding memory limits.' errors. You've already\nincreased 'yarn.scheduler.maximum- allocation-mb’ and 'yarn.nodemanager.resource.memory-mb'.\nHowever, the problem persists. Which of the following is the MOST likely reason for this continued\nfailure? (Select TWO)",
    "options": {
      "A": "The application is not requesting enough memory for its containers.",
      "B": "The application is exceeding its requested memory limits, and YARN is correctly killing the container.",
      "C": "The YARN queue's smaximum-allocation-mb' is set lower than the application's request.",
      "D": "There is insufficient disk space on the NodeManager's local directories.",
      "E": "The NodeManager's 'yarn-site.xml' is out of sync with the ResourceManager's configuration."
    },
    "correctAnswers": [
      "B",
      "C"
    ],
    "explanation": "If containers are being killed for exceeding memory limits even after increasing global memory settings, it's most likely because the application is exceeding its requested memory. YARN enforces these limits. Also, the queue's ‘maximum-allocation-mb' could be lower than the application's request, effectively capping the memory available to containers in that queue. Insufficient disk space might cause other errors, but memory limits trigger the container kill message. NodeManager configuration being out of sync would also cause problems, but its less likely. Option A is invalid since its already exceeding the memory limit."
  },
  {
    "id": 350,
    "question": "You want to use YARN Resource Profiles to simplify resource requests for your applications. How can\nyou define and manage these profiles within your YARN cluster?",
    "options": {
      "A": "By editing the ‘core-site.xmr file and adding custom properties.",
      "B": "By using the YARN CLI and creating a properties file with the profile definitions.",
      "C": "Resource profiles are not supported in Cloudera CDP On-premise.",
      "D": "By using the YARN REST API to create, update, and manage the profiles.",
      "E": "By defining the profiles directly within the application code."
    },
    "correctAnswers": [
      "D"
    ],
    "explanation": "YARN Resource Profiles are typically managed using the YARN REST API. This allows you to define and manage different profiles with specific resource configurations (memory, CPU, etc.). The application can then refer to these profiles when submitting jobs, simplifying resource requests. Editing ‘core-site.xmr is not the correct approach. While some YARN configurations are done through XML files, resource profiles are dynamic and managed through the REST API. Profiles cannot be created using the YARN CLI or defined within the application code."
  }
]