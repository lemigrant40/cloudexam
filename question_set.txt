Question: 1
Which of the following components are essential for the functioning of the Cloudera Data Platform
(CDP) on-premises Data Lake, focusing on data storage and metadata management?
A. HDFS (Hadoop Distributed File System) for data storage.
B. Hive Metastore for metadata management.
C. YARN (Yet Another Resource Negotiator) for resource management.
D. ZooKeeper for coordinating distributed processes.
E. Kafka for real-time data streaming.
Answer: A,B,C,D
Explanation:
HDFS provides distributed storage, Hive Metastore manages metadata, YARN manages resources and
ZooKeeper coordinates distributed processes are all essential for a functioning Data Lake. Kafka is more
related to streaming ingest rather than core Data Lake functionality, though it may be part of the overall
architecture. The question specifically asks about Data Lake functioning.
Question: 2
You are configuring resource pools in YARN for different departments in your organization. The
marketing department requires a guaranteed minimum of 40% of the cluster's resources, while the
engineering department requires at least 50%. What is the most appropriate configuration for the
'yarn.scheduler.capacity.maximum-am-resource-percent' property in 'yarn-site.xmP , considering
efficient resource utilization and potentially variable workloads?
A. Set 'yarn.scheduler.capacity.maximum-am-resource-percent’ to 0.1 (10%).
B. Set 'yarn.scheduler.capacity.maximum-am-resource-percent’ to 0.4 (40%).
C. Set 'yarn.scheduler.capacity.maximum-am-resource-percent’ to 0.5 (50%).
D. Set ‘yarn.scheduler.capacity.maximum-am-resource-percent’ to 0.9 (90%).
E. Set 'yarn.scheduler.capacity.maximum-am-resource-percent’ to 1.0 (100%).
Answer: D
Explanation:
‘yarn.scheduler.capacity.maximum-am-resource-percent' defines the maximum percentage of resources
in the cluster that can be used for ApplicationMasters (AMs). While marketing and engineering require
40% and 50% respectively of overall cluster resources, the AMs need resources too. The maximum
usage percentage should allow enough space for the Application Master processes to start for a
resource intensive job. Setting this to a lower value might lead to resource starvation for the application
masters.
Question: 3
A critical Hive query that performs complex aggregations on a large dataset is consistently slow. You
suspect the issue is related to suboptimal data partitioning and bucketing. How can you re-design the
Hive table definition to improve query performance?
A. Remove partitioning and bucketing altogether to simplify the table structure.
B. Partition the table by a low-cardinality column and bucket it by a high-cardinality column frequently
used in 'JOIN’ or ‘GROUP BY operations.
C. Partition the table by a high-cardinality column and bucket it by a low-cardinality column.
D. Use dynamic partitioning with a very large number of partitions.
E. Bucket the table only, with a very small number of buckets.
Answer: B
Explanation:
Partitioning by a low-cardinality column reduces the amount of data scanned for queries filtering on that
column. Bucketing by a high- cardinality column frequently used in ‘JOIN’ or ‘GROUP BY operations
distributes the data evenly across buckets, allowing for parallel processing and improved join
performance (bucket map joins). Using high cardinality column as partition would result in large number
of small files and low cardinality column as bucket will result in data skewness. Dynamic partitioning is
useful but can be inefficient if not managed properly.
Question: 4
Which of the following statements accurately describe the role of the Cloudera Manager Agent in a CDP
on-premises cluster?
A. The Cloudera Manager Agent is responsible for distributing the Cloudera Manager Server software
across the cluster.
B. The Cloudera Manager Agent collects metrics and status information from the services running on its
host and reports them to the Cloudera Manager Server.
C. The Cloudera Manager Agent executes commands issued by the Cloudera Manager Server, such as
starting, stopping, and reconfiguring services.
D. The Cloudera Manager Agent is only required on the Cloudera Manager Server host.
E. The Cloudera Manager Agent automatically optimizes query performance without direct intervention.
Answer: B,C
Explanation:
Cloudera Manager Agents monitor the services on each host, reporting back to the Cloudera Manager
Server, and execute commands issued by the Server. They don't distribute the Server software, and are
needed on all cluster hosts, not just the Server host. They don't directly optimize query performance
without configuration.
Question: 5
You are configuring Kerberos authentication for your CDP on-premises cluster After enabling Kerberos,
you notice that some of your existing Spark applications are failing with authentication errors. What are
the likely causes and how do you resolve them?
A. The Spark application is not configured to use Kerberos credentials. You need to configure the
‘spark.yarn.principar and ‘spark.yarn.keytab' properties in the Spark configuration.
B. The Kerberos ticket-granting ticket (TGT) has expired on the client machine. You need to run 'kinit' to
C. The necessary Java Cryptography Extension (JCE) Unlimited Strength Jurisdiction Policy Files are not
installed on the Spark driver and executor nodes.
D. The clock skew between the Kerberos Key Distribution Center (KDC) and the Spark nodes is too large.
You need to synchronize the clocks using NTP (Network Time Protocol).
E. Spark applications automatically inherit Kerberos authentication settings from the operating system;
no specific configuration is required.
Answer: A,B,C,D
Explanation:
Spark applications need to be explicitly configured to use Kerberos with correct principal and keytab.
Expired TGTs prevent authentication. JCE policy files are required for strong encryption used by
Kerberos. Clock skew can cause authentication failures. So all options except E are correct.
Question: 6
Your Cloudera Data Platform (CDP) on-premises cluster is experiencing frequent HDFS NameNode
failures, leading to data unavailability. You want to implement High Availability (HA) for the NameNode.
What steps are essential to configure NameNode HA using Quorum Journal Manager (QJM)?
A. Install and configure a single ZooKeeper server.
B. Configure two NameNode hosts and share a single edit log directory between them.
C. Install and configure a quorum of JournalNodes (typically 3 or more) to store the edit log.
D. Configure the 'dfs.nameservices’ and 'dfs.ha.namenodes.[nameserviceld]' properties in 'hdfs-site.xmr.
E. Format the NameNode using the '-format' option on both NameNode hosts.
Answer: C,D
Explanation:
QJM requires a quorum of JournalNodes to store the edit log redundantly. You also need to configure
'dfs.nameserviceS and dfs.ha.namenodes.[nameserviceld]' to enable HA. A single ZooKeeper server is
not enough for HA. NameNodes should not share a single edit log directory. Formatting with -format
should be done only on the initial NameNode setup and not on both after configuring HA.
Question: 7
A user reports that they are unable to access a specific directory in HDFS, even though they believe they
have the correct permissions. You need to troubleshoot the access issue. Which of the following actions
would be most helpful in diagnosing the problem?
A. Check the HDFS audit logs to see if the user's access attempts are being logged and if any errors are
reported.
B. Use the ‘hdfs dfs -Is -R command to recursively list the directory and its contents, paying attention to
the permissions of each file and directory.
C. Verify the user's group membership and check if any ACLs (Access Control Lists) are configured on the
directory or its parent directories.
D. Restart the NameNode to refresh the permissions cache.
E. Increase the HDFS replication factor to improve data availability.
Answer: A,B,C
Explanation:
Checking audit logs provides insights into access attempts and errors. Listing the directory recursively
shows the permissions of each file and directory. Verifying group membership and ACLs confirms if the
user has the necessary permissions. Restarting the NameNode is unlikely to resolve a permission issue,
and increasing the replication factor does not affect access control.
Question: 8
You are planning a CDP on-premises deployment and need to choose the appropriate hardware for your
HDFS DataNodes. Your workload consists of large sequential reads and writes. Which storage
configuration would provide the best performance and cost- effectiveness?
A. Solid-state drives (SSDs) in a RAID 0 configuration.
B. High-capacity SATA hard disk drives (HDDs) in a RAID 10 configuration.
C. Small-capacity SAS hard disk drives (HDDs) in a RAID 5 configuration.
D. A mix of SSDs for frequently accessed data and HDDs for less frequently accessed data, leveraging
HDFS tiering.
E. NVMe drives in JBOD (Just a Bunch of Disks) configuration.
Answer: B
Explanation:
For large sequential reads and writes, high-capacity HDDs in RAID 10 offer a good balance of
performance and cost-effectiveness. SSDs are faster but more expensive and may not be necessary for
sequential workloads. RAID 5 has write performance limitations. HDFS tiering is a good option for mixed
workloads, but the question specifies a sequential workload. NVMe drives in JBOD are very fast, but lack
redundancy unless HDFS replication covers it. So RAIDIO configuration of HDDs is the most appropriate
answer here.
Question: 9
You observe high disk I/O utilization on your HDFS DataNodes, impacting overall cluster performance.
Which of the following techniques can help reduce disk I/O and improve performance?
A. Increase the HDFS replication factor.
B. Enable HDFS data compression (e.g., using Snappy or LZO).
C. Adjust the 'dfs.datanode.handler.count' property to increase the number of DataNode worker
threads.
D. Implement HDFS caching to store frequently accessed data in memory.
E. Reduce the block size of HDFS files.
Answer: B,C,D
Explanation:
Enabling HDFS data compression reduces the amount of data written to disk. Adjusting
'dfs.datanode.handler.count can increase the number of DataNode worker threads that handle I/O
requests. Implementing HDFS caching stores frequently accessed data in memory, reducing disk I/O.
Increasing the replication factor increases disk I/O, and reducing the block size can lead to more
metadata overhead.
Question: 10
You are tasked with setting up a new CDP on-premises cluster. You want to ensure that data is
protected against node failures. What is the minimum recommended replication factor for HDFS data
blocks to provide fault tolerance?
A. 1
B. 2
C. 3
D. 4
E. 5
Answer: C
Explanation:
The minimum recommended replication factor for HDFS is 3. This ensures that data is stored on at least
three different DataNodes, providing fault tolerance against node failures.
Question: 11
You've configured Ranger for centralized access control in your CDP cluster. You need to grant a specific
user, 'data_scientist', read access to all tables within a particular Hive database named 'analytics db'.
What is the most efficient and recommended way to achieve this using Ranger?
A. Grant the user 'data_scientist' direct access to each table in 'analytics_db' individually using Hive's
'GRANT statement.
B. Create a Ranger policy that grants read access to the 'analytics db' database to the user
'data_scientist'. This will automatically apply to all tables within the database.
C. Modify the HDFS permissions of the underlying data directories for the 'analytics db' database to
grant read access to the user 'data_scientist'.
D. Create a new Linux group, add the user 'data_scientist' to the group, and then grant the group read
access to the 'analytics_db' database using Ranger.
E. Grant the user 'data_scientist' the ‘ALL' privilege on the 'analytics_db' database in Hive.
Answer: B
Explanation:
Ranger policies can be defined at the database level, granting access to all tables within that database.
This is the most efficient and recommended approach. Granting individual table access via Hive is
tedious. Modifying HDFS permissions bypasses Ranger's centralized control. A Linux group can be used,
but specifying the user directly is simpler if only one user needs access. Granting privilege would give
more access than requested.
Question: 12
Your organization needs to ingest streaming data from multiple sources (e.g., web server logs, sensor
data) into your CDP on-premises cluster for real-time analytics. Which of the following components is
best suited for this purpose, considering scalability, fault tolerance, and integration with other CDP
services?
A. Flume
B. Sqoop
C. Kafka
D. Oozie
E. HBase
Answer: C
Explanation:
Kafka is a distributed, fault-tolerant streaming platform that can handle high volumes of data from
multiple sources. Flume is another option but is generally less scalable than Kafka. Sqoop is for batch
data import. Oozie is a workflow scheduler. HBase is a NoSQL database. Therefore, Kafka fits this
scenario best.
Question: 13
Which of the following components are essential for a highly available Cloudera Manager deployment?
A. A single Cloudera Manager Server instance.
B. An external database (e.g., PostgreSQL, MySQL) for storing Cloudera Manager metadata.
C. A load balancer to distribute traffic between multiple Cloudera Manager Server instances.
D. A shared file system (e.g., NFS) for storing Cloudera Manager agent data.
E. ZooKeeper for leader election and coordination among Cloudera Manager Server instances.
Answer: B,C,E
Explanation:
A highly available Cloudera Manager deployment requires an external database to store metadata, a
load balancer to distribute traffic, and ZooKeeper for coordination and leader election between
Cloudera Manager Server instances. A single instance (A) defeats the purpose of HA. While a shared file
system can be used, it's not strictly essential for basic HA of the CM server itself. Agent data is managed
differently.
Question: 14
You're deploying Cloudera Manager and want to configure the database. Which database types are
officially supported for production environments in the latest CDP release?
A. Embedded Derby database.
B. MySQL/MariaDB.
C. PostgreSQL.
D. Oracle.
E. SQLite
Answer: B,C,D
Explanation:
Embedded Derby (A) is not supported for production. SQLite (E) also not supported for production,
while MySQL/MariaDB, PostgreSQL, and Oracle are officially supported database options for production
Cloudera Manager deployments.
Question: 15
What is the primary function of the Cloudera Manager Agent in the CDP on-premises architecture?
A. To provide a web-based UIfor cluster management.
B. To monitor and manage services on the host it is installed on, as instructed by the Cloudera Manager
Server.
C. To act as a central repository for all cluster logs.
D. To perform automatic software updates on all cluster nodes.
E. To authenticate users accessing the cluster.
Answer: B
Explanation:
The Cloudera Manager Agent is responsible for executing commands, monitoring services, and reporting
status back to the Cloudera Manager Server. It doesn't provide the UI(A), centrally store logs (C),
automatically update software (D) or handle authentication directly (E).
Question: 16
You observe high CPU utilization on the Cloudera Manager Server Which of the following actions could
help alleviate this issue?
A. Increase the heap size allocated to the Cloudera Manager Server Java process.
B. Reduce the monitoring frequency of the Cloudera Manager Agents.
C. Enable the Cloudera Manager API throttling to limit excessive API calls.
D. Migrate the Cloudera Manager Server to a more powerful host.
E. Disable HTTPS for the CM UI.
Answer: A,B,C,D
Explanation:
Increasing heap size, reducing monitoring frequency, enabling API throttling, and migrating to a more
powerful host can all help reduce CPU utilization on the Cloudera Manager Server. Disabling HTTPS,
while possibly reducing some CPU load related to encryption, is not a recommended or significant
solution and creates a security risk.
Question: 17
How can you configure Cloudera Manager to send alerts via email when a service goes down?
A. By configuring an SMTP server in the Cloudera Manager settings and defining alert publishing rules.
B. By installing a separate email client on the Cloudera Manager Server.
C. By using the 'cm-api' command-line tool to configure email notifications.
D. By configuring an external monitoring system (e.g., Nagios) to poll Cloudera Manager's API for service
status.
E. Email alerts are not supported in CDP on-premises.
Answer: A
Explanation:
Cloudera Manager has built-in email alert functionality. You configure an SMTP server in the Cloudera
Manager settings and then create alert publishing rules that specify when and how to send email
notifications. While option D is a valid approach, it is not directly configuring Cloudera Manager.
Question: 18
After upgrading Cloudera Manager, you notice that the agents are continuously disconnecting and
reconnecting. What is the MOST likely cause?
A. Incorrect firewall settings are blocking communication between the Cloudera Manager Server and the
Agents.
B. The Cloudera Manager Agents were not upgraded to the same version as the Cloudera Manager
Server.
C. The time is not synchronized between the Cloudera Manager Server and the Agents.
D. Insufficient memory allocated to the Cloudera Manager Agents.
E. The CM server has run out of disk space.
Answer: B
Explanation:
A version mismatch between the Cloudera Manager Server and the Agents is a common cause of
connection issues after an upgrade. While other options could contribute, a version mismatch is the
most likely root cause in this scenario. Firewall issues and time synchronization problems usually exist
before the upgrade.
Question: 19
Which of the following statements accurately describe the role of the Cloudera Management Service
(CMS)?
A. It's a single monolithic service responsible for all cluster management tasks.
B. It's a collection of services (e.g., Activity Monitor, Host Monitor, Reports Manager) that provides
monitoring, reporting, and diagnostic capabilities for the CDP cluster.
C. It is required only for clusters with Kerberos enabled.
D. It acts as a proxy server for all communication between the Cloudera Manager Server and the Agents.
E. The CMS is not necessary if using Cloudera Data Platform Private Cloud Base.
Answer: B
Explanation:
The Cloudera Management Service (CMS) is a suite of services responsible for monitoring, reporting, and
diagnostics within the CDP cluster. It is not a single monolithic service (A), is required regardless of
Kerberos (C & E), and doesn't act as a proxy (D).
Question: 20
You need to configure a custom alert in Cloudera Manager that triggers when the disk utilization on a
specific host exceeds 90%. Where within Cloudera Manager would you configure this?
A. Under the 'Roles' tab for the specific host.
B. Under the 'Configuration' tab for the Cloudera Management Service, then creating a new Metric
Monitor.
C. Under the 'Hosts' tab, selecting the host, and then creating a new 'Host Template'.
D. Using the 'cm-api' command-line tool with a custom Python script.
E. Alerts cannot be configured at the host level. They must be configured at the service level.
Answer: B
Explanation:
Custom alerts, including those based on host metrics like disk utilization, are configured within the
Cloudera Management Service settings, specifically by creating a new Metric Monitor. You define the
metric to monitor (disk utilization), the threshold (90%), and the triggering conditions.
Question: 21
Which configuration setting directly impacts the amount of historical data that Cloudera Manager stores
for metrics and monitoring purposes?
A. The java_heapsize’ setting for the Cloudera Manager Server.
B. The ‘event.expiry’ setting in the Cloudera Manager database configuration.
C. The metrics time to_live’ setting in the Cloudera Management Service configuration.
D. The setting in the Cloudera Manager Agent configuration.
E. The max concurrent_scanS setting for YARN.
Answer: C
Explanation:
The setting within the Cloudera Management Service configuration directly controls how long Cloudera
Manager retains historical metrics data. A shorter TTL will save disk space but limit the scope of
historical analysis. Setting B controls the event expiry for activity records within CM. Not metric data.
Question: 22
You are facing performance issues with the Cloudera Manager UI. You suspect that the queries to the
CM database are slow What tool can you use to analyze the database performance and identify slow
queries?
A. The Cloudera Manager UIitself provides detailed database performance metrics.
B. You can use standard database monitoring tools specific to your database type (e.g., ‘mysqltop’ for
MySQL, for PostgreSQL).
C. The ‘cm-audit’ command-line tool provides detailed database query logs.
D. The Cloudera Diagnostic Bundle automatically captures database performance snapshots.
E. The 'jstack' command can be used on the CM server process to identify database bottlenecks.
Answer: B
Explanation:
While Cloudera Manager provides some performance metrics, detailed database performance analysis
requires using database- specific monitoring tools. "mysqltop', (or similar tools for Oracle) are
appropriate for diagnosing slow queries. Other options are not specific to database query analysis.
‘jstacks’ is for thread dumps.
Question: 23
Your Cloudera Manager server runs out of disk space. Which of the following directories should you
investigate first to free up space?
A. /opt/cloudera/cm/run/cloudera-scm-server/cmf/reports
B. /var/log/cloudera-scm-server
C. /var/lib/cloudera-scm-server
D. /opt/cloudera/parcel-cache
E. /tmp
Answer: A,B
Explanation:
Ivar/log/cloudera-scm-server stores logs and /opt/cloudera/cm/run/cloudera-scm-server/cmf/reports
store historical CM reports which are both strong candidates for taking up significant disk space and
should be investigated first. /var/lib contains the database data and other persistent data which are
important to system operation and shouldn't be cleared without knowing consequences and backups.
Parcel cache is a location for downloaded packages and can be large but shouldn't grow without bound
and should be considered after CM reports and log directories. /tmp is not specific to Cloudera Manager
and should be investigated only as a general troubleshooting step.
Question: 24
You need to migrate your Cloudera Manager server to a new host with a different IP address. Assuming
you are using an external database, what steps are crucial to ensure a smooth transition?
A. Simply install Cloudera Manager on the new host, point it to the existing database, and start the
server. The agents will automatically connect.
B. Install Cloudera Manager on the new host, point it to the existing database, update the ‘server_host’
property in the file on all agent hosts, and restart the agents.
C. Back up the Cloudera Manager database, restore it on the new host, and update the agent
configurations to point to the new server's IP address.
D. Export the Cloudera Manager configuration using the CM API, import it on the new host, and update
the agent configurations to point to the new server's IP address.
E. You must rebuild the entire cluster and cannot migrate the Cloudera Manager server.
Answer: B
Explanation:
The crucial steps are installing Cloudera Manager on the new host, connecting it to the existing database
(preserving your metadata), and updating the agent configurations with the new server's hostname/lP
address . The agents need to know where the server is to communicate. Options A, C, D are incorrect
because agent reconfiguration is mandatory.
Question: 25
Which of the following are critical security considerations when designing a Cloudera CDP on-premises
deployment?
A. Network segmentation to isolate different CDP components.
B. Regularly patching and updating the operating system and CDP software.
C. Implementing strong authentication mechanisms, such as Kerberos, for all users and services.
D. Data encryption at rest and in transit.
E. Ignoring default passwords as they are secure.
Answer: A,B,C,D
Explanation:
Network segmentation reduces the attack surface. Regular patching fixes vulnerabilities. Strong
authentication prevents unauthorized access. Data encryption protects sensitive data. Ignoring default
passwords can lead to serious exploits.
Question: 26
You are configuring Kerberos for your Cloudera CDP on-premises cluster. Which configuration file is
primarily responsible for defining Kerberos client settings?
A. /etc/krb5.conf
B. /etc/hadoop/conf/core-site.xml
C. /etc/security/limits.conf
D. /var/log/krb5kdc.log
E. /etc/default/security
Answer: A
Explanation:
/etc/krb5.conf is the primary configuration file for Kerberos client settings, defining realms, KDCs, and
other Kerberos-related parameters. core-site.xml is for Hadoop core settings. limits.conf defines
resource limits. krb5kdc.log logs Kerberos Key Distribution Center activity, and /etc/default/security
configures other security parameters.
Question: 27
Which of the following are valid methods for encrypting data at rest in HDFS within a Cloudera CDP on-
premises environment?
A. HDFS Encryption Zones
B. Transparent Data Encryption (TDE) at the filesystem level
C. Encrypting individual files using GPG
D. Using a firewall to restrict access.
E. Disabling auditing.
Answer: A,B
Explanation:
HDFS Encryption Zones provide a framework for encrypting data within specific directories in HDFS.
Transparent Data Encryption encrypts the data at the filesystem level, making it unreadable without the
encryption key. Encrypting individual files via GPG is not a scalable or manageable solution within a
cluster. Firewalls restrict network access, not data at rest. Disabling auditing is a security risk.
Question: 28
In a Kerberized Cloudera CDP cluster, what is the purpose of the 'kinit' command?
A. To initialize a new Kerberos principal in the KDC.
B. To obtain a Kerberos ticket-granting ticket (TGT) for a user or service principal.
C. To list all available Kerberos realms.
D. To delete a Kerberos principal from the KDC.
E. To configure firewalls.
Answer: B
Explanation:
The ‘kinit' command is used to obtain a Kerberos ticket-granting ticket (TGT) for a user or service
principal, allowing them to authenticate to other Kerberized services without providing their password
each time. Initializing or deleting principals is done via 'kadmin' or 'kadmin.locar.
Question: 29
You need to implement Role-Based Access Control (RBAC) in your Cloudera CDP on-premises cluster.
Which component(s) within CDP are directly involved in managing and enforcing RBAC policies?
A. Cloudera Manager
B. Ranger
C. Hue
D. ZooKeeper
E. Kafka
Answer: A,B
Explanation:
Cloudera Manager provides the interface for managing users and assigning roles. Ranger is the policy
engine that enforces the access policies defined through Cloudera Manager or Ranger's own I-Jl. Hue
and Kafka are data access and streaming tools, respectively. ZooKeeper is a distributed coordination
service.
Question: 30
A security audit reveals that several users are accessing sensitive data without proper authorization. You
suspect that users have obtained Kerberos tickets for service principals they shouldn't have. How can
you revoke Kerberos tickets for specific users or services?
A. Using the 'kdestroy' command on each client machine.
B. Using the 'kadmin’ interface to expire or disable the Kerberos principals.
C. Restarting the entire Cloudera CDP cluster.
D. Reformatting the HDFS filesystem.
E. Deleting the user accounts in the OS
Answer: B
Explanation:
The 'kadmin’ interface provides tools to manage Kerberos principals, including expiring or disabling
them, which effectively revokes their Kerberos tickets. ‘kdestrov’ only deletes the local ticket cache, not
the underlying principal. Restarting the cluster or reformatting HDFS is not a solution. Deleting user
accounts prevents login, but does not revoke Kerberos tickets issued to those users while they were
active.
Question: 31
Which of the following configurations will enhance the security of communication within a Cloudera CDP
on-premises cluster? Assume Kerberos is already enabled.
A. Enabling TLS/SSL encryption for all services.
B. Configuring firewalls to restrict network access between services.
C. Disabling auditing.
D. Using default passwords for all services.
E. Granting all users administrative privileges.
Answer: A,B
Explanation:
TLS/SSL encryption secures the communication channel, preventing eavesdropping. Firewalls restrict
network access, limiting the potential damage from compromised services. Disabling auditing weakens
security. Using default passwords or granting excessive privileges are security risks.
Question: 32
You are setting up Ranger policies for Hive. You want to grant a specific group, 'data analysts', read
access to all tables in the 'sales_data' database, but prevent them from creating or deleting tables.
Which Ranger policy configuration achieves this?
A. Grant 'data_analysts' 'SELECT privilege on 'sales_data'.
B. Grant 'data_analysts' 'ALL' privilege on 'sales_data' database, then deny 'CREATE' and 'DROP'
privileges.
C. Grant 'data _analysts' 'SELECT' privilege on 'sales_data' database.
D. Grant 'data_analysts' 'READ' privilege on Hive metastore.
E. Grant 'data_analysts' 'ALL' privilege on the Hive service.
Answer: A
Explanation:
Granting 'SELECT' privilege on 'sales_data'. grants read access to all tables within the 'sales_data'
database. Granting 'ALL' then denying specific privileges can lead to unexpected behavior due to policy
evaluation order. 'READ' on the metastore is not sufficient. 'ALL' on the service is overly permissive.
Question: 33
How does HDFS Auditing help in maintaining a secure Cloudera CDP on-premises environment?
A. By recording all access attempts to HDFS data, enabling security administrators to track unauthorized
access and potential breaches.
B. By automatically encrypting all data written to HDFS.
C. By preventing users from accessing HDFS data without Kerberos authentication.
D. By automatically backing up HDFS data to a remote location.
E. By replacing damaged blocks in HDFS
Answer: A
Explanation:
HDFS auditing captures access attempts, providing an audit trail for security analysis. It does not encrypt
data, enforce Kerberos, backup data, or repair damaged blocks.
Question: 34
You need to implement data masking for sensitive fields (e.g., credit card numbers) in Hive tables. Which
Ranger plugin allows you to achieve this?
A. Ranger KMS plugin
B. Ranger Hive plugin
C. Ranger HDFS plugin
D. Ranger Kafka plugin
E. Ranger Solr plugin
Answer: B
Explanation:
The Ranger Hive plugin allows you to define policies for data masking and row-level filtering within Hive.
The Ranger KMS plugin manages encryption keys. The HDFS and Kafka plugins manage access to HDFS
data and Kafka topics, respectively. Ranger Solr plugin manages access to Solr data.
Question: 35
You are troubleshooting a Kerberos authentication issue. A user is receiving the error 'KDC has no
support for encryption type'. What is the most likely cause and how would you resolve it?
A. Cause: The user's Kerberos principal is locked. Resolution: Unlock the principal using 'kadmin’ .
B. Cause: The client and KDC are not configured to use a common encryption type. Resolution: Modify
the '/etc/krb5.conf file on the client and KDC to ensure a shared encryption type is enabled.
C. Cause: The Kerberos ticket has expired. Resolution: Run 'kinit -R to renew the ticket.
D. Cause: The KDC is not running. Resolution: Start the KDC service.
E. Cause: The user is not a member of the correct OS group. Resolution: add the user to the group
Answer: B
Explanation:
The 'KDC has no support for encryption type' error indicates a mismatch in supported encryption types
between the client and the KDC. Modifying Vetc/krb5.conf on both the client and KDC to enable a
shared encryption type (e.g., aes256-cts) is the correct solution. Principal lockouts, expired tickets, or a
stopped KDC would result in different errors.
Question: 36
Which of the following security features are available to protect data in transit in a Cloudera CDP
cluster?
A. SSL/TLS
B. SSH Tunneling
C. Ipsec
D. Kerberos Authentication
E. Data at Rest Encryption
Answer: A,B,C
Explanation:
SSL/TLS, SSH Tunneling and IPSec can all be used for securing data in transit, however, Kerberos is an
Authentication mechanism, not encryption and 'Data at Rest Encryption' ensures protection of physically
stored data and not during transit.
Question: 37
You are planning a CDP on-premises deployment for an organization with strict data governance
requirements. Which of the following are crucial considerations during the planning phase regarding
data lineage and security?
A. Choosing a CMDB (Configuration Management Database) integrated with Atlas to track metadata
changes.
B. Implementing Ranger policies at the HDFS directory level to control access based on user roles and
groups.
C. Ignoring Kerberos configuration until after the cluster is operational to simplify initial setup.
D. Defining a detailed data retention policy and configuring appropriate archival strategies.
E. Limiting the number of Ranger plugins to reduce complexity and improve performance.
Answer: A,B,D
Explanation:
Data governance requires careful planning around lineage (Atlas integration), security (Ranger policies),
and retention policies. Kerberos is crucial from the start, and limiting Ranger plugins can compromise
security requirements.
Question: 38
During the sizing exercise for your CDP on-premises cluster, you need to estimate the required storage
capacity for HDFS. Which of the following factors significantly influence this estimation?
A. The replication factor configured for HDFS blocks.
B. The number of compute nodes in the cluster.
C. The compression codec used for storing data.
D. The size of the NameNode's heap memory.
E. The expected data ingestion rate and retention period.
Answer: A,C,E
Explanation:
HDFS storage sizing is directly impacted by replication factor (more copies, more storage), compression
(reduces storage footprint), and the amount of data ingested and retained. The number of compute
nodes affects processing power, and NameNode heap size relates to metadata management, not raw
storage capacity.
Question: 39
You are deploying a new CDP on-premises cluster with high availability (HA) enabled for HDFS
NameNode. Which of the following components are essential for ensuring seamless failover in case of a
NameNode failure?
A. Quorum Journal Manager (QJM).
B. ZooKeeper.
C. Oozie Workflow Engine.
D. Standalone Backup NameNode.
E. ResourceManager HA.
Answer: A,B
Explanation:
QJM is responsible for sharing edits between active and standby NameNodes, while ZooKeeper manages
the automatic failover process by monitoring the active NameNode's health. Oozie is a workflow
scheduler, Backup NameNode is deprecated, and ResourceManager HA pertains to YARN.
Question: 40
Which of the following network configurations is generally recommended for optimal performance and
security in a CDP on-premises cluster?
A. A single flat network for all cluster components to simplify management.
B. Segmenting the network into separate VLANs for different service tiers (e.g., compute, storage,
management).
C. Disabling inter-node communication to minimize network traffic.
D. Using a dedicated private network for inter-node communication within the cluster.
E. Relying solely on the public internet for all data transfers.
Answer: B,D
Explanation:
Network segmentation (VLANs) enhances security and performance by isolating traffic. A dedicated
private network for inter-node communication minimizes latency and contention. A flat network is
insecure and doesn't scale well. Disabling inter-node communication would prevent the cluster from
functioning.
Question: 41
You are planning a CDP on-premises deployment that includes both batch processing (using Spark) and
real-time streaming analytics (using Kafka and Flink). How should you configure the YARN resource
queues to ensure fair resource allocation and prevent resource starvation?
A. Create a single large YARN queue for all applications to maximize resource utilization.
B. Configure separate YARN queues for Spark, Kafka, and Flink, with appropriate resource limits and
priorities.
C. Use the default YARN queue for all applications without any modifications.
D. Disable YARN queue management and rely on the default scheduler behavior.
E. Use capacity scheduler and configure resources based on guaranteed capacity and maximum capacity
for each Queue.
Answer: B,E
Explanation:
Separate YARN queues with resource limits and priorities are crucial for managing resources in a mixed
workload environment. This prevents one application from consuming all resources and starving others.
The capacity scheduler, with configurations like guaranteed and maximum capacity, provides
predictable resource allocation.
Question: 42
After deploying your CDP on-premises cluster, you observe slow query performance in Impala’. You
suspect that the issue might be related to metadata synchronization with the Hive Metastore. How can
you diagnose and troubleshoot this problem?
A. Restart all Impala daemons to force a full metadata refresh.
B. Use the ‘INVALIDATE METADATA’ command in Impala to refresh metadata for specific tables or
databases.
C. Examine the Impala daemon logs for errors related to Hive Metastore connectivity or metadata
retrieval.
D. Use 'COMPUTE STATS in Hive for all tables to refresh the statistics used by Impala query optimizer.
E. Reduce number of impalad processes
Answer: B,C,D
Explanation:
'INVALIDATE METADATA’ allows targeted metadata refresh. Analyzing Impala daemon logs helps
identify connection or retrieval issues. 'COMPUTE STATS' in Hive refreshes table statistics which Impala
uses for query optimization. Restarting all daemons is a blunt approach. Reducing number of Impalad
processes reduces parallelism.
Question: 43
You're deploying a CDP cluster with Ranger KMS (Key Management Server) and need to encrypt data at
rest in HDFS. Which configuration steps are essential to ensure Ranger KMS can successfully encrypt and
decrypt data?
A. Configure the 'hadoop.security.key.provider.path' property in core-site.xml to point to the Ranger
KMS URI.
B. Create encryption zones in HDFS and associate them with encryption keys managed by Ranger KMS.
C. Install the Ranger KMS client libraries on all nodes in the cluster.
D. Disable Kerberos authentication to simplify the encryption process.
E. Grant appropriate permissions to users and groups in Ranger KMS to access and manage encryption
keys.
Answer: A,B,E
Explanation:
The 'hadoop.security.key.provider.path' property tells HDFS where to find the key provider. Encryption
zones link directories to keys. Ranger KMS permissions control key access. Ranger KMS client libraries
are server-side components. Kerberos is required for secure operation.
Question: 44
Your organization wants to leverage cloud storage (e.g., AWS S3, Azure Blob Storage) for archival and
backup of data from your CDP on-premises cluster. What configuration is necessary to enable this
functionality?
A. Configure core-site.xml with the appropriate cloud storage endpoint and credentials (e.g.,
&fs.s3a.endpoint’, Sfs.s3a.access.key’ , ‘fs.s3a.secret.key’
B. Install the relevant cloud storage connector libraries (e.g., Hadoop AWS, Hadoop Azure) on all nodes
in the cluster.
C. Create external tables in Hive that point directly to the data stored in cloud storage.
D. Ensure that the cloud storage bucket is publicly accessible to all users.
E. Setup replication between the HDFS cluster and the cloud storage using DistCp or similar tools.
Answer: A,B,E
Explanation:
Cloud storage access requires configuring endpoints and credentials, installing connector libraries, and
using tools like DistCp to transfer data. External tables can access data, but archival needs replication.
Publicly accessible buckets are a security risk.
Question: 45
You are planning to upgrade your CDP on-premises cluster from a previous version. Describe a proper
rollback procedure?
A. Take backups for all configurations before starting upgrades. Then stop CDP services in reverse order
after failed upgrade and restore configurations
B. If the upgrade fails, simply restart the Cloudera Manager Server and the cluster will automatically
revert to the previous state.
C. Take snapshots of all virtual machines in the cluster before starting the upgrade. If the upgrade fails,
revert to the snapshots.
D. Backing up the Hive metastore database and restoring it in case of failure, backing up HDFS data,
backing up CM database
E. No rollback is possible and we should always plan for the upgrade to be successful.
Answer: A,C,D
Explanation:
Backups of the Hive metastore database, HDFS data and all configurations are crucial. VM snapshots
provide a fast, full-system rollback capability. Cloudera Manager does not automatically revert and
upgrades can sometimes fail. Plan the rollback procedure during the Planning for Deployment itself.
Question: 46
While planning a deployment, you need to account for node failures in the cluster. How do you handle
decommissioning and re- commissioning of datanodes in CDP?
A. Decommissioning datanodes involves stopping the DataNode service and removing the node from the
Cloudera Manager. All data on the node is automatically replicated to other nodes.
B. Recommissioning datanodes involves adding the node back to Cloudera Manager, starting the
DataNode service, and allowing HDFS to rebalance the data.
C. You must manually copy data off the datanode before decommissioning or it will be lost.
D. Before recommissioning, ensure that the old data directories are wiped or formatted to avoid
potential conflicts.
E. Data replication will automatically take care of decommissioning and decommission the node
Answer: A,B,D
Explanation:
Decommissioning gracefully moves data off the node (automatic replication). Recommissioning involves
adding the node back to the cluster and data rebalancing. Old data directories must be cleaned. Manual
copying isn't required if you decomission via CM.
Question: 47
You are setting up Kerberos authentication for your CDP cluster. A client application outside the cluster
needs to access HDFS. What steps are necessary to allow the application to authenticate with Kerberos?
A. The application must obtain a Kerberos ticket-granting ticket (TGT) using ‘kinit' with a valid principal
and keytab.
B. The application's Kerberos configuration file (krb5.conf) must be configured to point to the Kerberos
realm and KDC (Key Distribution Center) of the CDP cluster.
C. The application must be added to the 'hadoop.proxyuser’ configuration in core-site.xml.
D. The application must use simple authentication.
E. The application user must be granted the permission in ranger.
Answer: A,B,E
Explanation:
The application needs a Kerberos TGT. The Kerberos configuration needs to be correct. It needs to use
Ranger to grant the required permissions. simple authentication wont work.
Question: 48
Your company’s security policy requires all data at rest to be encrypted and all data in transit to be
encrypted. As a CDP administrator, what should you do to ensure the cluster follows these policies?
A. Enable encryption at rest using HDFS encryption zones and configure Ranger KMS to manage
encryption keys.
B. Enable TLS (Transport Layer Security) for all CDP services to encrypt data in transit.
C. Disable Kerberos authentication, since encryption already provides sufficient security.
D. Rely solely on network firewalls to protect data in transit.
E. Configure the right role based permissions and create required Tag based policies
Answer: A,B,E
Explanation:
HDFS encryption zones with Ranger KMS provide data-at-rest encryption. TLS encrypts data in transit.
Kerberos is essential for authentication. Firewalls are necessary but not sufficient for data-in-transit
protection and the application user has right permissions.
Question: 49
You are configuring a CDP Private Cloud Base cluster in an isolated network (no direct internet access).
Which of the following are essential considerations for successful deployment and operation?
A. Implementing a local artifact repository (e.g., Artifactory, Nexus) containing all necessary RPMs and
dependencies.
B. Configuring a local DNS server that can resolve all internal hostnames and forward external resolution
requests to a designated internet-connected DNS server (if allowed).
C. Ensuring all nodes have direct SSH access to the Cloudera Manager server.
D. Setting up a Network Time Protocol (NTP) server within the isolated network and synchronizing all
nodes to it.
E. Using Cloudera's public YUM repository directly through a proxy server without any local caching.
Answer: A,B,D
Explanation:
A, B, and D are crucial. A local artifact repository provides necessary software packages. A local DNS
server resolves internal hostnames, and NTP ensures time synchronization across the cluster. C is not
directly related to isolated network requirements, although it is good practice. E is incorrect because
relying solely on a proxy without local caching creates a single point of failure and impacts performance
within an isolated network, and also violates the network isolation policy.
Question: 50
Your organization mandates that all data at rest in your CDP cluster within an isolated network must be
encrypted using keys managed within the same isolated network. Which of the following components
and configurations are necessary to satisfy this requirement?
A. Using Cloudera Navigator Key Trustee Server (KTS) deployed within the isolated network.
B. Configuring HDFS encryption with a Key Management Server (KMS) located outside the isolated
network, but accessible via HTTPS.
C. Enabling encryption at the filesystem level on each node using LUKS.
D. Using an HSM (Hardware Security Module) located within the isolated network and integrated with
the KMS.
E. Relying solely on cloud provider-managed encryption keys because they offer superior security.
Answer: A,D
Explanation:
To comply with the isolated network requirement, keys must be managed within the network. KTS and
HSM integration fulfill this. KTS manages keys within the isolated network. HSM provides hardware-
backed key storage and cryptographic operations, also within the isolated network. KMS located outside
the network violates isolation. Filesystem encryption is a viable option, but not centrally managed
through KMS and doesn't address all components within CDP. Option E violates network isolation.
Question: 51
You are troubleshooting an issue where nodes in your CDP cluster within an isolated network are unable
to register with the Cloudera Manager server. You suspect a network issue. Which of the following tools
and techniques are most effective for diagnosing the problem?
A. Using ‘ping' and ‘traceroute' from the nodes to the Cloudera Manager server hostname and IP
address.
B. Using ‘netcat’ (nc) or 'telnet to verify connectivity to the Cloudera Manager server on port 7182.
C. Examining the Cloudera Manager server's logs for registration errors.
D. Relying solely on the Cloudera Manager UIfor connection status and error messages.
E. Temporarily disabling the firewall on all nodes to rule out firewall issues.
Answer: A,B, C
Explanation:
A, B, and C are effective. ‘ping' and ‘traceroute' identify basic network connectivity issues.
‘netcatTtelnet’ verify connectivity to the Cloudera Manager port. Examining server logs provides error
context. The Cloudera Manager UIcan be helpful, but doesn't replace direct troubleshooting. Disabling
the firewall is a security risk and should be avoided if possible. The preferred strategy is to examine the
firewall rules.
Question: 52
You are configuring network settings for a new CDP Private Cloud Base cluster in an isolated network.
The cluster requires communication with an external Active Directory server for authentication. Which
of the following approaches is most secure and recommended for enabling this communication?
A. Opening all ports between the cluster nodes and the Active Directory server.
B. Configuring a firewall to allow only the necessary ports (e.g., LDAP, LDAPS) between specific cluster
nodes (e.g., those running the Identity Management service) and the Active Directory server.
C. Placing the Active Directory server within the isolated network.
D. Configuring a VPN tunnel between the isolated network and the network containing the Active
Directory server, allowing unrestricted traffic flow.
E. Forwarding all DNS requests from the isolated network to the Active Directory server's DNS server.
Answer: B
Explanation:
Option B provides the most secure approach. Restricting access to only necessary ports and specific
nodes minimizes the attack surface. Option A is insecure. Option C might not be feasible or desirable.
Option D defeats the purpose of network isolation. Option E could expose internal network information.
Question: 53
You need to upgrade your CDP cluster in an isolated environment. You have downloaded the necessary
parcels to a local artifact repository. How do you configure Cloudera Manager to use this local repository
for parcel distribution?
A. Modify the property in the Cloudera Manager's advanced configuration to point to the local artifact
repository URL.
B. Set the property in the Cloudera Manager's configuration to the local artifact repository URL.
C. Add the local artifact repository URL as a remote repository in the Cloudera Manager Admin Console
under Parcels > Edit Settings.
D. Use the command-line tool to update the Cloudera Manager's parcel repository configuration.
E. Mount the local artifact repository as a network drive on each node in the cluster.
Answer: C
Explanation:
The most straightforward way is to add the local artifact repository URL as a remote repository through
the Cloudera Manager Admin Console. This allows Cloudera Manager to discover and distribute parcels
from the local repository. Options A, B, and D refer to internal properties that are not directly modifiable
or are deprecated. Option E is not practical or supported.
Question: 54
Your Security team wants to ensure only specific users are allowed to access data via Hive/lmpala in an
isolated environment. What are the key components and configurations required to enable fine-grained
authorization in this scenario?
A. Enable Apache Ranger and integrate it with Hive and Impala for authorization policies. Ensure Ranger
is deployed within the isolated network.
B. Use only Hive's built-in SQL standard-based authorization.
C. Configure Sentry with the appropriate privileges for each user.
D. Rely on the underlying file system permissions for data access control.
E. Configure Kerberos authentication without implementing any authorization framework.
Answer: A
Explanation:
Apache Ranger provides centralized, fine-grained authorization policies for Hive and Impala and
integrates well with CDP. It must be deployed inside the isolated network to maintain security and
isolation. Hive's SQL standard authorization is limited. Sentry is deprecated in CDP. File system
permissions are insufficient for comprehensive data access control. Kerberos handles authentication but
not authorization.
Question: 55
Assume you have a CDP cluster in an isolated network without direct internet access. You have
configured a local YUM repository. After adding a new node to the cluster, the Cloudera Manager Agent
fails to install because it cannot find necessary packages. What is the MOST likely cause and how can
you resolve this?
A. The node's 'letc/yum.repos.d/' configuration is pointing to the public Cloudera YUM repository
instead of your local mirror. Modify the repository configuration on the new node to point to the local
YUM repository URL.
B. The Cloudera Manager Agent requires direct internet access to download its dependencies. Provide
temporary internet access to the node during installation.
C. The local YUM repository is not properly synchronized with the Cloudera YUM repository. Run ‘yum
repodata’ on the YUM server to regenerate the metadata.
D. The firewall on the new node is blocking access to the local YUM repository. Configure the firewall to
allow access to the YUM repository port (e.g., 80 or 443).
E. The newly added node has not yet been registered within Cloudera Manager. Complete host
inspection from Cloudera Manager I-II.
Answer: A
Explanation:
The most common cause is an incorrect YUM repository configuration on the new node. Modifying
'/etc/yum.repos.dP to point to the local mirror solves this. Requiring internet access defeats the
isolation requirement. 'yum repodata’ regenerates metadata but doesn't address the configuration
issue. Firewall issues and registration problems are less likely if the core issue is incorrect repository
configuration. Re-running host inspection from CM also won't solve the YUM configuration error.
Question: 56
You have a Spark application that needs to access data stored in Hive tables within your isolated CDP
cluster. The application is running outside the cluster on a separate analytics server, also within the
isolated network, but not managed by Cloudera Manager. How do you configure secure and authorized
access for this Spark application to the Hive data?
A. Configure a JDBC connection from the Spark application to the HiveServer2 service, enabling Kerberos
authentication and authorization.
B. Grant the Spark application's user account full access to all Hive tables.
C. Disable Kerberos authentication on the HiveServer2 service.
D. Copy the Hive metastore database to the analytics server.
E. Create a separate Hive metastore specifically for the Spark application.
Answer: A
Explanation:
A JDBC connection with Kerberos authentication provides secure and authorized access. It leverages the
existing security framework of the cluster. Granting full access is a security risk. Disabling Kerberos is not
secure. Copying the metastore is complex and doesn't address authorization. A separate metastore is
unnecessary and complicates data management.
Question: 57
You are implementing a multi-tenant CDP cluster in an isolated network. Each tenant requires a
dedicated HDFS namespace and compute resources. Which of the following architectural approaches
would you recommend for achieving resource isolation and data security in this scenario?
A. Use HDFS quotas to limit storage for each tenant and YARN queues to manage compute resources, all
within a single cluster.
B. Create separate CDP clusters for each tenant.
C. Use HDFS ACLs for data isolation and rely on operating system-level resource limits for compute
isolation.
D. Utilize Kubernetes for resource isolation while still using a single Cloudera Data Platform cluster.
E. Configure separate namespaces within the Hive metastore for each tenant.
Answer: A
Explanation:
HDFS quotas and YARN queues provide a balance between resource isolation and management
overhead within a single cluster, without violating the isolated network requirement. Separate clusters
increase management complexity. HDFS ACLs are more granular, but do not provide adequate resource
isolation without quotas. While Kubernetes can be used with CDP, it is a complex solution for basic
tenant isolation and might increase management overhead inside a completely isolated network. Hive
namespaces do not provide compute resource isolation.
Question: 58
You are tasked with setting up monitoring for your CDP cluster in an isolated network. Since you do not
have external access, how would you configure monitoring and alerting without relying on cloud-based
solutions? Select all that apply.
A. Implement a local instance of Prometheus and Grafana within the isolated network to collect metrics
and visualize dashboards.
B. Configure Cloudera Manager to send email alerts via an internal SMTP server.
C. Forward all system logs to an external, cloud-based logging service using a secure VPN connection.
D. Utilize Cloudera Manager's built-in monitoring capabilities and dashboards exclusively.
E. Setup Nagios to monitor the hosts, including CPU Utilization, Disk Space and Network bandwidth.
Answer: A,B,D,E
Explanation:
A, B, and D are valid solutions. Prometheus and Grafana offer comprehensive monitoring and
visualization within the isolated network. Configuring Cloudera Manager to send email alerts via an
internal SMTP server provides notifications. Cloudera Manager's built-in monitoring offers basic
monitoring capabilities. Forwarding logs to the cloud violates the isolation requirement. Nagios would
be an option, which can give CPU Utilization, Disk Space and Network bandwidth metrics to the
monitoring team.
Question: 59
You are managing a CDP cluster in an isolated network. Your data science team needs to use Python
libraries not included in the base Anaconda distribution provided with CDP. What is the recommended
approach to install and manage these custom Python libraries within the isolated environment? Assume
no direct internet access.
A. Download the required Python packages (wheels or source distributions) from a machine with
internet access, transfer them to a local repository within the isolated network, and use 'pip' with the ‘--
find-linkS option to install them.
B. Use ‘conda’ to create a new environment and install packages directly from the Anaconda Cloud using
a proxy server.
C. Manually copy the Python library files to the appropriate directories within the base Anaconda
environment on each node.
D. Build a custom Anaconda distribution including the required libraries and distribute it to all nodes.
E. Install Miniconda on each node and download dependencies using ‘conda install' command.
Answer: A
Explanation:
Option A is the most practical and recommended approach. It leverages 'pips and a local repository to
manage dependencies without internet access. Using a proxy server for Anaconda Cloud access might
be possible but introduces complexity and potential security concerns. Manually copying files is error-
prone and difficult to maintain. Building a custom Anaconda distribution is complex and requires
significant effort. Miniconda requires internet access and might not be an ideal solution for a fully air-
gapped network.
Question: 60
An organization with a Cloudera Data Platform (CDP) on-premises deployment in an isolated network
requires a secure method for transmitting data to an external cloud environment for specific processing
tasks. Which of the following methods represents the MOST secure approach to accomplish this while
minimizing the risk of exposing the entire on-premises infrastructure?
A. Create a dedicated, one-way data diode that only allows data to flow from the on-premises CDP
cluster to a secure data ingestion point in the cloud environment.
B. Establish a permanent VPN tunnel between the on-premises network and the cloud environment,
allowing unrestricted access to all resources.
C. Utilize a secure file transfer protocol (SFTP) to periodically copy data to a publicly accessible cloud
storage bucket.
D. Implement a custom application that directly writes data to a cloud-based database using exposed
API endpoints.
E. Use Cloudera Replication Manager to replicate data to the cloud after ensuring correct network
configurations are in place.
Answer: A
Explanation:
A data diode ensures a unidirectional data flow, preventing any potential backdoors or lateral
movement from the cloud environment to the on-premises CDP cluster. This offers the highest level of
security. Options B, C, and D introduce significant security risks by allowing bidirectional communication
or exposing the data to potential vulnerabilities. Cloudera Replication Manager could be a valid
approach but relies on the network connectivity which may not be highly secure in isolated
environments.
Question: 61
You are configuring High Availability (HA) for the NameNode in your on-premises CDP cluster. You've set
up two NameNode hosts and configured shared storage. After initial setup, you notice frequent failovers
occurring. Which of the following could be the MOST likely cause of these frequent failovers, assuming
the network is stable and hosts are healthy?
A. Incorrect configuration of the 'dfs.ha.automatic-failover.enabled' property in 'hdfs-site.xmr.
B. Insufficient memory allocated to the ZKFailoverController (ZKFC) processes on both NameNode hosts.
C. Mismatching 'dfs.nameserviceS value between the client configuration and the NameNodes'
configuration.
D. Incorrect quorum configuration in ZooKeeper, leading to split-brain scenarios.
E. Using NFS as shared storage for the EditLog, leading to synchronization issues.
Answer: B
Explanation:
Frequent failovers, even with shared storage and stable network, are often caused by resource
contention on the ZKFC processes. Insufficient memory can cause ZKFC to incorrectly report the active
NameNode as unhealthy, triggering unnecessary failovers. While NFS can be problematic, it usually
manifests as data corruption rather than frequent failovers. Other options are less likely to cause
frequent failovers after the initial setup if the initial config was correct.
Question: 62
In a CDP on-premises deployment with Kerberos enabled and High Availability configured for the
NameNode, which configuration file requires modification BOTH on the client machines and on the
NameNode hosts to ensure proper communication after a failover?
A. 'core-site.xml'
B. ‘ krb5.conf
C. 'hdfs-site.xmr
D. ‘yarn-site.xml'
E. mapred-site.xmr
Answer: A
Explanation:
The ‘core-site.xmr file contains the 'fs.defaultFS' property, which defines the logical name service for
HDFS. This value is used by clients and other Hadoop components to locate the active NameNode. After
a failover, this property needs to point to the HA nameservice (e.g., 'hdfs://mycluster') rather than a
specific NameNode host. This update is necessary on both client machines and NameNode hosts to
ensure proper communication after a failover.
Question: 63
You are tasked with setting up HA for the Hive Metastore in your CDP on-premises cluster. Which of the
following is the MOST critical configuration step to ensure seamless failover and data consistency?
A. Configuring multiple HiveServer2 instances behind a load balancer.
B. Using a highly available, external database (e.g., PostgreSQL with replication) for the Hive Metastore.
C. Enabling HiveServer2 authentication using Kerberos.
D. Configuring the ‘hive.metastore.uris’ property to point to all Metastore instances.
E. Setting up replication for the Hive warehouse directory in HDFS.
Answer: B
Explanation:
The MOST critical step for Hive Metastore HA is using a highly available external database. The
Metastore contains the metadata that describes the structure of your Hive tables. If this metadata is lost
or corrupted during a failover, you could lose access to your data. Configuring multiple HiveServer2
instances behind a load balancer (option A) only ensures availability of the HiveServer2 service, not the
Metastore itself. A replicated warehouse directory helps with data availability, but doesn't solve
metadata unavailability.
Question: 64
You are troubleshooting an issue where applications are failing to connect to the ResourceManager in
your HA-enabled CDP cluster. The ResourceManager failover appears to be working correctly. After
checking the logs, you find errors related to 'Connection refused'. Which of the following is the MOST
likely cause?
A. The ‘yarn.resourcemanager.ha.id’ property is not consistently configured across all ResourceManager
instances.
B. The firewall on the ResourceManager host is blocking incoming connections on the ResourceManager
ports.
C. The ‘yarn.resourcemanager.hostname’ property is set to the hostname of the standby
ResourceManager instance in 'yarn-site.xml'.
D. The ‘yarn.resourcemanager.address’ property is incorrectly configured to point to a specific
ResourceManager instance instead of the HA nameservice address.
E. ZooKeeper is not running or is unreachable.
Answer: D
Explanation:
In an HA setup, clients should connect to the ResourceManager using a logical name service or a load
balancer, not a specific ResourceManager instance. 'yarn.resourcemanager.address (and related
properties like 'yarn.resourcemanager.webapp.address') should point to this abstract address. If it's
pointing to a specific instance, clients will fail to connect when that instance is not the active one. The
other options could cause issues, but 'Connection refused' strongly suggests an addressing problem.
Question: 65
You need to implement HA for the Oozie server in your CDP on-premises cluster. Which of the following
statements regarding Oozie HA configuration are TRUE ? (Select TWO)
A. Oozie HA relies on a shared file system (e.g., HDFS) to store the Oozie database.
B. Oozie HA requires a dedicated ZooKeeper ensemble separate from the one used by HDFS.
C. All Oozie server instances must connect to the same external database for storing workflow
definitions and runtime information.
D. Oozie HA is automatically enabled when you configure HA for HDFS and YARN.
E. The ‘oozie.ha.system.id' property must be unique across all Oozie server instances in the HA setup.
Answer: A,C
Explanation:
Oozie HA requires a shared file system (typically HDFS) to store the Oozie database and workflow
definitions. All Oozie server instances must connect to the same external database (e.g., MySQL,
PostgreSQL) for storing workflow definitions and runtime information, ensuring data consistency during
failover. Option B is incorrect; Oozie typically uses the same ZooKeeper ensemble as HDFS and YARN.
Option D is incorrect; Oozie HA needs to be explicitly configured. Option E is incorrect;
‘oozie.ha.system.id' should be identical across all instances. It defines the HA group.
Question: 66
Which of the following mechanisms does the ZKFailoverController (ZKFC) use to determine the health
status of the NameNode in a High Availability (HA) configuration?
A. Regularly checking the NameNode's JMX metrics for memory usage and CPU utilization.
B. Monitoring the NameNode's heartbeat signals sent to the JournalNodes.
C. Performing periodic ‘hdfs dfsadmin -report' commands and analyzing the output.
D. Using a configured health check script Cdfs.ha.fencing.methodS) to verify NameNode responsiveness.
E. Listening for notifications from the NameNode about its current state (active or standby) via RPC.
Answer: D
Explanation:
The ZKFC uses the configured fencing methods ('dfs.ha.fencing.methods') to determine the health of the
NameNode. This usually involves a script that checks the NameNode's responsiveness and attempts to
prevent split-brain scenarios by fencing off the old active NameNode. While the NameNode does
interact with JournalNodes (option B), the ZKFC doesn't directly monitor those interactions for health
status. The other options are not the primary mechanisms used by ZKFC for health checks.
Question: 67
You are planning to implement HA for your NameNode using QJM (Quorum Journal Manager). What is
the minimum number of JournalNodes required for a QJM-based HA configuration to tolerate one
JournalNode failure?
A. 2
B. 3
C. 4
D. 5
E. 1
Answer: B
Explanation:
For a QJM-based HA configuration to tolerate one JournalNode failure, you need a minimum of three
JournalNodes. This ensures that a majority (quorum) of JournalNodes are always available to record
EditLog transactions, even if one JournalNode is down. A quorum is (N/2) + 1, where N is the number of
JournalNodes.
Question: 68
After configuring HA for the ResourceManager in your CDP cluster, you notice that applications are
being submitted successfully, but they are not starting. The ResourceManager logs show errors related
to 'Invalid transition at ACTIVE to STANDBY'. What is the most likely cause of this issue?
A. The ‘yarn.resourcemanager.ha.enabled' property is set to in 'yarn-site.xmr.
B. ZooKeeper is not properly configured or is unreachable by the ResourceManager instances.
C. The ‘yarn.resourcemanager.recovery.enabled' property is set to 'false' in ‘yarn-site.xmr.
D. The ResourceManager state store is corrupted.
E. Incorrect fencing configuration is preventing the standby ResourceManager from transitioning to
active.
Answer: C
Explanation:
The error 'Invalid transition at ACTIVE to STANDBY' strongly suggests that the ResourceManager is not
configured to recover its state. The 'yarn.resourcemanager.recovery.enabled’ property controls whether
the ResourceManager attempts to recover its state from a state store (typically ZooKeeper). If this
property is set to 'false' , the ResourceManager will not be able to transition to the STANDBY state
correctly, preventing failover from working as expected. While ZooKeeper connectivity (option B) is
crucial, the error message points directly to the recovery mechanism being disabled.
Question: 69
You are implementing HA for your HDFS cluster. You chose Quorum Journal Manager (QJM) as your
shared edit log storage. After you formatted the NameNodes and started the JournalNodes, the standby
NameNode fails to synchronize with the active NameNode. The standby NameNode logs show the
following error: 'Failed to start standby services'. Further digging reveals that the root cause is:
'org.apache.hadoop.hdfs.server.namenode.SafeModeException: Cannot create directory /foo/bar.
Name node is in safe mode.' Which of the following steps would resolve this issue and allow the standby
NameNode to synchronize? (Select TWO)
A. Manually create the '/foo/bar’ directory on HDFS via the active NameNode.
B. Manually create the '/foo/bar’ directory on the standby NameNode local filesystem.
C. Take the active NameNode out of safemode using the command ‘hdfs dfsadmin -safemode leave'.
D. Restart the JournalNodes.
E. Increase the heap size of the NameNode.
Answer: A,C
Explanation:
The error message indicates that the NameNode is in safemode and cannot create the directory
'/foo/bar’ . The synchronization process requires creating directories and files in HDFS. Therefore, two
actions are required: 1 . Create the directory S/foo/bar’ on the active NameNode, which will then be
replicated to the standby. This is a one-time task if the directory is supposed to be on HDFS. 2. Take the
NameNode out of safemode, as it prevents any modifications to the filesystem. Option A is necessary
because it populates the directory expected. Option C is critical since being in safemode prohibits
making any modifications to the filesystem. Restarting JournalNodes or increasing NameNode heap size
are unrelated to this specific safemode issue. Directory S/foo/bar’ has to be in HDFS, not local standby
namenode.
Question: 70
You are responsible for ensuring high availability of the HBase Master in your CDP on-premises cluster.
Which of the following configuration parameters directly controls the number of backup HBase Master
instances that will be started?
A. hbase.master.distributed’
B. ‘hbase.backup.masters’
C. hbase.zookeeper.quorum’
D. ‘hbase.master.wait.for.regionserver.expiration'
E. hbase.rootdir’
Answer: A
Explanation:
The 'hbase.master.distributed' parameter is what instructs HBase to start more than one master. Setting
it to ‘true’ enables the distributed mode, which allows for multiple backup masters to be started. Setting
it to 'false' will configure single master for HBASE clusteL All backup instances participate in master
election via Zookeeper. 'hbase.backup.masterS specifies the hostnames for backup masters.
'hbase.zookeeper.quorum’ defines the ZooKeeper ensemble, which is required for HA, but does not
control the number of backup Masters. hbase.master.wait.for.regionserver.expiration’ related to region
servers and 'hbase.rootdirs specifies the HDFS path for HBase data.
Question: 71
Consider the following scenario: You have an HA-enabled HDFS cluster with NameNode HA configured
using QJM. During a maintenance window, you shut down all the JournalNodes. Subsequently, the
active NameNode crashes. What will be the immediate state of the HDFS cluster and its ability to serve
read/write requests?
A. The standby NameNode will automatically become active and the cluster will continue serving
requests with minimal interruption.
B. The standby NameNode will attempt to become active but will fail, and the cluster will be unavailable
for both read and write requests.
C. The standby NameNode will become active, but only read requests will be served. Write requests will
fail until the JournalNodes are restarted.
D. The cluster will enter read-only mode until the active NameNode is manually recovered.
E. DataNodes will continue to serve data from their local caches, but any new data written will be lost.
Answer: B
Explanation:
With QJM, the NameNodes rely on a quorum of JournalNodes to maintain a consistent view of the
filesystem. If all JournalNodes are down, the standby NameNode cannot become active because it
cannot verify the latest state of the filesystem. The standby cannot read edits from journal nodes. The
crash of active namenode will render HA setup unfunctional without journal node availability. This
makes HDFS completely unavailable until the JournalNodes are restored. Therefore, option B is correct.
Question: 72
You are setting up ResourceManager HA in your CDP cluster using automatic failover. You have noticed
that even though both
ResourceManagers are running, only one is ever active and the other always remains in standby mode.
The ZKFailoverController logs show frequent messages like 'Failed to acquire lock; another ZKFC is likely
already running'. What could be the cause?
A. The ‘yarn.resourcemanager.ha.rm-ids’ property contains duplicate ResourceManager IDs.
B. The clock synchronization between the ResourceManager hosts and the ZooKeeper ensemble is
significantly skewed.
C. Firewall rules are blocking communication between the ZKFailoverController and the ZooKeeper
ensemble.
D. The ZKFailoverController process is running multiple times on the same host for one or both
ResourceManager instances.
E. The ‘yarn.resourcemanager.hostname’ property is not set correctly.
Answer: D
Explanation:
The error message 'Failed to acquire lock; another ZKFC is likely already running' indicates that multiple
ZKFC processes are competing to become the active leader for the ResourceManager HA group. The
most likely cause is that the ZKFC process has been started multiple times on the same host for one or
both ResourceManager instances. This often happens due to misconfigured init scripts or systemd units.
ZooKeeper relies heavily on accurate time for leader election and session management.
Question: 73
You are configuring NameNode High Availability in CDP on-premise. Which of the following steps is
MANDATORY for ensuring proper failover functionality?
A. Deploying a Quorum Journal Manager (QJM) and configuring the 'dfs.namenode.shared.edits.dir’
property.
B. Setting up a manual failover procedure involving SSH access to the standby NameNode.
C. Configuring the Resource Manager to point to both NameNodes.
D. Creating a symbolic link from the active NameNode's data directory to the standby NameNode's data
directory.
E. Enabling NFS gateway on both namenodes.
Answer: A
Explanation:
Using a Quorum Journal Manager (QJM) and setting the ‘dfs.namenode.shared.edits.dir’ property
correctly allows the standby NameNode to stay synchronized with the active NameNode by replaying
edits from the shared journal. This ensures seamless failover. Manual failover is not ideal and QJM is the
automated approach for HA.
Question: 74
When planning backups for HDFS data in a CDP on-premise cluster, which of the following factors should
be considered to minimize the impact on cluster performance and ensure data consistency?
A. Run backups during peak hours to ensure all recent data is included.
B. Use distcp with a large number of mappers to maximize throughput, regardless of current cluster
load.
C. Schedule backups during off-peak hours and use incremental backups where possible.
D. Disable all other jobs running on the cluster during the backup process.
E. Use HDFS snapshots and distcp for efficient data replication.
Answer: C,E
Explanation:
Backups should be scheduled during off-peak hours to minimize the impact on production workloads.
Incremental backups can reduce the amount of data transferred, and using HDFS snapshots allows for
consistent data replication through distcp.
Question: 75
You're tasked with configuring automatic failover for the ResourceManager (RM) in your CDP on-
premise cluster. Which of the following components are essential for enabling this functionality?
A. Zookeeper
B. NameNode
C. ResourceManager state store
D. NodeManager
E. Quorum Journal Manager (QJM)
Answer: A,C
Explanation:
Zookeeper is used for leader election and maintaining the active ResourceManager state. The
ResourceManager state store (typically embedded or a separate database) persists application state,
enabling failover to a standby ResourceManager with minimal application interruption. QJM is for
NameNode and not ResourceManager failover.
Question: 76
Which configuration property controls the maximum number of concurrent distcp tasks when running
backups in CDP on-premise?
A. 'distcp.map.count’
B. ‘mapreduce.job.reduces’
C. mapreduce.job.mapS
D. 'distcp.dynamic.mappers’
E. yarn.app.mapreduce.am.resource.mb'
Answer: A
Explanation:
The 'distcp.map.count’ property in the Hadoop configuration controls the number of map tasks that
distcp will use to copy data. Increasing this value can improve the speed of the backup, but it also
increases the load on the cluster.
Question: 77
You are observing frequent failovers between two NameNodes in your HA-enabled CDP on-premise
cluster. The logs show errors related to fencing. What is the MOST likely cause?
A. Incorrect configuration of the Zookeeper quorum.
B. The standby NameNode is running on a machine with insufficient resources.
C. The fencing methods configured for the NameNodes are failing to reliably prevent split-brain
scenarios.
D. The 'dfs.ha.automatic-failover.enabled' property is set to false.
E. There is a network connectivity issue between the two NameNodes.
Answer: C
Explanation:
Fencing is critical for preventing split-brain scenarios in HA clusters. If fencing methods are failing, it
means that the active NameNode is not being reliably shut down or isolated before the standby
NameNode becomes active, leading to data corruption and frequent failovers.
Question: 78
Consider a scenario where you need to back up only the metadata of your Hive metastore in CDP on-
premise. Which command would be MOST appropriate?
A. "hdfs dfs -cp /user/hive/warehouse Ibackup/hive’
B. mysqldump -u hive -p hive > hive_metastore_backup.sqr (assuming MySQL metastore)
C. 'distcp hdfs://nameservicel luser/hive/warehouse hdfs://backupcluster/backup/hive’
D. Shive —service metastore_admin -backup'
E. ‘pg_dump -U hive -d hive > hive_metastore_backup.sqr (assuming PostgreSQL metastore)
Answer: B,E
Explanation:
Backing up the Hive metastore involves backing up the database that stores the metadata. ‘mysqldump’
(for MySQL) or ‘pg_dump’ (for PostgreSQL) are the appropriate tools for this. The 'hdfs dfS and 'distcp’
commands copy data in HDFS, not the metastore database itself.
Question: 79
You are using Cloudera Manager to manage your CDP on-premise cluster. You want to configure alerts
to notify you when the average write latency to the NameNode journal nodes exceeds a certain
threshold. Where would you configure this alert?
A. In the HDFS service's configuration properties within Cloudera Manager.
B. In the Cloudera Manager Admin Console, under 'Alerts' -> 'Create Alert'.
C. Directly in the journal node's ‘hdfs-site.xmr configuration file.
D. Using the Cloudera Manager API with a custom script.
E. Configuring an external monitoring tool like Prometheus.
Answer: B
Explanation:
Cloudera Manager provides a central interface for managing and monitoring your cluster. Alerts are
configured within the Cloudera Manager Admin Console under the 'Alerts' section.
Question: 80
In the context of HDFS snapshots, what is the primary advantage of using them for backups compared to
directly copying the data?
A. Snapshots are always faster because they create a full copy of the data instantaneously.
B. Snapshots consume less storage space because they only store the differences between the snapshot
and the current state of the files.
C. Snapshots are inherently more secure than direct data copies.
D. Snapshots allow you to restore individual files, while direct copies only allow for restoring entire
directories.
E. Snapshots eliminate the need for a secondary storage location.
Answer: B
Explanation:
HDFS snapshots are space-efficient because they employ a copy-on-write mechanism. Only
modifications made after the snapshot are stored, leading to significantly less storage consumption
compared to a full data copy. They are not a replacement for backup storage. Snapshots offer a point-in-
time view that you then copy somewhere else for backup.
Question: 81
You have a critical Hive table that needs to be backed up regularly. The data volume is very large. You
want to create a backup solution that minimizes downtime during the backup process and provides a
point-in-time consistent view of the table. Which approach would be MOST suitable?
A. Using 'hdfs dfs -cp' to copy the table's data directory.
B. Using 'EXPORT TABLE and ‘IMPORT TABLE commands in Hive.
C. Creating an HDFS snapshot of the table's data directory and then using 'distcp’ to copy the snapshot
to a backup location.
D. Using Hive replication.
E. Using ‘BACKUP TABLE and 'RESTORE TABLE commands.
Answer: C
Explanation:
Creating an HDFS snapshot provides a point-in-time consistent view of the table's data. Using 'distcp’ to
copy the snapshot allows you to back up the data without significantly impacting ongoing write
operations to the table, as the snapshot represents a fixed state. EXPORT/IMPORT has limitations. Hive
replication can replicate to another cluster which is a good backup strategy, but snapshots are still
useful. Hive backup and restore are newer features but snapshots and distcp may still be better for very
large tables.
Question: 82
Your organization mandates that all data at rest in your CDP on-premise cluster must be encrypted.
Which of the following components require specific configuration to enable encryption at rest?
A. HDFS
B. YARN
C. Hive Metastore
D. ZooKeeper
E. Oozie
Answer: A,C
Explanation:
HDFS requires the creation of encryption zones and key management configuration. The Hive
Metastore, particularly if it contains sensitive metadata, needs to be encrypted through database-
specific mechanisms. YARN, Zookeeper, and Oozie don't directly store persistent data that would be
subject to encryption at rest in the same way as HDFS data or the Hive Metastore. While Zookeeper and
Oozie have configurations, encryption at rest usually relates to HDFS and metastores.
Question: 83
You have configured HA for your HDFS NameNodes using Quorum Journal Manager (QJM). After a
failover, you notice that the new active NameNode is not able to start properly. Upon investigation, you
find that the shared edits directory used by QJM contains corrupted data’. What is the MOST likely
reason for this corruption?
A. The QJM servers are running on machines with different operating systems.
B. The number of QJM servers is not sufficient to tolerate the failure of one or more servers.
C. The 'dfs.namenode.shared.edits.dir’ property is not configured correctly on all NameNodes.
D. The clock skew between the NameNodes and the QJM servers is too high.
E. The QJM servers' disk space is full.
Answer: B
Explanation:
QJM uses a quorum-based approach to ensure the consistency of the edits log. If the number of QJM
servers is not sufficient (e.g., less than 3), the failure of even one server can lead to a loss of quorum and
potential data corruption in the shared edits directory. If the server numbers are correct corruption is
unlikely. But corruption may appear if you don't have enough QJM nodes to allow a quorum.
Question: 84
You are responsible for backing up a large HBase table in your CDP on-premise cluster. Which method
offers the best combination of efficiency and minimal impact on the running HBase service?
A. Using 'hbase org.apache.hadoop.hbase.mapreduce.CopyTable' to copy the table to another location.
B. Taking an HBase snapshot and then using ‘ExportSnapshot’ to export the snapshot data to HDFS.
C. Disabling the HBase table and then copying the underlying HFiles directly from HDFS.
D. Performing a full cluster backup using the Cloudera Manager.
E. Using the ‘truncate command.
Answer: B
Explanation:
Taking an HBase snapshot is a fast and non-blocking operation that creates a point-in-time consistent
view of the table. Using ExportSnapshot' allows you to efficiently export the snapshot data to HDFS for
backup purposes with minimal impact on the running HBase service. Disabling the table leads to
downtime. CopyTable can put load on the cluster. Truncate would delete the data.
Question: 85
You are designing a multi-tenant Cloudera CDP on-premises cluster where each tenant requires strict
isolation for compute resources. Which resource management technique provides the strongest
guarantee of isolation between tenants?
A. Using YARN queues with fair scheduler and resource limits.
B. Implementing Ranger policies to control access to data directories.
C. Deploying multiple Cloudera Management Service (CMS) instances, one for each tenant.
D. Utilizing Kubernetes namespaces to deploy separate YARN clusters for each tenant.
E. Configuring Knox to serve as a gateway, authenticating and authorizing all tenant requests.
Answer: D
Explanation:
Kubernetes namespaces allow for the creation of completely isolated environments, including separate
YARN clusters, offering the strongest isolation. YARN queues provide resource isolation, but within a
single cluster. Ranger and Knox focus on authorization and authentication, not resource isolation at the
compute level. Multiple CMS instances don't guarantee compute isolation.
Question: 86
In a Cloudera CDP on-premises deployment, you need to configure High Availability (HA) for the
NameNode. Which of the following steps are crucial for ensuring a successful HA setup?
A. Configuring a shared edit log storage (e.g., using NFS or a quorum-based journal node).
B. Deploying a ZooKeeper quorum to manage active NameNode election.
C. Ensuring both NameNodes have identical hardware specifications.
D. Setting up automatic failover using fencing methods (e.g., STONITH).
E. Configuring multiple Replication factor with in HDFS file system
Answer: A,B,D
Explanation:
A shared edit log is essential for NameNode HA to maintain state consistency. ZooKeeper manages the
active NameNode election process. Automatic failover with fencing prevents split-brain scenarios.
Identical hardware is recommended, not strictly crucial. Replication factor is related to data redundancy
not related to name node fail over.
Question: 87
Which of the following configuration settings directly impacts the multi-tenancy capabilities of YARN in a
Cloudera CDP on-premises environment?
A. The ‘yarn.scheduler.capacity.maximum-am-resource-percent’ property.
B. The yarn.resourcemanager.ha.enabled' property.
C. The 'fs.defaultFS' property in core-site.xml.
D. The 'dfs.nameservices’ property in hdfs-site.xml.
E. The ‘yarn.nodemanager.resource.memory-mb' property
Answer: A
Explanation:
‘yarn.scheduler.capacity.maximum-am-resource-percent' limits the percentage of cluster resources that
can be used by application masters, preventing a single tenant from monopolizing resources.
‘yarn.resourcemanager.ha.enabled’ enables HA for the ResourceManager. 'fs.defaultFS and
'dfs.nameservices' configure the default filesystem. ‘yarn.nodemanager.resource.memory-mb' defines
total available memory on NM
Question: 88
You have a multi-tenant Cloudera CDP cluster. One tenant reports slow query performance. Upon
investigation, you find they are consuming a disproportionate amount of CPU resources. Which YARN
queue property can you adjust to limit the CPU usage for that specific tenant?
A. yarn.scheduler.fair.queue..weight’
B. yarn.scheduler.fair.queue..maxResourceS
C. yarn.scheduler.fair.queue..minResources'
D. yarn.scheduler.fair.queue..maxAMShare'
E. ‘yarn.scheduler.capacity.queue..maximum-capacity’
Answer: B
Explanation:
‘yarn.scheduler.fair.queue..maxResourceS lets you define the maximum resources (CPU and memory) a
queue can use. While 'weight' influences share allocation, 'maxResources' sets a hard limit.
‘minResourceS guarantees a minimum allocation. ‘maxAMShare’ limits AM resource usage.
‘yarn.scheduler.capacity.queue..maximum-capacity’ applies to the CapacityScheduler
Question: 89
A Cloudera CDP on-premises cluster has several tenants sharing the same Hive Metastore. You want to
ensure that one tenant cannot access data belonging to another tenant through Hive. Which is the
MOST effective security measure to achieve this?
A. Implement Hive authorization using SQL standards-based authorization (Storage Based Authorization)
with Ranger.
B. Configure separate HiveServer2 instances for each tenant.
C. Create separate Kerberos principals for each tenant.
D. Enable Hive's built-in authorization (HiveServer2 authorization).
E. Isolate each tenant's data in separate HDFS clusters.
Answer: A
Explanation:
SQL standards-based authorization (Storage Based Authorization) with Ranger provides granular access
control based on SQL privileges and policies defined in Ranger, allowing you to restrict access to specific
databases, tables, or even columns. Separate HiveServer2 instances offer some isolation, but still rely on
the same Metastore and underlying data. Kerberos principals handle authentication but not
authorization within Hive. Hive's built-in authorization is less powerful than Ranger. Separate HDFS
clusters increase complexity significantly.
Question: 90
You are planning to migrate a Cloudera CDH 5 cluster to CDP on-premises, focusing on high availability.
Which component's HA configuration requires the most significant architectural change during the
migration?
A. HDFS NameNode
B. YARN ResourceManager
C. Hive Metastore
D. Oozie Server
E. Spark History Server
Answer: B
Explanation:
While all components benefit from HA, the YARN ResourceManager HA configuration often undergoes
the most significant architectural change because CDH 5 HA configuration is active/standby, while CDP
uses a more distributed active/active ResourceManager HA using LevelDB as its state store that better
utilizes cluster resources. CDH5 using Zookeeper and automatic failover, while CDP leverages improved
LevelDB state management and active/active configuration. NameNode HA is generally similar in
concept, though implementation details differ. Hive Metastore, Oozie, and Spark History Server HA are
relatively straightforward migrations.
Question: 91
Which of the following are valid fencing mechanisms used in HDFS NameNode High Availability (HA) to
prevent split-brain scenarios?
A. STONITH (Shoot The Other Node In The Head)
B. Shared Storage Fencing
C. Resource Manager Fencing
D. Kafka Based Fencing
E. ZooKeeper Fencing
Answer: A,B
Explanation:
STONITH is a common fencing method that physically powers off or resets the failed NameNode. Shared
Storage Fencing prevents the failed NameNode from writing to the shared edit log. Resource Manager
Fencing, Kafka and Zookeeper fencing are not direct fencing mechanisms for HDFS NameNode HA.
Question: 92
You have a Cloudera CDP on-premises cluster with multiple tenants. One tenant is running a Spark
application that is causing excessive disk I/O on the DataNodes, impacting other tenants. How can you
mitigate this issue using YARN?
A. Configure YARN node labels and assign the tenant's application to a specific set of nodes with faster
storage.
B. Increase the replication factor of the data used by the Spark application.
C. Implement a YARN queue with a lower priority for the tenant's application.
D. Enable data locality optimizations in Spark.
E. Throttle the number of executors used by the application through spark-defaults.conf
Answer: A
Explanation:
YARN node labels allow you to tag specific nodes with characteristics (e.g., 'fast-storage') and then
target applications to those nodes. Increasing replication impacts storage usage, not I/O. Lowering
queue priority might delay execution, but not address the I/O issue directly. Data locality is a general
optimization, not a specific mitigation for excessive I/O. Throttling the number of executors could
mitigate somewhat the load but node label is the best option.
Question: 93
A critical Impala daemon in your CDP cluster fails. You have configured Impala HA. Which process is
responsible for automatically restarting the failed Impala daemon?
A. Cloudera Manager Agent
B. ZooKeeper
C. Impala Statestore
D. YARN ResourceManager
E. Systemd
Answer: A
Explanation:
The Cloudera Manager Agent monitors the health of the Impala daemons and is responsible for
automatically restarting them in case of a failure, provided that auto-restart is enabled in the Cloudera
Manager configuration.
Question: 94
Your organization requires different tenants in a Cloudera CDP on-premises cluster to use different
versions of Spark. How can you achieve this level of isolation and version control?
A. Deploy multiple Spark3 services, each configured with a specific version of Spark, and assign each
tenant to a dedicated Spark3 service.
B. Use YARN's containerization features to isolate each tenant's Spark applications with specific
versioned libraries.
C. Configure the ‘spark-defaults.conf' file with different Spark versions for each tenant's user account.
D. Utilize Kubernetes and Spark on Kubernetes to deploy separate Spark clusters for each tenant, each
with its desired Spark version.
E. Use the ‘spark.version’ property to dynamically select the Spark version at runtime for each tenant's
application.
Answer: D
Explanation:
Kubernetes and Spark on Kubernetes allows for complete isolation, including using different Spark
versions. The Spark driver and executors run in isolated containers. Deploying Multiple Spark3 services
not best way because the Spark3 service in CDP meant to provide spark functionality instead of isolating
tenants, it is better to isolate by other means. YARN containers provide some isolation but not complete
version isolation. User specific ‘spark-defaults.conf not possible. The ‘spark.version’ property doesn't
exist.
Question: 95
You observe frequent "Out of Memory errors in your Cloudera CDP on-premises cluster, primarily
affecting a specific tenant. This tenant is running complex Hive queries. Which of the following actions
can you take to mitigate these errors without impacting other tenants?
A. Adjust the 'hive.tez.container.size' property for the specific YARN queue assigned to the tenant.
B. Increase the 'hive.auto.convert.join.noconditionaltask.size’ property globally in Hive's configuration.
C. Implement resource quotas in Ranger to limit the tenant's overall resource consumption.
D. Force all queries to use MapReduce as the execution engine.
E. Adjust the global ‘yarn.nodemanager.resource.memory-mb' property
Answer: A
Explanation:
Adjusting 'hive.tez.container.size’ for the tenant's YARN queue allows you to control the memory
allocated to Tez containers, preventing OOM errors for that tenant without impacting others. Increasing
'hive.auto.convert.join.noconditionaltask.size' could lead to increased memory consumption. Resource
quotas in Ranger primarily control access, not memory allocation within a job. Forcing MapReduce is
usually less efficient. ‘yarn.nodemanager.resource.memory-mb' affects all NM containers.
Question: 96
You are configuring HA for the Hive Metastore in your Cloudera CDP on-premises cluster. You choose to
use a highly available database as the backend. Which of the following is the MOST important factor to
consider when selecting the database?
A. The database's ability to handle a large number of concurrent connections.
B. The database's compatibility with the Hive Metastore schema.
C. The database's storage capacity.
D. The database's backup and recovery capabilities.
E. The database's geographical proximity to the Hadoop cluster.
Answer: A
Explanation:
The Hive Metastore needs to serve a large number of concurrent requests from HiveServer2 instances,
Impala, and other services. Therefore, the database's ability to handle concurrent connections is the
most critical factor. While compatibility is necessary, most common databases are compatible. Storage
capacity is usually not a major concern for the Metastore. Backup and recovery are important, but
secondary to concurrency. Geographical proximity can help with latency, but is less important than
concurrency.
Question: 97
During a CDP Private Cloud Base installation using Cloudera Manager, which of the following statements
regarding the use of custom parcel repositories is MOST accurate?
A. Custom parcel repositories are automatically detected by Cloudera Manager without any
configuration.
B. You must explicitly add the URL of each custom parcel repository in Cloudera Manager's settings
before Cloudera Manager can use them.
C. Custom parcel repositories can only be used if they are accessible via HTTPS with a valid SSL
certificate.
D. Cloudera Manager only supports a single custom parcel repository.
E. Custom parcel repositories can be used for any service in CDP, including services not provided by
Cloudera.
Answer: B
Explanation:
Cloudera Manager requires explicit configuration for custom parcel repositories. You need to add the
URL in Cloudera Manager Admin Console to allow it to use them.
Question: 98
You are tasked with automating the installation of CDP Private Cloud Base using the Cloudera Manager
API. Which of the following API endpoints would be used to trigger the installation of a service?
A. /api/v31/clusters/{clusterName}/services/{serviceName}/commands/start
B. /api/v31/clusters/{clusterName}/commands/deployClientConfig
C. /api/v31/clusters/{clusterName}/services/{serviceName}/commands/install
D. /api/v31/clusters/{clusterName}/commands/installServices
E. /api/v31/clusters/{clusterName}/services/{serviceName}/commands/deployClientConfig
Answer: C
Explanation:
The
endpoint is used to trigger the installation of a specific service on a cluster through the Cloudera
Manager API.
Question: 99
During a Cloudera Manager installation, you encounter an error indicating that the database schema
version is incompatible. Which of the following steps are MOST likely to resolve this issue? (Select TWO)
A. Upgrade Cloudera Manager to the latest version.
B. Drop and recreate the Cloudera Manager database schema.
C. Downgrade the database server to an older version.
D. Run the Cloudera Manager schema upgrade tool.
E. Reinstall the operating system on all nodes.
Answer: A,D
Explanation:
An incompatible database schema version typically indicates a need to upgrade Cloudera Manager to a
version compatible with the existing schema or to use the schema upgrade tool provided by Cloudera
Manager to update the schema to the required version. Dropping and recreating the schema is generally
a last resort and should be done with extreme caution.
Question: 100
After installing Cloudera Manager, you are unable to connect to the Cloudera Manager Admin Console
via your web browser. Which of the following could be the MOST likely cause?
A. The Cloudera Manager Server process is not running.
B. The Cloudera Manager Agent is not installed on the Cloudera Manager Server host.
C. The Cloudera Manager license has expired.
D. The web browser cache needs to be cleared.
E. The Cloudera Manager Server is configured to listen on the wrong port.
Answer: A
Explanation:
If you cannot connect to the Cloudera Manager Admin Console, the most likely cause is that the
Cloudera Manager Server process is not running. You can check the status of the process using systemctl
or a similar command.
Question: 101
You are configuring a high-availability Cloudera Manager deployment. Which of the following databases
are supported for the Cloudera Manager Server's metadata?
A. MySQL/MariaDB, PostgreSQL, Oracle
B. MongoDB, Cassandra, HBase
C. SQLite, Derby, HSQLDB
D. Redis, Memcached, etcd
E. Only Oracle is supported for HA.
Answer: A
Explanation:
Cloudera Manager supports MySQL/MariaDB, PostgreSQL, and Oracle as databases for its metadata.
These databases are suitable for high-availability deployments due to their robustness and support for
replication.
Question: 102
You are deploying CDP Private Cloud Base and need to configure a specific Java Development Kit (JDK)
for Cloudera Manager. Where would you typically specify the JDK path during installation?
A. In the ‘cm-admin-env.sh' file on the Cloudera Manager Server host.
B. In the ‘cloudera-scm-agent.inr file on each host.
C. Through the Cloudera Manager Admin Console after installation.
D. By setting the environment variable in the system-wide environment configuration.
E. In the /etc/cloudera-scm-server/config.ini file
Answer: A
Explanation:
The ‘cm-admin-env.sm file on the Cloudera Manager Server host is the typical location to specify the JDK
path for Cloudera Manager. This allows you to control the specific JDK version used by Cloudera
Manager.
Question: 103
You are using Cloudera Manager to deploy a new CDP Private Cloud Base cluster. The installation fails
with an error related to insufficient swap space on one of the hosts. How would you MOST effectively
address this issue?
A. Disable swap on all hosts in the cluster.
B. Increase the amount of physical RAM on the host.
C. Increase the swap space on the host by modifying the swap file size or adding a swap partition.
D. Ignore the warning and proceed with the installation.
E. Decrease the amount of physical RAM on other hosts to compensate.
Answer: C
Explanation:
The most effective way to address insufficient swap space is to increase the swap space on the affected
host. This can be done by modifying the swap file size or adding a swap partition. While increasing RAM
is beneficial, it doesn't directly address the swap space requirement.
Question: 104
Which of the following is the MOST accurate description of the role of the Cloudera Manager Agent?
A. It is responsible for managing the Cloudera Manager Server.
B. It monitors the health of the operating system on each host.
C. It executes commands and monitors the services on each host in the cluster, reporting back to the
Cloudera Manager Server.
D. It provides a web interface for managing the cluster.
E. It only installs packages; it does not monitor services.
Answer: C
Explanation:
The Cloudera Manager Agent is responsible for executing commands and monitoring services on each
host in the cluster. It reports the status of these services back to the Cloudera Manager Server, allowing
the server to manage and monitor the entire cluster.
Question: 105
During the host inspection phase of a CDP installation using Cloudera Manager, you receive an error
message indicating that the clocks on the hosts are not synchronized. Which service should you
configure on all cluster nodes to resolve this issue and ensure that clocks are synchronized?
A. DNS
B. NTP
C. LDAP
D. Kerberos
E. Syslog
Answer: B
Explanation:
NTP (Network Time Protocol) is the standard protocol for synchronizing the clocks of computers over a
network. Configuring NTP on all cluster nodes ensures that their clocks are synchronized, resolving the
clock synchronization error during the host inspection phase.
Question: 106
You are installing CDP Private Cloud Base on a cluster with limited internet access. Which of the
following strategies would be the MOST effective for managing the installation and deployment of
software packages?
A. Configure each host to use a public proxy server.
B. Download all necessary parcels and packages to a local repository and configure Cloudera Manager to
use this repository.
C. Manually install all software packages on each host.
D. Use a satellite server for patching.
E. Disable all network services on the cluster.
Answer: B
Explanation:
Downloading all necessary parcels and packages to a local repository and configuring Cloudera Manager
to use this repository is the most effective strategy for managing software installations in an
environment with limited internet access. This approach allows you to control the packages installed
and avoid relying on a constant internet connection.
Question: 107
After installing CDP, you find that certain services are failing to start with errors related to Kerberos
authentication. Which of the following steps are necessary to verify Kerberos configuration issues during
CDP installation and startup? (Select TWO)
A. Verify that the KDC (Key Distribution Center) is reachable and running.
B. Ensure that all hosts in the cluster have synchronized clocks using NTP.
C. Reformat all disks in the cluster.
D. Disable SELinux.
E. Check the service logs for Kerberos-related error messages.
Answer: A,E
Explanation:
Verifying that the KDC is reachable and running is crucial for Kerberos authenticatiom Additionally,
checking the service logs for Kerberos-related error messages provides valuable insights into the specific
authentication failures. While clock synchronization (B) is important for Kerberos to work, the question
is asking about verifying the config issues not how to prevent them initially.
Question: 108
You are automating CDP Private Cloud Base cluster creation through Cloudera Manager API. You want to
ensure that specific host templates are applied to different groups of hosts during installation. What API
call and parameter is primarily used to map hosts to host templates?
A. /api/v31/clusters/{clusterName}/hosts, use the 'rackld' parameter to assign templates.
B. /api/v31/clusters/{clusterName}/hosts/{hostld}, use the 'hostTemplateName' parameter to assign
templates.
C. /api/v31/clusters/{clusterName}/hostTemplates, create separate host templates for each rack.
D. /api/v31/clusters/{clusterName}/hosts, after installation templates are not assigned to hosts.
E. Host templates can only be assigned through the IJI; API does not support them.
Answer: B
Explanation:
The
endpoint allows you to modify
individual host configurations. The 'hostTemplateName’ parameter enables you to associate a specific
host with a pre-defined host template, allowing for customized configurations based on host roles or
hardware specifications.
Question: 109
You are deploying Cloudera CDP on-premises using Cloudera Manager. During the installation process,
you encounter the following error: 'ERROR: Could not find a suitable JDK on host You have verified that a
supported JDK is installed. Which of the following steps is MOST likely to resolve this issue?
A. Ensure the 'JAVA HOME' environment variable is correctly set on the host and accessible by the
Cloudera Manager Agent.
B. Restart the Cloudera Manager Server and Agent.
C. Manually install the JDK using the Cloudera Manager package repository.
D. Disable the Cloudera Manager Agent firewall.
E. Re-run the Cloudera Manager Server installation process from the beginning.
Answer: A
Explanation:
The most common reason for this error is an incorrectly configured 'JAVA HOME' environment variable.
Cloudera Manager Agents rely on this variable to locate the JDK. Restarting the services might help but
directly setting JAVA_HOME is more reliable. Manual installation via Cloudera Manager is a valid option,
but checking JAVA HOME is the quicker first step.
Question: 110
Which of the following statements are TRUE regarding the use of the Cloudera Manager Agent
heartbeat mechanism during the installation of core services in a CDP on-premises cluster?
A. The Cloudera Manager Agent heartbeat is used to monitor the health and status of the host.
B. The Cloudera Manager Server uses the heartbeat to detect the presence of the Agent on a given host.
C. If the heartbeat is lost, the Cloudera Manager Server immediately uninstalls all services on the host.
D. The heartbeat interval is configurable via the Cloudera Manager I-Jl.
E. Heartbeat information is stored in ZooKeeper.
Answer: A,B,D
Explanation:
The Cloudera Manager Agent heartbeat serves to inform the Cloudera Manager Server of the host's
status and agent availability. It is crucial for monitoring and management. A lost heartbeat doesn't
immediately trigger service uninstallation; rather, it flags the host as unhealthy and allows for
intervention. The interval can indeed be configured. Heartbeat information is primarily stored and
managed within the Cloudera Manager Server, not directly in ZooKeeper, although ZooKeeper might be
used for related coordination tasks.
Question: 111
You are configuring the Kerberos security settings for your CDP on-premises cluster using Cloudera
Manager. Which of the following steps is MANDATORY before enabling Kerberos?
A. Install and configure a Key Distribution Center (KDC) that is accessible from all nodes in the cluster.
B. Configure Cloudera Manager to use LDAP for authentication.
C. Enable TLS/SSL encryption for all services in the cluster.
D. Back up all data stored in HDFS.
E. Configure the cluster to use a highly available NameNode.
Answer: A
Explanation:
A KDC is essential for Kerberos to function. It handles authentication and authorization by issuing and
managing Kerberos tickets. The other options are good practices, but the KDC is absolutely required for
Kerberos to work.
Question: 112
After deploying a new CDP on-premises cluster, you observe that the NameNode is consistently failing
to start. Upon inspecting the NameNode logs, you find the following error message: 'java.io.lOException:
Incompatible namespacelD'. What is the MOST likely cause of this issue?
A. The NameNode's metadata directory contains an older version of the namespace image.
B. The NameNode is configured to use an incorrect port number.
C. The NameNode does not have sufficient memory allocated.
D. The NameNode's data directories are corrupted.
E. The NameNode is unable to connect to the ZooKeeper quorum.
Answer: A
Explanation:
An incompatible namespacelD indicates that the NameNode is trying to start with a namespace image
(metadata) that is either from a different cluster or an older/incompatible version. This often happens
after a failed upgrade or a misconfiguration during initial setup.
Question: 113
You are tasked with automating the installation of CDP core services on-premises using a script. Which
of the following methods is the MOST recommended way to interact with Cloudera Manager for
automated deployment?
A. Directly modifying the Cloudera Manager database.
B. Using the Cloudera Manager API.
C. Manually editing the Cloudera Manager configuration files.
D. Using SSH to execute commands on the Cloudera Manager Server.
E. Executing SQL queries against the Hive Metastore directly.
Answer: B
Explanation:
The Cloudera Manager API provides a stable and supported interface for interacting with Cloudera
Manager programmatically. Modifying the database or configuration files directly is highly discouraged
as it can lead to instability and is not supported.
Question: 114
During the initial deployment of a CDP on-premises cluster, you are configuring the database settings for
the Hive Metastore. Which database types are officially supported by Cloudera for production
environments?
A. Derby
B. MySQL
C. PostgreSQL
D. Oracle
E. SQLite
Answer: B,C,D
Explanation:
Derby is only suitable for development or testing, not for production. MySQL, PostgreSQL, and Oracle
are the supported database types for the Hive Metastore in a production environment. SQLite is not
supported.
Question: 115
You are installing the Data Hub cluster in CDP On-Premises, and the installation fails with the error
"Insufficient Disk Space". You have checked and confirmed that each node has enough disk space. What
could be the reason for the error?
A. The root partition on one or more nodes is full.
B. The '/tmp’ directory on one or more nodes is full.
C. The user running the Cloudera Manager Agent does not have write permissions to the installation
directory.
D. The HDFS data directories are not properly configured.
E. The Cloudera Manager Server's disk is full.
Answer: B
Explanation:
During installation, the '/tmp’ directory is used for temporary files. Even if other partitions have
sufficient space, a full '/tmp’ can cause installation failures. Checking and cleaning 7tmp’ is a common
troubleshooting step. Although other issues can cause failures, '/tmp’ filling up is a prevalent culprit in
this scenario.
Question: 116
You're setting up Ranger KMS for encryption at rest. You have configured the connection to Vault, but
Ranger KMS fails to start. The logs show a 'javax.net.ssl.SSLHandshakeException'. Which of the following
is the MOST likely cause?
A. The Vault server is not running.
B. Ranger KMS is not properly configured with the Vault token.
C. The Vault certificate is not trusted by the Ranger KMS server.
D. The Ranger KMS database is corrupted.
E. Firewall rules are blocking communication between Ranger KMS and Vault.
Answer: C
Explanation:
An SSLHandshakeException generally indicates a problem with the SSL/TLS handshake. In this case, the
most likely cause is that Ranger KMS does not trust the certificate presented by Vault. This can happen if
Vault is using a self-signed certificate or a certificate signed by a CA that Ranger KMS doesn't recognize.
While the other issues might cause problems, the SSLHandshakeException points directly to a certificate
trust issue.
Question: 117
During the Cloudera Manager Agent installation, you encounter the following error: 'Connection
refused'. Assuming the Cloudera Manager Server is running and reachable via ping, what are the
possible root causes of this issue?
A. The Cloudera Manager Agent is not running on the host.
B. A firewall is blocking communication on the Cloudera Manager Agent port (default 7182).
C. The Cloudera Manager Agent is configured to use an incorrect hostname or IP address for the
Cloudera Manager Server.
D. The hostname resolution is not working correctly.
E. SELinux is preventing the agent from connecting.
Answer: B,C,D,E
Explanation:
A 'Connection refused' error means that the connection attempt was actively refused by the target
host. This can be caused by a firewall blocking the connection, the agent being configured to connect to
the wrong address, hostname resolution issues preventing the agent from finding the server, or SELinux
blocking the connection. The Agent not running wouldn't lead to 'connection refused' but 'connection
timeout'.
Question: 118
You are installing core services in a CDP cluster and need to ensure high availability for the NameNode.
Which of the following approaches is the MOST effective way to achieve NameNode HA using Cloudera
Manager?
A. Configure a single NameNode with a large amount of memory and CPU resources.
B. Deploy two NameNodes in an Active/Passive configuration using a shared edit log storage.
C. Manually configure multiple NameNodes and synchronize their metadata using rsync.
D. Configure a single NameNode with automatic failover to a backup server.
E. Deploy multiple NameNodes in an Active/Active configuration with distributed metadata
management.
Answer: B
Explanation:
The standard and recommended approach for NameNode HA in CDP is to use two NameNodes in an
Active/Passive configuration. The shared edit log storage (e.g., using a Quorum Journal Manager - QJM)
ensures that the passive NameNode can quickly take over if the active NameNode fails. While other
solutions exist, this is the most supported and easiest to manage within Cloudera Manager.
Question: 119
While configuring TLS encryption for Kafka brokers during CDP installation, you encounter the following
error: ‘java.security.cert.CertificateException: No subject alternative names present’. What steps can
you take to troubleshoot and resolve this issue?
A. Disable TLS encryption for the Kafka brokers.
B. Regenerate the Kafka broker certificates with the correct Subject Alternative Names (SANs) including
the broker's hostname and IP address.
C. Update the Kafka broker's 'server.properties’ file to ignore the certificate validation.
D. Import the Kafka broker's certificate into the Java keystore on the Cloudera Manager Server.
E. Ensure all broker hostnames resolve to the same IP address.
Answer: B
Explanation:
The error 'No subject alternative names present' means that the Kafka client (or another broker) is
trying to connect to a Kafka broker using a hostname or IP address that is not listed in the Subject
Alternative Name (SAN) extension of the broker's certificate. The correct solution is to regenerate the
certificate with the appropriate SANs. While importing the certificate might seem like a solution, it just
masks the problem instead of fixing it at the root.
Question: 120
You are upgrading your CDP on-premises cluster from CDH 6.x to CDP 7.x. After the upgrade, you notice
that some of your MapReduce jobs are failing with ClassNotFoundException. The missing classes are
related to custom dependencies included in your old CDH environment. Which of the following is the
MOST effective approach to resolve this issue in CDP?
A. Copy the missing JAR files directly into the [lib directory of the Hadoop installation on each node.
B. Modify the Hadoop classpath using the 'hadoop-env.sm script to include the location of the missing
JAR files.
C. Package the custom dependencies into a shaded JAR and include it with your MapReduce job.
D. Recompile the MapReduce jobs against the CDP 7.x Hadoop libraries and redeploy them.
E. Use Cloudera Manager to deploy the missing JARs to all nodes in the cluster.
Answer: C,D
Explanation:
Copying JARs directly or modifying ‘hadoop-env.sh' is generally discouraged as it can lead to dependency
conflicts and makes the cluster harder to manage. The best approaches are to either package the
dependencies into a shaded JAR to isolate them from the system classpath (C) or to recompile the jobs
against the CDP 7 .x libraries to ensure compatibility (D). In some scenarios, recompilation (D) might be
necessary, but using a shaded JAR (C) provides better isolation. Using Cloudera Manager for deployment
is not directly supported in this fashion.
Question: 121
You're deploying a CDP on-premises cluster using parcels. After distributing the parcels to the cluster
nodes, you notice that the activation process consistently fails. Which of the following is the MOST likely
cause?
A. The Cloudera Manager Agent user lacks sufficient permissions to write to the parcel staging directory.
B. The parcel's checksum does not match the checksum stored in the Cloudera Manager database.
C. The /opt/cloudera/parcel directory is not created on all cluster nodes.
D. The Cloudera Manager Server's database is corrupted.
E. Insufficient disk space on the nodes in /opt/cloudera/parcel.
Answer: A,B,E
Explanation:
Parcel activation requires proper permissions for the Cloudera Manager Agent user to unpack the
parcel. A mismatching checksum indicates data corruption during transfer or storage. Ensure you have
enough disk space on each node.
Question: 122
You are upgrading your CDP on-premises cluster using parcels. During the upgrade process, one of the
services fails to start after the activation of the new parcel. Examining the service logs, you find errors
related to missing libraries. What's the most probable reason?
A. The new parcel version has a dependency on a library not present on the system.
B. The old parcel version was not properly deactivated before activating the new parcel.
C. The Cloudera Manager Server was not restarted after the parcel activation.
D. The service's configuration files were not updated to reflect the new parcel location.
E. Incorrect user permissions on the service's binaries within the new parcel.
Answer: A
Explanation:
New parcel versions can introduce dependencies on libraries not present in the base OS or previous
parcel versions. This is a common cause of service startup failures after upgrades.
Question: 123
You are configuring a local parcel repository for your CDP on-premises cluster After configuring the
repository URL in Cloudera Manager, the parcels do not appear. What is the first step to troubleshoot
this?
A. Restart the Cloudera Manager Server.
B. Verify that the Cloudera Manager Agent user has read permissions on the parcel files in the local
repository.
C. Check the Cloudera Manager Server logs for errors related to accessing the parcel repository.
D. Run the ‘sudo yum update' command on all cluster nodes.
E. Verify that a .sha file exists for each .parcel file in the repository and that the SHA matches.
Answer: C,E
Explanation:
Checking the Cloudera Manager Server logs provides immediate insight into any connectivity or
permission issues accessing the repository. Also, verification of .sha files are important for parcel
validation.
Question: 124
When installing a parcel, Cloudera Manager reports a 'checksum mismatch' error. What steps should
you take to resolve this issue?
A. Download the parcel file again from the official Cloudera repository.
B. Disable checksum verification in Cloudera Manager (not recommended).
C. Verify the SHA checksum of the downloaded parcel against the checksum provided by Cloudera.
D. Restart the Cloudera Manager Server.
E. Delete the incomplete parcel from lopt/cloudera/parcel and retry.
Answer: A,C,E
Explanation:
The first step is to re-download the parcel, as it's likely corrupted during the initial download. Then,
verify the downloaded parcel's checksum against the official checksum to confirm its integrity.
Question: 125
You have activated a new parcel on your CDP cluster. However, one of the services continues to use the
binaries from the older parcel.
What's the most likely reason, assuming service configuration is correct?
A. The service was not restarted after the parcel activation.
B. The /var/run directory is pointing to older version files.
C. The service's environment variables are still pointing to the old parcel directory.
D. The service is using a cached version of the binaries.
E. The active parcel symlink in /opt/cloudera/parcels is not updated correctly.
Answer: A,C,E
Explanation:
Services need to be restarted to use the new binaries. Also environment variable settings and broken
symlinks are likely root causes of the services using the older version files.
Question: 126
You are deploying a custom service on CDP on-premises using a parcel. The service requires specific
system-level libraries. How should you ensure these libraries are available on all cluster nodes?
A. Include the required libraries directly within the custom service parcel.
B. Install the libraries using the system package manager (e.g., yum, apt) on each node before
distributing the parcel.
C. Create a separate parcel containing the required libraries and ensure it's activated before the custom
service parcel.
D. Configure Cloudera Manager to automatically install the libraries during parcel distribution.
E. Update /etc/ld.so.conf to include the path to custom libraries.
Answer: B,C
Explanation:
Distributing OS level packages and the use of dependencies parcels are recommended options.
Question: 127
You are tasked with upgrading a specific service within your CDP cluster to a newer version provided as
a parcel. However, this upgrade should only affect a subset of the cluster nodes. How can you achieve
this?
A. It is not possible to target parcel activation to specific nodes. Parcels are always activated cluster-
wide.
B. Manually distribute the parcel to the targeted nodes and activate it through the command line.
C. Use Cloudera Manager's host template feature to apply different parcel configurations to different
node groups.
D. Modify the Cloudera Manager Agent configuration on the targeted nodes to point to a different
parcel repository.
E. Use Cloudera Manager API to script custom parcel deployments to target hosts.
Answer: C,E
Explanation:
Host Templates and CM API's allow for precise application of configurations and parcels to specific
nodes.
Question: 128
After activating a new parcel, you notice increased CPU utilization across the cluster. You suspect the
new service version is causing this. How can you quickly revert to the previous parcel version?
A. Deactivate the new parcel and activate the previous parcel version through Cloudera Manager.
B. Manually replace the binaries in /opt/cloudera/parcels with the older versions.
C. Restore the Cloudera Manager database to a backup taken before the parcel activation.
D. Reinstall the older service version using the system package manager.
E. Rollback option from Cloudera Manager IJI.
Answer: A,E
Explanation:
The fastest way to revert is to deactivate the new parcel and activate the previous version through
Cloudera Manager and rolling back . This ensures that all services are using the older binaries.
Question: 129
Your CDP cluster has multiple services installed via parcels. You need to determine the exact parcel
version used by a specific service instance. How can you achieve this?
A. Check the service's process ID (PID) file for the parcel version.
B. Examine the service's configuration files for references to the parcel directory.
C. Use the Cloudera Manager API to query the service instance for its parcel version.
D. Check the service logs for startup messages indicating the parcel version.
E. Login to cloudera manager and navigate to specific service and then you can determine parcel version
under configuration tab.
Answer: C,D,E
Explanation:
Cloudera Manager API and the service logs often contain information about the parcel version being
used. Also UI can show such details.
Question: 130
You have downloaded a parcel and its corresponding ' .sha' file. However, when attempting to verify the
checksum using 'sha256sum' , the output doesn't match the value in the .sha' file, but you are sure you
have downloaded the correct file. What might be causing this discrepancy?
A. The .sha’ file is corrupted.
B. The encoding of the .sha’ file is incorrect (e.g., UTF-16 instead of UTF-8).
C. You are using the wrong hashing algorithm (e.g., 'md5sum’ instead of ‘sha256sum').
D. There's a hidden character (like a newline) at the end of the checksum in the .sha’ file.
E. Permissions are incorrect on the parcel file.
Answer: B,C,D
Explanation:
Incorrect encoding or the presence of hidden characters in the .sha’ file can alter the calculated
checksum. Also, using the incorrect hash algorithm will obviously lead to a mismatch.
Question: 131
You are using a custom parcel repository served over HTTPS. However, Cloudera Manager is unable to
download parcels from the repository, throwing SSL errors. What steps should you take to resolve this
issue?
A. Disable SSL verification in Cloudera Manager (highly discouraged).
B. Import the SSL certificate of the HTTPS server into the Java truststore used by the Cloudera Manager
Server.
C. Ensure that the HTTPS server is using a valid SSL certificate signed by a trusted Certificate Authority
(CA).
D. Configure the Cloudera Manager Agent to use HTTP instead of HTTPS for accessing the repository.
E. Ensure the hostname used in the URL exactly matches the CN/SAN in the SSL certificate.
Answer: B,C,E
Explanation:
Importing the certificate into the truststore allows the Java-based Cloudera Manager Server to trust the
HTTPS server. Also validating that the certificate is issued by CA and hostname validations are important
to resolve SSL issues.
Question: 132
Assume you are facing issues related to Parcel distribution. The below Python script is intended to
download parcel files. Find out which of the below code Snippet will resolve the problem if CM API call is
failing due to invalid credentials, connection refused errors or invalid URL etc. Assume other code is
correct. In other words provide code to handle exceptions
A.
B.
C.
D.
E.
Answer: A,E
Explanation:
Snippet A, E incorporates robust error handling using a ‘try...except’ block to catch
‘requests.exceptions.RequestException’ , which covers various potential issues, including connection
errors, invalid URLs, and HTTP errors. Additionally, using 'response.raise_for_status()' is to make sure
that the URL exists, if not its thrown exception.
Question: 133
You are deploying a CDP Private Cloud Base cluster with Kerberos enabled. After the initial deployment,
you notice that some Hadoop services are failing to start, with errors related to keytab files. What are
the MOST likely causes of this issue? (Select TWO)
A. The keytab files were not properly distributed to all nodes in the cluster.
B. The Kerberos principal used for the service does not exist in the KDC.
C. The clocks on the Hadoop nodes are not synchronized with the KDC server.
D. Incorrect permissions are set on the keytab files, preventing the service from reading them.
E. The Java Cryptography Extension (JCE) Unlimited Strength Jurisdiction Policy Files are not installed.
Answer: A,D
Explanation:
Keytab files are essential for Kerberos authentication. If they are not distributed correctly, or the service
doesn't have the correct permissions to read them, services will fail to start. Clock skew is also a
common Kerberos issue, but is not directly related to keytab distribution. While JCE can be a cause of
other Kerberos issues, its is not the first cause to investigate in this case. Ensuring that Kerberos
principals exist in the KDC is part of pre-deployment setup.
Question: 134
During the installation of CDP Private Cloud Base, you encounter an error in Cloudera Manager related
to network connectivity between the Cloudera Manager Server and the hosts. Specifically, you receive a
message indicating that the 'Ping Test' failed. What is the MOST likely root cause of this issue?
A. The Cloudera Manager Agent is not installed on the target hosts.
B. Firewall rules are blocking ICMP traffic between the Cloudera Manager Server and the hosts.
C. The /etc/hosts file is not correctly configured on either the Cloudera Manager Server or the hosts.
D. There is a DNS resolution problem, preventing the Cloudera Manager Server from resolving the
hostnames of the target hosts.
E. Insufficient memory is allocated to the Cloudera Manager Server.
Answer: B
Explanation:
The 'Ping Test' in Cloudera Manager specifically checks for basic network connectivity using ICMP. A
firewall blocking ICMP is the most direct and common reason for this test to fail. While DNS resolution
issues and incorrect /etc/hosts entries can cause connectivity problems, the 'Ping Test' is primarily
reliant on ICMP, and firewall settings are more likely cause a Ping test failure directly.
Question: 135
You are deploying a new CDP Private Cloud Base cluster and want to automate the host configuration
process. Which of the following methods is the MOST efficient and recommended approach?
A. Manually configure each host through the Cloudera Manager Admin Console.
B. Use a custom shell script to install the Cloudera Manager Agent and perform initial configurations on
each host.
C. Leverage automated deployment tools such as Ansible, Puppet, or Chef, in conjunction with Cloudera
Manager's API.
D. Create a custom RPM package containing the Cloudera Manager Agent and manually install it on each
host.
E. Use Cloudera Altus Director to provision and configure the cluster.
Answer: C
Explanation:
Automated deployment tools like Ansible, Puppet, and Chef are designed for infrastructure as code.
Using these tools in conjunction with the Cloudera Manager API allows for consistent and repeatable
deployments, reducing manual errors and saving time. Cloudera Altus Director has been replaced with
newer technologies like CDP and Cloudera Manager's API, so is not correct.
Question: 136
You have deployed a CDP Private Cloud Base cluster and are using Ranger for centralized security. You
need to configure Ranger to authorize access to Hive resources based on user roles. Which of the
following steps are necessary to achieve this? (Select TWO)
A. Create Ranger policies that map user roles to specific Hive resources and permissions.
B. Configure the Hive metastore to synchronize with the Ranger authorization policies.
C. Enable the Ranger Hive plugin in the Hive service configuration within Cloudera Manager.
D. Import the Hive metastore schema directly into Ranger.
E. Set the 'hive.security.authorization.manager’ property in ‘hive-site.xmr to
‘org.apache.hadoop.hive.ql.security.authorization.plugin.RangerHiveAuthorizer’.
Answer: A,C
Explanation:
Ranger policies define the access control rules based on roles and resources. Enabling the Ranger Hive
plugin ensures that Hive uses Ranger for authorization. Configuring Hive metastore to directly
synchronize with Ranger policies is not a standard procedure. The 'hive.security.authorization.manager’
property is critical for enabling Ranger authorization. Importing the metastore schema directly into
Ranger is unnecessary, Ranger plugin automatically handles that connection.
Question: 137
During a CDP Private Cloud Base deployment, you are configuring the Cloudera Manager database. You
choose to use an external PostgreSQL database. Which of the following steps are CRITICAL to ensure
that Cloudera Manager can successfully connect to and utilize the PostgreSQL database? (Select TWO)
A. Install the PostgreSQL JDBC driver on the Cloudera Manager Server host.
B. Create a dedicated PostgreSQL user and database for Cloudera Manager with appropriate
permissions.
C. Configure the PostgreSQL server to listen on all interfaces (0.0.0.0).
D. Disable SELinux on the Cloudera Manager Server host.
E. Ensure that the PostgreSQL server's ‘pg_hba.conf file allows connections from the Cloudera Manager
Server's IP address.
Answer: A,E
Explanation:
The JDBC driver is required for Java applications (like Cloudera Manager) to interact with PostgreSQL.
The ‘pg_hba.conf file controls which hosts are allowed to connect to the PostgreSQL server. Creating a
dedicated user and database are recommended best practices but are not strictly critical for basic
connectivity, as the 'postgres' superuser could technically be used. Disabling SELinux is generally not
recommended. Configuring PostgreSQL to listen on all interfaces is a potential security risk. The correct
way to enable Cloudera Manager access to PostgreSQL is to allow only the necessary connections via the
‘pg_hba.conf file.
Question: 138
You are using Cloudera Manager to deploy a new service within your CDP Private Cloud Base cluster.
The deployment fails with a 'Configuration validation failed' error. How do you MOST effectively
troubleshoot this issue?
A. Examine the Cloudera Manager Server logs for detailed error messages and configuration conflicts.
B. Check the service's logs on each host in the cluster.
C. Review the Cloudera Manager Agent logs on each host.
D. Search the Cloudera Knowledge Base for known issues related to the service and configuration
parameters.
E. Restart the Cloudera Manager Server.
Answer: A
Explanation:
Configuration validation errors typically originate within the Cloudera Manager Server's logic, which
validates configurations before deploying them to agents. Cloudera Manager logs will contain the most
relevant and detailed error messages related to the configuration validation failure. Checking service or
agent logs will generally not give the reason for this configuration validation error. A restart is unlikely to
help.
Question: 139
You're setting up a highly available (HA) Cloudera Manager Server. Which of the following components
are REQUIRED to achieve HA for Cloudera Manager? (Select TWO)
A. A load balancer to distribute traffic between the active and passive Cloudera Manager Servers.
B. A shared Cloudera Manager database accessible to both the active and passive servers.
C. A distributed file system (e.g., HDFS) to store Cloudera Manager configuration files.
D. A dedicated ZooKeeper ensemble to manage the failover process.
E. A separate network interface card for each Cloudera Manager Server instance.
Answer: A,B
Explanation:
A load balancer directs traffic to the active Cloudera Manager server, automatically switching to the
passive server in case of failure. A shared database is essential to maintain consistent state between the
active and passive instances. While HDFS is used by CDP, it is not used for Cloudera Manager
configuration files during HA setup. ZooKeeper is used by some services within CDP but not directly by
Cloudera Manager HA. Multiple NICs are not a requirement for HA of Cloudera Manager.
Question: 140
You are deploying a CDP Private Cloud Base cluster with a large number of nodes (over 200). You notice
that the Cloudera Manager Server is experiencing performance issues, particularly during service
deployments and configuration updates. Which of the following strategies would be MOST effective in
improving the performance of the Cloudera Manager Server? (Select TWO)
A. Increase the heap size allocated to the Cloudera Manager Server process.
B. Migrate the Cloudera Manager database to a faster storage system (e.g., SSD).
C. Reduce the number of monitoring metrics collected by Cloudera Manager agents.
D. Distribute the Cloudera Manager Agent load across multiple Cloudera Manager Servers.
E. Disable the Cloudera Manager Event Server.
Answer: A,B
Explanation:
Increasing the heap size allows the Cloudera Manager Server to handle more data in memory, reducing
the need for disk I/O. Moving the database to faster storage significantly improves the speed of
database queries, which are frequent during deployments and updates. Reducing metrics can help, but
the impact is less significant than heap size or database performance. Distributing Agent load is not
possible. Disabling the event server impacts monitoring, but does not directly address performance.
Question: 141
You have deployed a CDP Private Cloud Base cluster with both HDFS and Ozone services. You want to
configure Hive to use Ozone as its storage backend instead of HDFS for a specific set of tables. What
configuration steps must you take within Hive to achieve this?
A. Set the property in ‘core-site.xml' to the Ozone namespace (e.g., '03fs://bucket.omserviceidP).
B. Create a new Hive warehouse directory on Ozone and configure Hive to use it for the specific tables
using the ‘hive.metastore.warehouse.dir’ property.
C. Use the 'LOCATION’ clause in the 'CREATE TABLE statement to specify the Ozone path for each table
you want to store on Ozone (e.g., 'CREATE TABLE my_table LOCATION
'03fs://bucket.omserviceid/my_table'$).
D. Set the ‘ozone.enabled' property to ‘true' in 'hive-site.xmr.
E. Configure the Ozone delegation token for the Hive user in the Ranger policies.
Answer: C
Explanation:
The ‘LOCATION' clause in the ‘CREATE TABLE statement allows you to specify the exact storage location
for each table, overriding the default HDFS location. The 'fs.defaultFS' affects all access to the filesystem.
While a new warehouse directory would be required, it is the LOCATION clause that binds a table to that
location. The 'ozone.enabled' property is not a standard Hive property and Ozone delegation token
configuartion is related to authorization, not storage destination.
Question: 142
You have deployed a CDP Private Cloud Base cluster. You notice that the Cloudera Manager Server is
consuming a high amount of CPU resources. After investigation, you determine that the resource
utilization is due to frequent garbage collection (GC) cycles. Which of the following configuration
changes would be MOST effective in reducing the frequency of GC cycles and improving Cloudera
Manager Server performance?
A. Increase the maximum heap size C-Xmk) allocated to the Cloudera Manager Server process.
B. Reduce the number of monitored services in the cluster.
C. Switch to a different garbage collection algorithm (e.g., GIGC instead of CMS).
D. Decrease the frequency of Cloudera Manager's metric collection intervals.
E. Enable JVM compression.
Answer: A
Explanation:
Increasing the maximum heap size gives the JVM more memory to work with, reducing the need for
frequent garbage collection cycles. While reducing monitored services or metric collection intervals may
have a slight impact, the primary bottleneck is usually the available heap space. Switching GC algorithms
can help in some cases, but increasing heap size is the first and most effective step. Enabling JVM
compression is not directly related to decreasing Garbage collection.
Question: 143
You are automating the deployment of a CDP Private Cloud Base cluster using Ansible. You need to
configure the 'cm_api' module in Ansible to authenticate with the Cloudera Manager API using a
username and password. Which of the following code snippets demonstrates the correct way to define
the authentication parameters in your Ansible playbook?
A.
B.
C.
D.
E.
Answer: A
Explanation:
The ‘cm_api' module in Ansible uses the 'hostname', 'port’ , ‘username' , and ‘password' parameters for
authentication. The example demonstrates the correct way to define these parameters within the
'cm_api' task. It is important to use 'username' and 'password' for credentials, as ‘user' and ‘pass' or
other attribute names will not be understood by the Ansible module.
Question: 144
You are planning the upgrade of your CDP Private Cloud Base cluster from version 7.1 .x to 7.2.x. Before
initiating the upgrade process through Cloudera Manager, what are the MOST critical pre-upgrade tasks
you MUST perform to ensure a successful upgrade? (Select TWO)
A. Back up the Cloudera Manager Server database and the Cloudera Manager Agent configuration
directories.
B. Run the Cloudera Manager pre-upgrade health checks and resolve any identified issues.
C. Download and install the latest version of the Cloudera Manager Agent on all hosts in the cluster.
D. Update all operating system packages on all hosts in the cluster to the latest versions.
E. Disable all running services in the cluster to prevent data corruption during the upgrade.
Answer: A,B
Explanation:
Backing up the Cloudera Manager database and Agent configurations ensures that you can restore the
cluster to its previous state in case of an upgrade failure. Running the pre-upgrade health checks
identifies potential issues that could cause the upgrade to fail, allowing you to resolve them proactively.
While updating OS packages is good practice, it is not a mandatory pre-upgrade task. Agents are
upgraded by Cloudera Manager and don't require manual intervention before upgrade. Services do not
need to be disabled as part of the upgrade process.
Question: 145
You are troubleshooting a slow-running MapReduce job. Cloudera Manager shows the YARN queue is
consistently at 100% utilization. Which of the following actions would be MOST effective in improving
job performance and overall cluster efficiency, assuming fair share scheduler is in use?
A. Increase the ‘yarn.scheduler.minimum-allocation-mb' and ‘yarn.scheduler.maximum-allocation-mb'
properties globally.
B. Create a new YARN queue with a higher guaranteed resource allocation and submit the job to that
queue. Also make sure queue placement is properly configured.
C. Reduce the number of mappers and reducers for the job to decrease resource contention.
D. Increase the ‘mapreduce.map.memory.mb’ and 'mapreduce.reduce.memory.mb' properties for the
job.
E. Disable preemption on the default queue to ensure running jobs are never killed.
Answer: B
Explanation:
Creating a new YARN queue with a higher guaranteed resource allocation ensures the slow-running job
gets the resources it needs without excessively impacting other jobs. Adjusting queue placement
improves efficiency. While other options might provide temporary relief, they don't address the core
issue of resource contention within a single over-utilized queue.
Question: 146
You need to configure Cloudera Manager to send email alerts when disk utilization on a specific
DataNode exceeds 85%. Which of the following steps accurately describes how to achieve this through
Cloudera Manager?
A. Navigate to the DataNode's configuration page, add a new custom alert rule, and set the threshold to
85% for disk utilization.
B. Create a new metric monitor in Cloudera Manager Monitoring, select the 'Disk Usage' metric for the
DataNode, configure the alert threshold to 85%, and configure the email notification settings.
C. Edit the DataNode's advanced configuration snippet (Safety Valve) for core-site.xml to include
parameters for disk usage monitoring and email alerts.
D. Go to Alerts Create Alert Select 'Host' as entity type Select 'Disk Used (%)' metric Configure threshold
to 85% Configure notification settings.
E. Install and configure a third-party monitoring tool, as Cloudera Manager does not support disk
utilization alerts.
Answer: D
Explanation:
Cloudera Manager provides a built-in alerting system. Creating an alert through Alerts -> Create Alert
and selecting the appropriate entity type (Host in this case), metric ('Disk Used (%)'), threshold (85%),
and notification settings is the standard way to configure such alerts. Other options are either incorrect
or less efficient.
Question: 147
You are tasked with enabling Kerberos authentication on your CDP cluster After initial setup, users
report issues accessing HDFS. When examining the DataNode logs, you see errors related to failed
authentication. Which of the following is the MOST likely cause of this issue?
A. The HDFS service principal is not properly configured in the 'krb5.conf file on the client machines.
B. The DataNode's keytab file does not contain the correct service principal for HDFS.
C. The users do not have valid Kerberos tickets (TGTs) or their tickets have expired.
D. The NameNode is not properly configured to use Kerberos.
E. The HDFS client is not configured to use secure mode (Kerberos).
Answer: B
Explanation:
If DataNode logs show authentication failures after enabling Kerberos, the most likely reason is that the
DataNode itself is unable to authenticate to the Kerberos KDC because its keytab file doesn't contain the
correct service principal. While other options could contribute to the problem, the DataNode's inability
to authenticate is the most direct cause in this scenario.
Question: 148
You are using Cloudera Manager to manage a CDP cluster. You notice that the Cloudera Manager Agent
on one of the DataNodes is frequently restarting. Examining the agent's logs reveals OutOfMemoryError
exceptions. How should you address this issue?
A. Increase the heap size allocated to the Cloudera Manager Agent on the affected DataNode via
Cloudera Manager's configuration settings.
B. Decrease the ‘ulimit’ for the Cloudera Manager Agent process to prevent it from consuming excessive
resources.
C. Disable the Cloudera Manager Agent on the affected DataNode to prevent further restarts.
D. Reinstall the DataNode service on the affected host using Cloudera Manager.
E. Decrease the frequency of health checks performed by the Cloudera Manager Agent.
Answer: A
Explanation:
OutOfMemoryError exceptions in the Cloudera Manager Agent logs indicate that the agent is running
out of memory. Increasing the heap size allocated to the agent allows it to handle its workload without
crashing. The configuration setting for the agent's heap size is usually found within Cloudera Manager's
agent configuration.
Question: 149
You need to roll back a configuration change made through Cloudera Manager that has negatively
impacted the performance of the Hive Metastore. What is the MOST efficient way to revert to the
previous working configuration?
A. Manually edit the 'hive-site.xmr file on all Hive Metastore hosts to revert the changes.
B. Restart the Hive Metastore service, which will automatically revert to the last known good
configuration.
C. Use Cloudera Manager's 'Deploy Client Configuration' feature to re-deploy the client configuration,
ovemriting the incorrect settings.
D. In Cloudera Manager, navigate to the Hive Metastore service, select 'Configuration Changes', identify
the change, and click 'Revert to Previous Configuration'.
E. Restore the entire cluster from a backup.
Answer: D
Explanation:
Cloudera Manager provides a built-in mechanism to revert configuration changes. Navigating to
'Configuration Changes', identifying the specific change, and selecting 'Revert to Previous Configuration'
is the most efficient and reliable way to roll back changes. This method leverages Cloudera Manager's
version control and ensures consistency across the cluster.
Question: 150
Which of the following statements are true regarding the use of Cloudera Manager API? (Select TWO)
A. The Cloudera Manager API can be used to automate cluster deployment and configuration tasks.
B. The Cloudera Manager API only supports the Python programming language.
C. The Cloudera Manager API allows for programmatic monitoring of cluster health and performance.
D. The Cloudera Manager API requires a separate license to be used.
E. The Cloudera Manager API can only be accessed from the Cloudera Manager server itself.
Answer: A,C
Explanation:
The Cloudera Manager API is a REST API that enables automation of cluster management tasks such as
deployment, configuration, and monitoring. It supports various programming languages and does not
require a separate license. Access is not limited to the Cloudera Manager server.
Question: 151
You have a critical Spark application that requires a specific version of a third-party library which
conflicts with the version included in the default Spark environment managed by Cloudera Manager.
How can you isolate the environment to the spark application?
A. Modify the global Spark configuration in Cloudera Manager to include the specific library version,
potentially impacting other Spark applications.
B. Create a separate Spark application directory on the cluster, manually install the required library
version, and configure the Spark application to use that directory.
C. Package the specific library version with the Spark application's JAR file (uber JAR) and ensure it's
loaded before the default Spark libraries.
D. Utilize Spark's 'spark.driver.extraClassPatm and ‘spark.executor.extraClassPatm configuration options
to specify the location of the desired library version, creating an isolated environment for this
application.
E. Force update the default spark installation using Cloudera Manager to the specific library version
required by the Spark application.
Answer: D
Explanation:
Using ‘spark.driver.extraClassPatW and ‘spark.executor.extraClassPatW allows you to specify additional
classpath entries for the driver and executors, effectively creating an isolated environment for the Spark
application without affecting other applications or the default Spark installation. Uber JARs can work,
but classpath management provides greater flexibility. Modifying global configs or forcing updates are
generally not recommended for a single application's needs.
Question: 152
You are monitoring a CDP cluster with Cloudera Manager and notice consistently high CPU utilization on
the NameNode. Which of the following could contribute to this issue? (Select TWO)
A. A large number of small files in HDFS.
B. An insufficient number of DataNodes in the cluster.
C. Frequent HDFS metadata operations (e.g., listing directories, getting file status).
D. High network latency between the NameNode and DataNodes.
E. Too many concurrent YARN applications running on the cluster.
Answer: A, C
Explanation:
A large number of small files in HDFS puts a strain on the NameNode because it needs to store metadata
for each file in memory. Frequent metadata operations also increase CPU utilization as the NameNode
constantly needs to process these requests. The number of DataNodes or network latency could affect
overall cluster performance, but they don't directly explain high CPU utilization on the NameNode itself.
YARN applications impact the ResourceManager more directly.
Question: 153
You have configured Cloudera Navigator integration with Cloudera Manager. However, you are not
seeing any audit events for HDFS operations in Cloudera Navigator. What are the potential reasons for
this?
A. The HDFS auditing feature is not enabled in Cloudera Manager.
B. The Cloudera Navigator Audit Server is not running.
C. The HDFS service is not configured to send audit events to the Cloudera Navigator Audit Server.
D. All of the above.
E. The NameNode's heap size is too small.
Answer: D
Explanation:
For HDFS audit events to be visible in Cloudera Navigator, several conditions must be met: HDFS
auditing must be enabled in Cloudera Manager, the Cloudera Navigator Audit Server must be running,
and the HDFS service must be configured to send audit events to the server. If any of these conditions
are not met, audit events will not be visible. The NameNode's heap size is not directly related to audit
events.
Question: 154
You are using the Cloudera Manager API to automate the deployment of a new service. You need to
ensure that the service is only deployed if all its dependencies are met. Which API endpoint would you
use to check the service's deployment readiness?
A. /api/v31/clusters/{clusterName}/services/{serviceName}/commands/deployClientConfig
B. /api/v31/clusters/{clusterName}/services/{serviceName}/commands/start
C. /api/v31/clusters/{clusterName}/services/{serviceName}/stage
D. /api/v31/clusters/{clusterName}/services/{serviceName}/commands/canStage
E. /api/v31/clusters/{clusterName}/services/{serviceName}/roles/{roleName}/commands/restart
Answer: D
Explanation:
The
endpoint is specifically designed to check whether a service is ready to be deployed (staged). It verifies
if all dependencies are met before proceeding with the deployment. The other endpoints are for
deploying client configurations, starting the service, staging without checking dependencies, or
restarting a role.
Question: 155
Assume you've enabled TLS encryption for all services in your CDP cluster through Cloudera Manager.
After some time, one of the custom applications interacting with HDFS begins failing with SSL handshake
errors. You suspect there's an issue with the application's truststore. Where can you find the correct CA
certificate to add to the application's truststore?
A. The CA certificate is automatically distributed to all client machines through the Cloudera Manager
Agent.
B. The CA certificate is located in the ‘krb5.conf file on the Cloudera Manager server.
C. You must manually generate a new CA certificate from the Cloudera Manager Admin Console.
D. The CA certificate used by Cloudera Manager is typically stored in a file within the Cloudera Manager
server's configuration directory (e.g., '/var/lib/cloudera-scm-server/cert’ The exact path depends on the
CM version and custom configuration.
E. The CA certificate is not required for client applications; only the server-side components need it.
Answer: D
Explanation:
When TLS is enabled, clients need to trust the server's certificate. This is done by adding the CA
certificate that signed the server's certificate to the client's truststore. Cloudera Manager generates a CA
certificate when TLS is enabled, and this certificate is typically stored on the Cloudera Manager server in
a specific directory. The exact location depends on the CM version and custom settings. You need to
retrieve this certificate and add it to the application's truststore.
Question: 156
You are trying to diagnose a persistent issue with a Hive query failing due to insufficient memory. You
need to identify the specific YARN container that is running the Hive query and its resource usage at the
time of failure. Which of the following steps would you take to accomplish this using Cloudera Manager
and available logs?
A. Examine the HiveServer2 logs for the query ID, then correlate the query ID with the YARN application
ID in the ResourceManager logs. Finally, use the YARN application ID to find the specific container and
its resource usage in the NodeManager logs on the node where the container ran.
B. Use Cloudera Manager's YARN Resource Manager UIto search for the Hive query by its SQL
statement, which will directly display the associated container and its resource consumption.
C. Check the Hive Metastore logs for the query ID and associated container ID. The Metastore logs will
directly provide resource usage information for each query.
D. Enable debug logging for the HiveServer2 service, rerun the query, and then analyze the debug logs to
find the container ID and resource usage details.
E. Use the 'yarn top' command on the ResourceManager to identify the container consuming the most
memory. This container is likely associated with the failing Hive query.
Answer: A
Explanation:
The most accurate approach involves tracing the Hive query through different logs. First, the
HiveServer2 logs provide the query ID. This ID can be correlated with the YARN application ID in the
ResourceManager logs. The YARN application ID allows you to pinpoint the specific container in the
NodeManager logs on the node where the container was executed. The NodeManager logs contain
detailed resource usage information for each container. While Cloudera Manager provides some
visibility into YARN applications, it does not directly link Hive queries to specific containers and their
resource usage with the same level of detail. Other options are either inaccurate or inefficient.
Question: 157
You're tasked with enabling Kerberos authentication for an existing CDP on-premises cluster using
Cloudera Manager. After running the Kerberization wizard and restarting the cluster, some jobs are
failing with 'GSS initiate failed' errors. What are the MOST likely causes?
A. Incorrectly configured DNS resolution for Kerberos principals.
B. Missing or incorrect entries in the '/etc/krb5.conf file on client machines.
C. Keytab files are not properly distributed or have incorrect permissions.
D. Clock skew between the KDC and cluster nodes exceeding the Kerberos tolerance.
E. The Cloudera Manager Agent is unable to communicate with the KDC.
Answer: A,B,C,D
Explanation:
Kerberos authentication issues frequently arise from DNS problems (A), incorrect krb5.conf (B), keytab
issues (C), and clock skew (D). While (E) is possible, it's less likely if the Kerberization wizard completed
successfully. Addressing these areas helps to resolve the 'GSS initiate failed' errors.
Question: 158
A user reports that a YARN application consistently fails with a 'java.lang.OutOfMemoryError: Java heap
space' error. As a CDP administrator, what configuration setting(s) within Cloudera Manager can you
adjust to address this issue for ALL applications, while minimizing impact to other applications?
A. Increase the ‘yarn.scheduler.maximum-allocation-mb' property for the YARN service.
B. Increase the ‘yarn.nodemanager.resource.memory-mb' property for the YARN service.
C. Modify the ‘mapreduce.map.java.opts' and 'mapreduce.reduce.java.opts’ properties within the YARN
service configuration.
D. Adjust the JVM heap size C-Xmk) within the ApplicationMaster's launch context via
‘yarn.app.mapreduce.am.resource.mb' and the corresponding yarn.app.mapreduce.am.command-optS.
E. Change 'yarn.nodemanager.vmem-check-enabled' to false
Answer: D
Explanation:
Adjusting the ApplicationMaster's memory allocation (D) allows you to control the heap size for
MapReduce applications without impacting other application types or the overall YARN capacity.
Modifying ‘mapreduce.map.java.opts and 'mapreduce.reduce.java.opts' would impact all MapReduce
jobs, which might be too broad. 'yarn.nodemanager.resource.memory-mb' and
'yarn.scheduler.maximum-allocation-mb' control total resources available but doesn't directly address
individual AM heap size. Disabling Virtual Memory check is not a solution for OOM errors.
Question: 159
You're using Cloudera Manager to configure a new Kafka topic. You want to ensure high availability and
durability for the messages published to this topic. Which of the following configuration options are
MOST important to consider?
A. Setting ‘default.replication.factor’ to at least 3.
B. Setting ‘min.insync.replicas’ to be greater than 1.
C. Setting ‘num.partitions’ to the number of brokers in the cluster.
D. Setting ‘auto.create.topics.enable' to 'false'.
E. Enabling auto topic creation using ‘auto.create.topics.enable=true'
Answer: A,B
Explanation:
The ‘default.replication.factor’ (A) determines how many copies of each message are stored, providing
fault tolerance. The 'min.insync.replicas' (B) setting ensures that a minimum number of replicas have
acknowledged a write before it's considered successful, preventing data loss. While increasing
‘num.partitionS (C) can improve throughput, it's not directly related to HAIdurability. Disabling topic
auto- creation (D) is more about security and control than HAIdurability and enabling topic auto-
creation is about management, not HA or durability.
Question: 160
You're troubleshooting a slow-running Hive query in your CDP on-premises cluster. You suspect the
issue is related to insufficient resources allocated to the HiveServer2 service. Using Cloudera Manager,
which metric(s) would provide the MOST direct indication of resource contention within the HiveServer2
process?
A. CPU Usage (%) and Memory Usage (GB) of the HiveServer2 host.
B. HiveServer2's 'JVM Heap Usage' and 'Garbage Collection Time' metrics.
C. Network 1/0 on the HiveServer2 host.
D. HDFS Read and Write throughput.
E. The total number of active Hive queries.
Answer: B
Explanation:
JVM Heap Usage and Garbage Collection Time (B) directly reflect the resource consumption and
efficiency within the HiveServer2 process. High JVM heap usage coupled with long garbage collection
times strongly suggests memory pressure. Host CPU and memory usage (A) provide a general overview,
but not specific to the HiveServer2 process. Network I/O (C) and HDFS throughput (D) are relevant to
data access but not necessarily indicative of resource contention within HiveServer2 itself. Number of
active queries shows load but not performance inside the process.
Question: 161
After upgrading your CDP on-premises cluster, you notice that the DataNode service is repeatedly failing
to start on one of the nodes. The Cloudera Manager agent is running, and the DataNode logs show the
following error: ‘java.lang.lllegalArgumentException: Invalid configuration value detected:
dfs.datanode.failed.volumes.tolerated'. What is the MOST likely cause and how would you resolve it
using Cloudera Manager?
A. The 'dfs.datanode.failed.volumes.tolerated' property is set to a percentage value, which is no longer
supported. Update the property to an absolute number using Cloudera Manager's HDFS service
configuration.
B. The DataNode is unable to connect to the NameNode. Verify NameNode HA configuration and
network connectivity using Cloudera Manager.
C. The DataNode's disk space is full. Use Cloudera Manager to check disk utilization and free up space.
D. There's a mismatch in the HDFS version between the DataNode and NameNode. Upgrade the
DataNode using Cloudera Manager's rolling restart functionality.
E. The DataNode user doesn't have permission to access HDFS directories. Grant necessary access rights
using Cloudera Manager.
Answer: A
Explanation:
The error message indicates that the 'dfs.datanode.failed.volumes.tolerated' property has an invalid
value. In newer HDFS versions, percentage values for this property are deprecated, requiring an
absolute number representing the maximum number of failed volumes tolerated. You would correct this
by changing the property value in Cloudera Manager (A).
Question: 162
You are managing a CDP cluster with Spark jobs failing due to resource contention. Which of the
following configuration changes in Cloudera Manager would be MOST effective in isolating Spark
workloads and ensuring they get sufficient resources?
A. Increase the ‘yarn.nodemanager.resource.memory-mb' property for the YARN service to allocate
more memory to each NodeManager.
B. Create a dedicated YARN queue specifically for Spark jobs and configure resource limits for that
queue in the YARN service configuration.
C. Increase the ‘spark.driver.memory’ and ‘spark.executor.memory’ properties in the Spark service
configuration.
D. Enable preemption for the default YARN queue to allow Spark jobs to take resources from other
applications.
E. Reduce the 'dfs.block.size' property in HDFS.
Answer: B
Explanation:
Creating a dedicated YARN queue (B) provides the most effective isolation by allowing you to control the
resources allocated specifically to Spark jobs. This ensures that Spark jobs have a guaranteed level of
resources regardless of other workloads. Increasing yarn.nodemanager.resource.memory-mb' (A) might
help but doesn't guarantee isolation. Adjusting Spark memory properties (C) affects individual job
resource usage, not overall cluster resource allocation. Enabling preemption can cause instability. (D)
has not guaranteed isolation.
Question: 163
You want to configure automatic failover for the NameNode in your HA-enabled HDFS cluster. Using
Cloudera Manager, you have configured a ZooKeeper quorum and verified its health. However,
automatic failover is not working as expected. Which of the following configuration steps are essential
to ensure automatic failover functions correctly?
A. Ensure that the 'dfs.ha.automatic-failover.enabled' property is set to 'true' in the HDFS service
configuration.
B. Configure the 'dfs.nameserviceS and 'dfs.ha.namenodes.[nameservice ID]' properties correctly,
specifying the NameNode logical names and IDs.
C. Deploy and configure the 'ZKFailoverController’ (ZKFC) process on each NameNode host using
Cloudera Manager.
D. Ensure that all DataNodes are configured to use the NameNode logical names defined in
Sdfs.nameservicess.
E. Manually start the secondary NameNode after failover.
Answer: A,B,C,D
Explanation:
Automatic failover requires enabling the feature Cdfs.ha.automatic-failover.enabled=true’ - A), correct
NameNode logical name configuration Cdfs.nameserviceS and 'dfs.ha.namenodes' - B), deploying and
configuring ZKFC (C), and ensuring all DataNodes use the logical names (D). Option (E) contradicts the
automatic nature of the failover; no manual intervention is needed in automatic failover, making it the
incorrect answer.
Question: 164
You are trying to apply a custom configuration to a specific role group (e.g., 'datanode_base') in
Cloudera Manager using the API. You want to update the property 'dfs.datanode.max.locked.memory'
to '4294967296'. Which of the following curl commands is the MOST appropriate to achieve this?
Assume you have a valid authentication token.
A. Option A
B. Option B
C. Option C
D. Option D
E. Option E
Answer: A
Explanation:
The correct command is (A). Cloudera Manager's API requires a PUT request to update configurations.
The data should be a JSON array with 'items' containing the 'name' and 'value' of the property. Other
options are syntactically incorrect or use the wrong HTTP method.
Question: 165
You've noticed that the Cloudera Manager Server is consuming a high amount of CPU resources. After
investigation, you determine that the Cloudera Manager database (using PostgreSQL) is experiencing
performance issues. What are the MOST effective steps you can take to improve the performance of the
Cloudera Manager database directly ?
A. Increase the memory allocated to the Cloudera Manager Server process.
B. Analyze the PostgreSQL query logs and identify slow-running queries for optimization.
C. Run the 'ANALYZE command on the Cloudera Manager database to update statistics used by the
query planner.
D. Increase the number of Cloudera Manager Agents reporting to the server.
E. Tune PostgreSQL configuration parameters such as and ‘work_mem' based on the available system
resources.
Answer: B,C,E
Explanation:
The most direct ways to improve the database performance are analyzing and optimizing slow queries
(B), updating table statistics (C), and tuning PostgreSQL configuration parameters (E). Increasing CM
server memory (A) might indirectly help, but it doesn't directly address database performance.
Increasing the number of agents (D) would likely worsen the issue.
Question: 166
You are auditing the configuration of your CDP cluster and need to verify that all services are using TLS
encryption for inter-service communication. How can you MOST efficiently achieve this verification
using Cloudera Manager?
A. Manually inspect the configuration files of each service on each host.
B. Use the Cloudera Manager API to retrieve the configuration for each service and check for the
presence of TLS-related properties.
C. Run a custom script on each host to check for the presence of TLS-related configuration files.
D. Use Cloudera Manager's built-in search functionality to search for TLS-related configuration
properties across all services.
E. Check Cloudera Manager's audit logs for TLS configuration changes.
Answer: D
Explanation:
Cloudera Manager's search functionality (D) provides the most efficient way to search for specific
configuration properties (like those related to TLS) across all services. Manually inspecting files (A) is
time-consuming and error-prone. Using the API (B) requires scripting and is less efficient than using the
built-in search. A custom script (C) has similar drawbacks as A. Audit logs (E) only show changes, not the
current state.
Question: 167
You need to implement a configuration change to the Hive Metastore service that requires a rolling
restart. After applying the configuration change in Cloudera Manager, the rolling restart process fails
with the error 'Timeout waiting for Hive Metastore to become healthy'. What are the MOST likely causes
for this issue?
A. The Hive Metastore database connection is failing due to incorrect credentials or network issues.
B. The Hive Metastore service is experiencing high load and cannot start within the configured timeout
period.
C. The Hive Metastore is configured to use an external database, and the database server is unavailable.
D. The Hive Metastore's heap size is insufficient, causing it to crash during startup.
E. The rolling restart timeout is configured too short.
Answer: A,B,C,D
Explanation:
A timeout waiting for the Hive Metastore to become healthy can stem from several issues during
startup. Database connectivity problems (A and C), high load (B), and insufficient heap (D) are all
potential reasons why the Metastore might fail to start within the timeout. While increasing the timeout
(E) might help, it doesn't address the underlying cause, so the other options are more likely root causes.
Question: 168
Your organization has a requirement to rotate the encryption keys used by the HDFS encryption zones
on a regular basis. What steps must you take to ensure that all data is re-encrypted with the new key
and that no data loss occurs during the key rotation process?
A. Use the ‘hdfs crypto key -rotate’ command to rotate the key. HDFS automatically handles the re-
encryption of all data.
B. Use the ‘hdfs crypto key -rotate' command to rotate the key, then run the ‘hdfs dfs -reencrypt’
command on the encryption zone to re-encrypt all data.
C. Create a new encryption zone with the new key, copy all data to the new zone, and then delete the
old encryption zone.
D. Disable the encryption zone, rotate the key, and then re-enable the encryption zone.
E. Rotate the key in the KMS, and all new writes will use the new key. Old data will remain encrypted
with the old key, which is acceptable.
Answer: B
Explanation:
The correct process (B) involves rotating the key using 'hdfs crypto key -rotates and then explicitly re-
encrypting the data within the encryption zone using 'hdfs dfs -reencrypt. Option (A) is incorrect
because 'hdfs crypto key -rotate' does not automatically re-encrypt data. Options (C) and (D) are
disruptive and unnecessary. Option (E) might be acceptable in some scenarios but doesn't meet the
requirement to re-encrypt all data.
Question: 169
You are troubleshooting a slow-running MapReduce job on your Cloudera Data Platform (CDP) on-
premises cluster. After examining the logs, you notice excessive garbage collection (GC) pauses in the
TaskTracker logs. Which of the following actions is MOST likely to improve the job's performance?
A. Increase the 'mapreduce.map.java.opts' and 'mapreduce.reduce.java.opts’ parameters to allocate
more heap memory to the TaskTracker processes.
B. Reduce the number of concurrent tasks that each TaskTracker can run by adjusting the
‘mapreduce.tasktracker.map.tasks.maximum’ and mapreduce.tasktracker.reduce.tasks.maximum’
parameters.
C. Switch the garbage collector algorithm used by the TaskTrackers to a concurrent collector like Gl GC
by setting the appropriate JVM options.
D. Increase the block size in HDFS to reduce the number of disk 1/0 operations performed by the
MapReduce tasks.
E. All of the above are potentially valid actions.
Answer: E
Explanation:
Excessive GC pauses indicate the JVM is spending too much time reclaiming memory, impacting
performance. Increasing heap size (A), reducing concurrent tasks (B), and switching to a more efficient
GC algorithm (C) can all alleviate this. Increasing HDFS block size (D) helps with I/O but is less directly
related to GC pauses.
Question: 170
A user reports that their Hive query is failing with a 'java.lang.OutOfMemoryError: Java heap space'
error. You suspect the error is occurring during the execution of a UDF. Where would you MOST likely
find the relevant error details to confirm this?
A. The HiveServer2 log file C/var/log/hive/hive-server2.loØ)
B. The Hadoop ResourceManager log file C/var/log/hadoop-yarn/yarn-rm/’) on the ResourceManager
node.
C. The Hive Metastore log file C/var/log/hive/hive-metastore.log')
D. The YARN container logs associated with the task executing the IJDF. These can be found via the YARN
ResourceManager UIor using the ‘yarn logs command.
E. The operating system's syslog ('Ivar/log/syslog’ or ‘ Ivar/log/messageS)
Answer: D
Explanation:
The ‘java.lang.OutOfMemoryError' during IJDF execution occurs within the YARN container running the
task. The container logs, accessible via the YARN ResourceManager UIor the ‘yarn logs' command,
contain the stack trace and details needed to diagnose the issue. HiveServer2 logs (A) are useful for
general Hive errors, ResourceManager logs (B) for cluster resource issues, and Metastore logs (C) for
Metastore-related problems.
Question: 171
You are investigating an issue where HBase regions are frequently splitting. Which configuration
property should you examine FIRST to determine if the splits are due to exceeding the maximum region
size?
A. 'hbase.hregion.max.filesize'
B. 'hbase.hregion.memstore.flush.size’
C. 'hbase.regionserver.global.memstore.upperLimit
D. ‘hbase.hregion.majorcompaction.period'
E. 'hbase.hregion.split.policy'
Answer: A
Explanation:
'hbase.hregion.max.filesize’ directly controls the maximum size a region can grow to before it is split.
'hbase.hregion.memstore.flush.size’ (B) controls when the memstore is flushed to disk.
'hbase.regionserver.global.memstore.upperLimit’ (C) controls the total memstore usage.
'hbase.hregion.majorcompaction.period' (D) controls the frequency of major compactions.
'hbase.hregion.split.policy’ (E) dictates the split policy not the maximum file size.
Question: 172
A Data Engineer reports that their Spark application is consistently failing with ExecutorLostFailure
exceptions. You suspect network connectivity issues between the driver and the executors. Which log
files would you analyze to confirm this hypothesis?
A. The Spark driver log and the Spark executor logs on the nodes where the executors were running.
B. The YARN ResourceManager logs on the ResourceManager node.
C. The HDFS NameNode logs.
D. The Oozie server logs.
E. The Cloudera Manager agent logs on the host running the Spark Driver.
Answer: A
Explanation:
ExecutorLostFailure exceptions indicate that the driver cannot communicate with the executors.
Analyzing the Spark driver log and the executor logs will reveal connection errors, timeouts, or other
network-related problems. The YARN ResourceManager logs (B) provide information about resource
allocation, but not necessarily network connectivity. HDFS logs (C) are irrelevant for network
connectivity between Spark components. Oozie logs (D) are for workflow orchestration, and Cloudera
Manager agent logs (E) primarily deal with management tasks.
Question: 173
You notice that the HDFS DataNodes are frequently logging 'DiskOutOfSpaceException' errors, but the
HDFS capacity utilization reported by Cloudera Manager is only at 80%. What is the MOST likely cause of
this discrepancy?
A. The DataNodes are configured with multiple data directories, and one or more of those directories
are full.
B. The HDFS NameNode is reporting incorrect capacity utilization statistics.
C. The DataNodes have not been restarted recently, and the disk space information is stale.
D. The HDFS replication factor is set too high, consuming excessive disk space.
E. HDFS Quotas are restricting the space usable by specific users or directories even though the overall
disk usage is low.
Answer: A
Explanation:
DataNodes often have multiple data directories. If one directory fills up, it triggers the
‘DiskOutOfSpaceException’ even if other directories have free space. Cloudera Manager reports overall
HDFS capacity, so it doesn't reflect the individual directory issue. NameNode reporting inaccuracies (B)
are less likely. Restarting DataNodes (C) won't solve a disk full issue. High replication (D) could
exacerbate the issue but isn't the primary cause. HDFS Quotas (E) could also be the issue restricting
users or directories.
Question: 174
A Kerberized Hive query fails with the error 'javax.security.auth.login.LoginException: Cannot get key for
principal hive/your.hive.server@YOUR.REALM from keytab /etc/security/keytabs/hive.service.keytab'.
Which of the following steps should you take to resolve this issue? (Select two)
A. Ensure that the keytab file '/etc/security/keytabs/hive.service.keytab' exists and is readable by the
Hive user.
B. Verify that the Hive principal is present in the keytab file using the ‘klist -kt
/etc/security/keytabs/hive.service.keytab’command.
C. Restart the Kerberos Key Distribution Center (KDC).
D. Update the Hive configuration property 'hive.security.authorization.enabled' to ‘false’ .
E. Ensure the system time is synchronized across all cluster nodes using N TP.
Answer: A,B
Explanation:
The error indicates a problem with the Hive service's keytab file. A) confirms the keytab exists and is
accessible. B) verifies the correct principal is present in the keytab. Restarting the KDC (C) is unnecessary
unless there's a KDC issue. Disabling authorization (D) bypasses Kerberos but isn't a solution. Time
synchronization (E) is important for Kerberos, but the error message points directly to a keytab problem.
Question: 175
You are tasked with troubleshooting a Flume agent that is experiencing significant data loss. The agent is
configured with a memory channel, a spooldir source, and an HDFS sink. What configuration change
would MOST directly address the data loss issue?
A. Switch the memory channel to a file channel.
B. Increase the capacity of the memory channel.
C. Enable transaction guarantees on the HDFS sink.
D. Increase the batch size of the spooldir source.
E. Implement a custom interceptor to filter out invalid events.
Answer: A
Explanation:
Memory channels are volatile; data is lost if the agent crashes. Switching to a file channel provides
persistence and prevents data loss in case of agent failure. Increasing memory channel capacity (B)
might delay data loss, but doesn't prevent it. Transaction guarantees on the HDFS sink (C) ensure data is
written to HDFS atomically, but don't address loss in the channel. Source batch size (D) affects
throughput, not data loss. An interceptor (E) filters data, it does not prevent loss.
Question: 176
A user reports that their Impala query is consistently timing out. After examining the Impala logs, you
see numerous entries indicating 'Metadata load failure'. What is the MOST likely cause of this issue?
A. The Impala catalog server is overloaded and unable to serve metadata requests.
B. The HDFS NameNode is experiencing high latency, preventing Impala from accessing metadata.
C. The Hive Metastore is unavailable or returning errors.
D. There are insufficient resources (CPU, memory) allocated to the Impala executors.
E. The query is too complex and requires optimization.
Answer: C
Explanation:
Impala relies on the Hive Metastore for metadata about tables and partitions. 'Metadata load failure'
strongly suggests a problem with the Metastore being unavailable or returning errors. While NameNode
latency (B) can impact performance, the error message is more specific. Catalog server overload (A) is
possible, but Metastore issues are more common. Insufficient executor resources (D) would likely lead
to different error messages. Query complexity (E) might cause timeouts, but the 'Metadata load failure'
is a more direct indicator of a Metastore problem.
Question: 177
You are observing high CPU utilization on your ZooKeeper nodes. Which of the following actions is MOST
likely to reduce the CPU load on the ZooKeeper ensemble?
A. Increase the heap size allocated to the ZooKeeper processes.
B. Reduce the number of clients connecting to the ZooKeeper ensemble.
C. Increase the tick time for ZooKeeper sessions.
D. Enable authentication for ZooKeeper clients.
E. Move all ZooKeeper data to an SSD for faster access.
Answer: B
Explanation:
ZooKeeper's CPU usage is primarily driven by the number of client requests it needs to handle. Reducing
the number of clients (B) directly reduces the processing load. Increasing heap size (A) might help with
memory management but doesn't directly reduce CPU load. Increasing the tick time (C) affects session
timeouts, not CPU usage. Authentication (D) adds to the CPU load. Faster storage (E) could improve
performance but doesn't fundamentally address high CPU usage caused by excessive client requests.
Question: 178
A newly deployed Spark application is failing with a 'java.lang.ClassNotFoundException'. You've verified
that the application's JAR file contains the missing class. What are the two MOST likely causes of this
issue in a YARN cluster environment? (Select two)
A. The application's JAR file is not being distributed to the YARN containers running the Spark executors.
B. The Spark application is not specifying the correct class path for the JAR file.
C. The Hadoop classpath is corrupted on the client machine submitting the job.
D. The Kerberos ticket for the user submitting the job has expired.
E. The Spark driver program is using an incompatible version of Java.
Answer: A,B
Explanation:
ClassNotFoundException indicates the JVM cannot find the class at runtime. A) directly addresses the
issue of the JAR not being available in the container. B) refers to the spark application jar/packages are
not explicitly specified in the Spark configuration. If the container does not know where to find the
dependency, it will throw a class not found exception. Corrupted Hadoop classpath (C) is possible but
less likely, as it would affect other Hadoop components. Kerberos ticket expiration (D) would cause
authentication failures, not ClassNotFoundException. Incompatible Java version (E) could cause other
issues but is less likely if the class is present in the JAR.
Question: 179
You're managing a Cloudera CDP cluster where several services are experiencing intermittent
connection issues to the external database used for metadata storage. Analyzing the logs, you see
recurring errors related to database connection timeouts. Identify the TWO most appropriate strategies
to mitigate this issue. (Select two)
A. Increase the connection timeout values for the services connecting to the external database.
B. Implement connection pooling on the services to reuse existing database connections.
C. Migrate the metadata to the embedded database provided by Cloudera CDR
D. Decrease the number of concurrent users accessing the services.
E. Disable auditing across all Cloudera CDP services.
Answer: A,B
Explanation:
Connection timeouts directly indicate the connection is failing after a specific period. Increasing the
timeout (A) gives the connection more time to succeed. Connection pooling (B) reduces the overhead of
establishing new connections, making the database more efficient.
Migrating to embedded database (C) is a major architectural change, while it will resolve external
connection issue, it has its own drawback such as limited scalability. Decreasing concurrent users (D)
might alleviate load but doesn't address the underlying connection problem. Disabling auditing (E) is
unrelated to database connection issues.
Question: 180
You are monitoring a Cloudera Data Platform on-premises cluster. You observe that the HDFS 'Available'
disk space is consistently low, and the 'Under-replicated Blocks' metric is increasing despite the cluster
not experiencing any node failures. After investigation, you discover a large number of recently deleted
files are still occupying space in HDFS. What TWO actions should you take to remediate this situation?
(Select two)
A. Run the HDFS balancer to redistribute blocks across DataNodes.
B. Execute the 'hdfs dfs -expunge' command to remove the deleted files from the trash directories.
C. Increase the replication factor for critical HDFS files.
D. Reduce the 'fs.trash.interval' configuration property to shorten the trash retention period.
E. Decommission underutilized DataNodes to consolidate data on fewer nodes.
Answer: B,D
Explanation:
Deleted files in HDFS are moved to the trash and are retained for a configured period Cfs.trash.interval')
before being permanently deleted. B) Immediately running ‘hdfs dfs -expunge' removes those files and
frees up space. D) Reducing 'fs.trash.interval' ensures that files are automatically purged from the trash
more frequently. Running the balancer (A) redistributes existing data, not deleted data. Increasing
replication (C) would consume more space. Decommissioning nodes (E) might be a valid long-term
strategy but doesn't address the immediate issue of trash occupying space.
Question: 181
You need to retrieve the current health checks for all the hosts in your Cloudera Data Platform (CDP)
cluster using the Cloudera Manager REST API. Which of the following REST API endpoints would you
use?
A. /api/v32/hosts/{hostld}/healthChecks
B. /api/v32/clusters/{clusterName}/hosts/healthChecks
C. /api/v32/hosts/healthChecks
D. /api/v32/clusters/{clusterName}/hosts/{hostld}/healthChecks
E. /api/v32/clusters/{clusterName}/hosts/healthSummary
Answer: A
Explanation:
The correct endpoint is 7api/v32/hosts/{hostld}/healthChecks’. This allows you to target a specific host
by its ID and retrieve its health checks. While provides a summary, it doesn't give detailed health check
information for each host.
Question: 182
You want to automate the deployment of a new configuration file to all DataNodes in your Hadoop
cluster using the Cloudera Manager
REST API. What is the correct sequence of API calls you need to execute? Assume you have the correct
cluster name, service name, and config file content.
A. 1. POST to /api/v32/clusters/{clusterName}/services/{serviceName}/config 2. POST to
/api/v32/clusters/{clusterName}/commands/deployClientConfig
B. 1. PUT to /api/v32/clusters/{clusterName}/services/{serviceName}/config 2. POST to
/api/v32/clusters/{clusterName}/commands/refreshServiceConfigs
C. 1. PUT to /api/v32/clusters/{clusterName}/services/{serviceName}/config 2. POST to
/api/v32/clusters/{clusterName}/commands/deployClientConfig
D. 1. POST to /api/v32/clusters/{clusterName)/services/{serviceName}/config 2. POST to
/api/v32/clusters/{clusterName}/commands/restart
E. 1. PUT to /api/v32/clusters/{clusterName}/services/{serviceName}/config 2. POST to
/api/v32/clusters/{clusterName}/commands/restartService
Answer: C
Explanation:
The correct sequence is to first use ‘PUT' to update the service configuration. 'POST ' is typically used for
creating new resources. Then, use "POST with 'Icommands/deployClientConfig' to deploy the updated
configuration to the client nodes. Note that for core configuration changes, a service restart may be
required instead of deployClientConfig, but deploying the client config is the appropriate immediate first
action in many configuration change scenarios.
Question: 183
You are writing a Python script to interact with the Cloudera Manager REST API to start a specific role in
your cluster. The role name is 'datanode-01.example.com'. Which of the following code snippets is the
most appropriate to achieve this?
A.
B.
C.
D.
E.
Answer: C
Explanation:
The correct URL structure to start a role through the Cloudera Manager API is
You need to specify the cluster name and the service name to which the role belongs.
Question: 184
You want to monitor the CPU usage of a specific host in your Cloudera CDP cluster using the Cloudera
Manager REST API. Which of the following API endpoints can be used to retrieve this metric?
A. /api/v32/metrics?query=cpu_percent
B. /api/v32/timeseries?query=cpu_percent&from=now-1h
C. /api/v32/hosts/{hostld}/metrics?query=cpu_percent
D. /api/v32/clusters/{clusterName}/hosts/{hostld}/metrics?query=cpu_percent
E. /api/v32/timeseries?query=select cpu_percent where category = HOST and entityName = {hostld}
Answer: E
Explanation:
The '/api/v32/timeserieS endpoint is the appropriate endpoint to retrieve time-series data, including
CPU usage. The query parameter format uses a selection syntax (select where category = and
entityName = ). You need to specify the category (HOST) and the entityName (hostld) to target the
specific host.
Question: 185
You are using the Cloudera Manager API to create a new Hive service within an existing cluster named
'MyCluster'. Which of the following JSON payloads is the most correct and complete (assuming a valid
API version and authentication) for creating the Hive service with default settings, including necessary
dependencies?
A.
B.
C.
D.
E.
Answer: E
Explanation:
Option E is the most comprehensive and accurate. While option A provides basic information, it lacks
the cluster association and dependency on HDFS, which is crucial for Hive to function. Options B and C
have syntax or missing element issues. Option D lists role types which may need to be created
separately and are not required in initial service creation. Option E explicitly specifies the HDFS
dependency, ensuring that Cloudera Manager can properly configure the Hive service. The cluster name
is not passed in the body, it's passed as part of URL
Question: 186
You're trying to configure a new property, 'my.custom.property' , for the Hive Metastore service via the
Cloudera Manager API. You want this property to be reflected in the 'hive-site.xmr configuration file.
However, after applying the configuration and restarting the service, the property is not present in the
file. What are the potential reasons and solutions? (Select all that apply)
A. The property was set using the wrong API endpoint. Configuration properties should be set via .
B. The property name is incorrect or contains invalid characters. Ensure the property name follows
standard naming conventions.
C. The service was not restarted after applying the configuration changes. A restart is always required
for new properties to take effect.
D. The property may not be recognized by the Hive Metastore service. Check the Hive documentation
for supported properties.
E. The property has incorrect data type. API doesn't validate, therefore Hive will fail to pick up the
setting.
Answer: A,B,D
Explanation:
Several reasons can cause the configuration to fail: A) The correct endpoint is. B) Incorrect property
names or invalid characters prevent the service from recognizing the property. D) The Hive Metastore
service might not recognize the property if it's not a supported configuration option. While restarting
the service is often necessary, it won't help if the property is invalid or not supported.E) API doesn't
validate, therefore Hive will fail to pick up the setting but after restart and therefore not valid answer.
Question: 187
You're managing a Cloudera Data Platform (CDP) cluster and want to use the Cloudera Manager REST
API to retrieve a list of all active YARN applications along with their resource usage (CPU, memory). How
can you achieve this using the API?
A. Option A
B. Option B
C. Option C
D. Option D
E. Option E
Answer: D
Explanation:
The correct approach is to use to get a list of applications associated with the YARN service in the
specified cluster, and filter the results to obtain only active applications. While other options may
provide some information, they are not designed specifically to provide both the list of applications and
their resource usage directly.
Question: 188
You've updated the Java heap size for the Hive Metastore service using the Cloudera Manager API. After
restarting the service, you observe that the Metastore service is crashing repeatedly with
OutOfMemoryErrors. What are the most likely reasons, assuming the new heap size is larger than the
previous one?
A. The new heap size is too large for the available physical memory on the host. The OS is unable to
allocate the requested memory, leading to crashes.
B. The garbage collection settings are not optimized for the new heap size, causing frequent full GC
pauses and eventual 00M errors.
C. The ‘ulimit’ settings for the Hive Metastore user are too restrictive, preventing the service from using
the configured heap size.
D. The ‘hive-site.xmr is corrupted.
E. The initial heap size C-XmS) and maximum heap size ('-Xmx') are set to different values which can lead
to JVM instabilities. It needs to be same to have consistant allocation.
Answer: A,B,C
Explanation:
A) If the requested heap size exceeds the available physical memory, the OS will be unable to allocate
the memory, leading to crashes. B) If garbage collection is not tuned for larger heaps, it can lead to
frequent pauses and 00M errors despite the increased memory. C) ulimit’ settings can restrict the
maximum memory a process can use, even if the JVM is configured with a larger heap size.D) Although
‘hive- site.xmr corruption could be a reason, is not directly related to 00M and increased Heap Size. E)
Initial and Maximum heap size can be different, it's about dynamically alloacting memory, it's not a
direct cause of OOM.
Question: 189
You are using the Cloudera Manager REST API to retrieve the configuration history for the HDFS service.
You need to filter the results to only show changes made by a specific user, 'admin', within the last 24
hours. How can you achieve this?
A. Option A
B. Option B
C. Option C
D. Option D
E. Option E
Answer: B
Explanation:
The endpoint does not support filtering by user or time range directly using query parameters. You need
to retrieve all the configuration history and then filter the results programmatically in your application
code.
Question: 190
You are programmatically deploying a new Cloudera DataFlow (CDF) service using the Cloudera Manager
REST API. After submitting the creation request, the API returns a success code (200 0K), but the service
fails to start. Upon inspecting the Cloudera Manager IJI, you see an error indicating a missing license key.
How can you use the REST API to upload a valid license key to resolve this issue?
A. Use a POST request to '/api/v32/license’ with the license key in the request body.
B. Use a PUT request to with the license key as a file attachment.
C. Use a POST request to '/api/v32/cm/config’ to update 'license_key’ property to the key value.
D. Use a POST request to '/api/v32/license’ endpoint with a multipart/form-data content type to upload
the license file.
E. License keys cannot be uploaded programmatically via the Cloudera Manager REST API and must be
uploaded through the IJI.
Answer: D
Explanation:
The correct way to upload a license key using the Cloudera Manager REST API is to use a POST request
to the S/api/v32/license’ endpoint with a multipart/form-data content type to upload the license file.
This allows you to provide the license key as a file attachment within the request.
Question: 191
You are using the Cloudera Manager REST API to perform a rolling restart of the HDFS service. You want
to monitor the progress of the restart operation in real-time. Which of the following approaches is the
most efficient and reliable way to track the restart progress?
A. Poll the endpoint repeatedly to check the service's overall health and status.
B. Poll the '/api/v32/commands/{commandldY endpoint repeatedly to check the command's status. This
endpoint will provide details on the progress and any errors encountered.
C. Use the '/api/v32/eventS endpoint and filter for events related to the HDFS service restart.
D. Use the endpoint and filter by HDFS service and restart operation to see the operation events.
E. Implement a WebSocket connection to receive real-time updates on the command's progress. This
requires a custom client that supports WebSocket communication.
Answer: B
Explanation:
The most efficient and reliable way is B) to poll the '/api/v32/commands/{commandldY endpoint. This
endpoint provides detailed information about the command's progress, including its current state (e.g.,
RUNNING, SUCCESS, FAILED), start and end times, and any errors encountered. While A) can give a
general service health, it doesn't provide granular restart progress. C) is event based so may not be real
time. D) is less specific on the actual command progress. E) provides real-time progress is the most
complex and not always needed.
Question: 192
You're tasked with automating the creation of a new Cloudera Manager user with specific roles and
permissions using the REST API. You need to create a user 'data_analyst' with 'Read-Only' access to the
'MyCluster' cluster and 'Operator' access to the 'Hive' service. Which of the following API call(s) and
body structures would accomplish this? Note that a single user creation might require multiple API calls
to set all the desired permissions.
A. Option A
B. Option B
C. Option C
D. Option D
E. Option E
Answer: B
Explanation:
Question: 193
You are configuring Auto-TLS for a CDP on-premise cluster. After enabling Auto-TLS, you notice some
services are failing to start, and the Cloudera Manager UIreports certificate-related errors. What is the
MOST likely cause of this issue?
A. The DNS configuration is incorrect, and services cannot resolve each other's hostnames.
B. The Cloudera Manager Agent on some nodes is not running with root privileges.
C. The Java Cryptography Extension (JCE) Unlimited Strength Jurisdiction Policy Files are not installed on
all nodes.
D. The property in the Cloudera Manager database is incorrect.
E. The certificate authority (CA) certificate used by Auto-TLS has expired.
Answer: A
Explanation:
Incorrect DNS configuration is a common cause of Auto-TLS failures. Services rely on DNS to resolve
hostnames and validate certificates. If DNS resolution is failing, certificate validation will also fail,
preventing services from starting.
Question: 194
During the Auto-TLS configuration, you need to specify the certificate validity period. Which Cloudera
Manager property controls this validity period?
A. tls.certificate.validity.days
B. cm.auto_tls.certificate_validity
C. auto_tls.certificate.validity_days
D. cm_auto_tls_certificate_validity_days
E. ssl_certificate_validity
Answer: D
Explanation:
The property in ‘cm_auto_tls_certificate_validity_days’ in Cloudera Manager controls the validity period
of the certificates generated by Auto-TLS. This property determines how long the certificates will be
valid before they need to be renewed.
Question: 195
You have enabled Auto-TLS in your CDP cluster. You want to verify that all services are using the Auto-
TLS generated certificates. How can you confirm this?
A. Examine the service configuration files (e.g., core-site.xml, hdfs-site.xml) for references to the Auto-
TLS generated certificate paths.
B. Use the ‘openssl s_client’ command to connect to each service and inspect the presented certificate
chain.
C. Check the Cloudera Manager audit logs for certificate-related events.
D. Review the Cloudera Manager UIfor each service instance to ensure that the TLS/SSL settings are
configured to use Auto-TLS.
E. Check the Cloudera Manager server logs for any errors or warnings related to certificate loading.
Answer: A,B,D
Explanation:
All of the options are valid ways to confirm that services are using Auto-TLS certificates. A, B, and D are
direct methods to verify configuration and certificate usage. C and E can provide indirect evidence or
help troubleshoot issues.
Question: 196
You are configuring Auto-TLS for a multi-tenant CDP clusten Which of the following statements is TRUE
regarding Auto-TLS and multi- tenancy?
A. Auto-TLS automatically creates separate certificate authorities (CAs) for each tenant.
B. Auto-TLS uses a single CA for the entire cluster, regardless of the number of tenants.
C. Auto-TLS cannot be used in a multi-tenant environment.
D. Auto-TLS requires manual configuration of certificates for each tenant.
E. Auto-TLS creates separate namespaces within a single CA for each tenant.
Answer: B
Explanation:
Auto-TLS in CDP on-premise uses a single CA for the entire cluster, regardless of the number of tenants.
While separate CAS might be desirable for stronger isolation, Auto-TLS simplifies the process by using a
single CA.
Question: 197
You need to configure Auto-TLS to use a custom certificate authority (CA) instead of the default
Cloudera Manager CA. What steps are required to achieve this?
A. Import the custom CA certificate into the Cloudera Manager truststore and configure the property to
‘true'.
B. Replace the default Cloudera Manager CA certificate with the custom CA certificate in the
'Ivar/lib/cloudera-scm-server/truststore.jks’ file.
C. Configure the and 'cm_auto_tls_custom_ca_private_key’ properties with the paths to the custom CA
certificate and private key files, respectively, and set to ‘true'.
D. There is no option to use custom CA with Auto-TLS, only internal Cloudera Manager CA is supported.
E. Import the custom CA certificate into the Java truststore of each node in the cluster.
Answer: C
Explanation:
Question: 198
After enabling Auto-TLS, you notice that the Hive Metastore service is unable to connect to the
HiveServer2 service. The error message indicates a certificate validation failure. You have verified that
DNS resolution is working correctly. What is the MOST likely cause and how can you resolve it?
A. The Hive Metastore truststore is not properly configured with the Auto-TLS CA certificate. You need
to update the ‘javax.net.ssl.truststore’ and ‘javax.net.ssl.trustStorePassword’ properties in the Hive
Metastore configuration.
B. The Hive Metastore service principal is not correctly configured. You need to ensure that the principal
includes the correct hostname.
C. The HiveServer2 SSL settings are incorrect. You need to verify the 'hive.server2.use.SSL4 and
Shive.server2.keystore.patW properties.
D. The JCE Unlimited Strength Jurisdiction Policy Files are not installed on the Hive Metastore host.
Install the JCE files and restart the Hive Metastore service.
E. The Hive Metastore Kerberos delegation token is expired. Renew the delegation token using ‘hive —
service metastore –hiveconf hive.metastore.kerberos.principal=‘.
Answer: A
Explanation:
When Auto-TLS is enabled, the Hive Metastore needs to trust the CA that signed the HiveServer2
certificate. This is achieved by configuring the ‘javax.net.ssl.truststore’ and
‘javax.net.ssl.trustStorePassword' properties in the Hive Metastore configuration to point to the correct
truststore containing the Auto-TLS CA certificate.
Question: 199
After enabling Auto-TLS on your cluster, the LDAP authentication for Hue stopped working. Which of the
following steps are necessary to fix the problem?
A. Update Hue's LDAP configuration to use the Auto-TLS CA certificate.
B. Disable Auto-TLS for Hue, as it is not compatible with LDAP authentication.
C. Reconfigure the LDAP server to use a non-SSL connection.
D. Import the LDAP server's certificate into Hue's truststore.
E. Regenerate the Hue's Kerberos keytab.
Answer: A
Explanation:
When Auto-TLS is enabled, all services, including Hue, must be configured to trust the Auto-TLS CA. This
involves updating Hue's LDAP configuration to use the Auto-TLS CA certificate for secure communication
with the LDAP server.
Question: 200
You have enabled Auto-TLS and are using a custom CA. During a rolling restart of the cluster, you
encounter errors related to certificate validation for newly created nodes. The existing nodes are
working fine. What could be the cause?
A. The custom CA certificate is not installed on the newly added nodes.
B. The DNS configuration for the new nodes is incorrect.
C. The Cloudera Manager Agent on the new nodes is using an outdated version.
D. The property is not set to 'true' on the new nodes.
E. The time on the new nodes is not synchronized with the rest of the cluster.
Answer: A,E
Explanation:
If a custom CA is being used, it's crucial to ensure that the custom CA certificate is installed on all nodes,
including the newly added ones. If the CA is not installed, the new nodes will not trust the certificates
issued by that CA. Time synchronization is critical for certificate validation, incorrect time causes
validation to fail.
Question: 201
Which of the following actions will trigger a certificate renewal in a CDP cluster configured with Auto-
TLS?
A. Restarting the Cloudera Manager Server.
B. When certificate reaches its expiry date
C. Changing the hostname of a node in the cluster.
D. Adding a new service to the cluster.
E. Adding a new role to the cluster.
Answer: B,C
Explanation:
Auto-TLS certificates are automatically renewed when they are close to expiring, ensuring continuous
security. Changing the hostname of a node invalidates the existing certificate, requiring a renewal for
the affected service. Adding a new service also cause new certificate to be issued.
Question: 202
Your organization requires that all certificates used in the CDP cluster have a specific Subject Alternative
Name (SAN) format. How can you customize the SANs generated by Auto-TLS?
A. Auto-TLS does not support customization of SANs. The SANs are automatically generated based on
the hostname and IP address of the nodes.
B. Use the property in Cloudera Manager to specify the desired SAN format using a regular expression.
C. Modify the Auto-TLS script located in S/opt/cloudera/cm/scripts/auto_tls.sh' to generate SANs
according to the required format.
D. Customize the ‘ssl_client.cnf and ‘ssl_server.cnf files in 'letc/pki/tls/openssl.cnf to define the SAN
format.
E. Configure the Kerberos realm to match the desired SAN format.
Answer: A
Explanation:
Auto-TLS does not support the customization of SANs. The SANs are automatically generated based on
the hostname and IP address of the nodes. Altering the generation of SANs would require custom
scripting and could break the functionality of Auto-TLS.
Question: 203
After enabling Auto-TLS, you observe that the Namenode is failing to start with a 'java.io.lOException:
Incompatible clusterlD' error.
What could be the reason and how to fix it?
A. The format of the Namenode has changed during the Auto-TLS configuration process. You need to
reformat the Namenode using 'hdfs namenode -format.
B. The 'dfs.namenode.shared.edits.dir’ property is not correctly configured. Verify that it points to a
valid shared directory accessible by all Namenodes.
C. The Kerberos keytab for the Namenode is corrupted. Regenerate the keytab using 'kinit -kt and
update the 'hadoop.security.key.provider.path' property.
D. The Auto-TLS configuration has inadvertently altered the Namenode's cluster ID. You need to revert
the configuration changes or manually set the property to the correct value in 'hdfs-site.xml' on all
nodes.
E. The HDFS client configuration is not up-to-date. Refresh the client configuration using ‘hdfs dfsadmin -
refreshServiceAcl'.
Answer: D
Explanation:
Auto-TLS configuration can sometimes lead to an 'Incompatible clusterlD error if it inadvertently
modifies the Namenode's cluster ID. The cluster ID is a unique identifier for the HDFS cluster, and if it's
inconsistent across Namenodes, the service will fail to start. You need to identify and revert the
configuration changes or manually set the 'dfs.cluster.id' property to the correct value in ‘hdfs-site.xmr
on all nodes.
Question: 204
After enabling Auto-TLS in CDP, you want to use external clients (e.g., beeline, spark-submit) to securely
access the cluster services.
What steps are required to ensure these clients can connect successfully?
A. Distribute the Auto-TLS CA certificate to the client machines and configure the client applications to
trust the CA certificate.
B. Configure the clients to use Kerberos authentication with the appropriate service principals.
C. Disable TLS/SSL verification on the client applications.
D. Use the —truststore’ option to configure the client to point to the correct location.
E. Configure the clients to use the hostnames in letc/hosts file to use secure communication.
Answer: A
Explanation:
For external clients to connect securely to services secured by Auto-TLS, they need to trust the CA that
signed the service certificates. This requires distributing the Auto-TLS CA certificate to the client
machines and configuring the client applications to trust the CA certificate by specifying the path of
trustore. The client applications use the trusted certificates to validate connections to server.
Question: 205
After enabling Kerberos on your CDP on-premises cluster, users are unable to access HDFS through the
command line, even though their Kerberos tickets are valid. The error message suggests an
authentication failure. What is the MOST likely cause?
A. The 'hadoop.security.authentication' property in ‘core-site.xmr is not set to 'kerberoS.
B. The user's principal is not correctly mapped to their OS user in HDFS.
C. The 'hdfs-site.xmr file is missing.
D. The NameNode is not running.
E. The user does not have a Kerberos ticket.
Answer: A
Explanation:
The ‘hadoopsecurity.authentication’ property in ‘core-site.xmr MUST be set to 'kerberos’ for Hadoop
services to authenticate using Kerberos. If it's missing or set to 'simple' , authentication will fail even
with valid Kerberos tickets.
Question: 206
You are configuring Kerberos for your CDP cluster and need to create a principal for the Hive Metastore
service. Which command is the correct way to create a principal with the service keytab in KDC?
A.
B.
C.
D.
E.
Answer: E
Explanation:
The correct command is ‘kadmin -p admin/admin -w password addprinc hive/ _HOST@YOUR.REALM
&& ktadd –k letc/security/keytabs/hive.service.keytab This creates the principal with the fully qualified
hostname C_HOST is replaced by the hostname) and then uses Sktadd' to add the principal to the
service keytab.
Question: 207
After Kerberizing your CDP cluster, you notice that some YARN applications are failing with
'Authentication failed' errors. You suspect it's related to delegation tokens. What configuration property
in ‘yarn-site.xml' directly controls the lifecycle of YARN delegation tokens?
A. yarn.resourcemanager.keytab'
B. yarn.resourcemanager.principal'
C. yarn.resourcemanager.delegation.key.update-intervar
D. ‘yarn.resourcemanager.delegation-token.max-lifetime’
E. yarn.resourcemanager.delegation-token.renew-intervar
Answer: D,E
Explanation:
'yarn.resourcemanager.delegation-token.max-lifetime’ controls the maximum lifetime of a delegation
token, and yarn.resourcemanager.delegation-token.renew-intervar sets the interval at which tokens can
be renewed. These properties are crucial for managing the validity of delegation tokens used for
authentication in YARN applications.
Question: 208
You've configured Kerberos authentication for Hive, and users can successfully query tables. However,
when they attempt to use User
Defined Functions (UDFs) that access external resources, they encounter permission errors. What is a
potential cause of this issue related to Kerberos?
A. The UDFs are not properly configured to use Kerberos delegation tokens.
B. The UDFs are running with the Hive service principal, which does not have permissions to access the
external resources.
C. The HiveServer2 is not configured to pass the user's Kerberos ticket to the IJDF process.
D. The external resources are not Kerberized.
E. The Kerberos ticket cache on the Hive client is outdated.
Answer: A,B
Explanation:
UDFs often need to access resources on behalf of the user. If Kerberos delegation isn't configured
correctly, the IJDF might attempt to access the resources using the Hive service principal's credentials,
which may not have the necessary permissions. Alternatively, UDFs may not be setup to use delegation
tokens.
Question: 209
What is the PRIMARY role of the 'kinit' command in a Kerberized CDP environment?
A. To create Kerberos principals in the KDC.
B. To destroy existing Kerberos tickets.
C. To obtain and cache a Kerberos ticket-granting ticket (TGT) for a user or service.
D. To list all available Kerberos principals.
E. To modify the Kerberos configuration file (krb5.conf).
Answer: C
Explanation:
The ‘kinit command is used to obtain a Kerberos ticket-granting ticket (TGT) from the KDC. This TGT is
then cached and used to request service tickets for accessing Kerberized services.
Question: 210
You are troubleshooting Kerberos authentication issues with Oozie. You've verified that the Oozie
service principal and keytab are correctly configured. However, Oozie workflows that access HDFS are
still failing with authentication errors. Which of the following steps are MOST likely to resolve the issue?
A. Ensure that the 'hadoop.security.authenticatiorf property is set to 'kerberoS in the Oozie server's
‘core-site.xmr file.
B. Configure Oozie to use delegation tokens for HDFS access.
C. Ensure the 'oozie.service.HadoopAccessorService.kerberos.principal' and
'oozie.service.HadoopAccessorService.keytab.file’ properties are set correctly in oozie-site.xmr.
D. Restart the NameNode.
E. Ensure that the Oozie client has a valid Kerberos ticket.
Answer: B
Explanation:
Oozie needs to be configured to use delegation tokens for HDFS access. Delegation tokens allow Oozie
to act on behalf of the user without requiring the user's Kerberos credentials to be passed to the Oozie
server.
Question: 211
You are planning to enable Kerberos on a CDP cluster that includes Kafka’. Which of the following is the
MOST important consideration regarding Kafka and Kerberos?
A. Kafka does not support Kerberos authentication.
B. You must configure Kafka brokers to use SASL/GSSAPI for authentication with Kerberos.
C. You only need to Kerberize Zookeeper; Kafka will automatically authenticate through Zookeeper.
D. The Kerberos realm name must match the Kafka cluster ID.
E. Kafka Connect does not support Kerberos.
Answer: B
Explanation:
Kafka uses SASL/GSSAPI for Kerberos authentication. You need to configure the Kafka brokers to use this
mechanism and create appropriate Kerberos principals for the brokers and clients.
Question: 212
After enabling Kerberos, users complain that Impala queries are slow You suspect the issue is related to
Kerberos overhead. What can you configure to mitigate Kerberos overhead in Impala? Choose two.
A. Enable short-circuit reads in HDFS.
B. Configure Impala to use delegation tokens for HDFS access.
C. Disable Kerberos authentication for Impala.
D. Increase the Kerberos ticket lifetime.
E. Enable TLS/SSL encryption for Impala.
Answer: A,B
Explanation:
Delegation tokens allow Impala to access HDFS on behalf of the user without repeatedly authenticating.
Short-circuit reads allow Impala to bypass the NameNode for some data access, reducing network traffic
and authentication overhead. Increasing the ticket lifetime is not recommanded.
Question: 213
Which of the following configuration files is used to configure Kerberos client settings on a Linux
system?
A. /etc/hadoop/conf/core-site.xml
B. /etc/krb5.conf
C. /etc/security/limits.conf
D. /etc/nsswitch.conf
E. /etc/resolv.conf
Answer: B
Explanation:
The 'letc/krb5.conf file is the standard configuration file for Kerberos clients. It specifies the KDC, realm,
and other settings needed for Kerberos authentication.
Question: 214
You have a CDP cluster with Kerberos enabled. A user is running a Spark application that needs to access
data in Hive. The application fails with an authentication error related to Hive metastore. The user has a
valid Kerberos ticket. What additional configuration might be needed for the Spark application to
successfully authenticate with Hive Metastore?
A. No additional configuration is needed; a valid Kerberos ticket should be sufficient.
B. The Spark application needs to be configured to use Hive's delegation token mechanism.
C. The ‘hive-site.xmr file must be copied to the Spark application's classpath.
D. The user's principal needs to be granted explicit permissions on the Hive metastore database.
E. The application must use a Kerberos-enabled Hive JDBC driver and specify the Hive service principal.
Answer: B,C,E
Explanation:
Spark applications often need to be explicitly configured to use Hive's delegation token mechanism.
Copying hive-site.xml to Spark's classpath makes Hive configuration available to Spark. Using a Kerberos-
enabled Hive JDBC driver configured with service principal ensures Spark can properly authenticate to
the Metastore.
Question: 215
After enabling Kerberos, users report that they can no longer submit jobs through the command line to
a secured YARN cluster, even though they have valid Kerberos tickets. The error message indicates a
failure to obtain a delegation token. Which of the following properties should you examine or configure
in 'yarn-site.xmP to address this issue? Select all that apply.
A. yarn.resourcemanager.principar
B. ‘yarn.resourcemanager.keytab'
C. yarn.nodemanager.aux-services’
D. ‘yarn.resourcemanager.delegation.key.update-intervar
E. yarn.nodemanager.container-executor.class'
Answer: A,B,D
Explanation:
‘yarn.resourcemanager.principar and ‘yarn.resourcemanager.keytab' define the Kerberos identity for
the ResourceManager. yarn.resourcemanager.delegation.key.update-intervar controls how frequently
the ResourceManager updates its delegation key, influencing the availability of delegation tokens. If the
principal or keytab are incorrect, or the update interval is too long, delegation tokens may not be
properly generated or available. ‘yarn.nodemanager.aux-services’ and 'yarn.nodemanager.container-
executor.class' are related to other aspects of YARN and don't directly impact delegation token
generation.
Question: 216
You are configuring Kerberos for a CDP cluster and wish to automate the keytab generation and
principal creation process. You have access to a configuration management tool. What is the most
secure and recommended approach for storing the Kerberos administrator password used by the
automation script?
A. Store the password in plain text in the automation script.
B. Store the password in an environment variable.
C. Store the password encrypted within the configuration management tool's secret management
system (e.g., Vault, HashiCorp Vault, Ansible Vault).
D. Store the password in a file with restricted permissions (e.g., 400).
E. Store the password hashed using MD5.
Answer: C
Explanation:
The most secure approach is to store the Kerberos administrator password encrypted within a dedicated
secret management system like Vault or Ansible Vault. These systems provide secure storage, access
control, and auditing for sensitive information. Storing the password in plain text, an environment
variable, or a file with restricted permissions is insecure. MD5 is an outdated and insecure hashing
algorithm.
Question: 217
You are tasked with implementing Kerberos authentication for your CDP on-premises cluster. Which of
the following steps are essential for configuring users and groups within Cloudera Manager after you
have successfully installed and configured the Kerberos Key Distribution Center (KDC)?
A. Create principals for Cloudera Manager, each service, and user accounts using 'kadmin.locar or a
similar tool. Ensure each principal has a corresponding keytab file.
B. Configure the 'krb5.conf file on all nodes in the cluster to point to the KDC server.
C. Import users and groups from an existing LDAP directory to Cloudera Manager.
D. Manually create local Linux user accounts on all nodes in the cluster that match the Kerberos
principals.
E. Enable Kerberos authentication via the Cloudera Manager Admin Console and provide the necessary
KDC details (Realm, KDC host).
Answer: A,B,E
Explanation:
Creating Kerberos principals and keytabs (A), configuring krb5.conf (B), and enabling Kerberos via
Cloudera Manager (E) are the core steps. While LDAP import can be used (C), it's not always essential,
particularly if the KDC is the primary source of user authentication. Manual user creation on each node
(D) is generally not necessary when using Kerberos.
Question: 218
You want to grant a specific user, 'data analyst', read-only access to data in a specific Hive table, 'sales
data', within the 'marketing' database. Assuming Kerberos is enabled, what is the most secure and
appropriate way to achieve this using Hive authorization (grant privileges)?
A. GRANT SELECT ON TABLE marketing.sales_data TO USER data_analyst;
B. GRANT ALL ON TABLE marketing.sales_data TO USER data_analyst;
C. GRANT READ ON TABLE marketing.sales_data TO USER data_analyst;
D. GRANT SELECT ON DATABASE marketing TO USER data_analyst;
E. GRANT SELECT ON TABLE sales_data TO USER data_analyst;
Answer: A
Explanation:
The correct syntax for granting read-only access (SELECT privilege) on a specific table in Hive is 'GRANT
SELECT ON TABLE database_name.table_name TO USER user_name;'. Option B grants ALL privileges,
which is not read-only. Option C is invalid Hive syntax. Option D grants SELECT on the entire database,
which is broader than necessary. Option E misses the database name, so Hive won't know from which
database to get table
Question: 219
You are auditing user access within your CDP on-premises cluster. You need to identify all users
belonging to the 'finance' group who have 'ADMIN' role assigned in Ranger for HDFS. How can you
efficiently retrieve this information? Assume you have access to the Ranger Admin UIand the command-
line interface.
A. Use the Ranger Admin UI to filter users by group 'finance' and then manually inspect each user's role
assignments for HDFS policies.
B. Use the Ranger REST API to retrieve all users and their group memberships, then filter the results
programmatically to identify users in the 'finance' group and their HDFS roles.
C. Use the Ranger Admin UIto search for the 'ADMIN' role assignment in the HDFS policies and then filter
the results to identify users belonging to the 'finance' group.
D. Query the Ranger database directly using SQL to retrieve user-group mappings and role assignments
for HDFS.
E. Manually review the HDFS ACLs to determine effective permissions for the 'finance' group.
Answer: B,D
Explanation:
The Ranger REST API (B) allows for programmatic retrieval and filtering of user and group information,
providing efficiency. Directly querying the Ranger database (D) allows a DBA or admin to perform
complex queries, joining tables to find user-group mappings and role assignments related to HDFS. The
Ranger UI is possible (A, C) but less efficient for auditing, and HDFS ACLs (E) don't directly correlate with
Ranger roles.
Question: 220
After integrating LDAP with Cloudera Manager, you notice that new users added to LDAP are not
automatically recognized in Cloudera Manager. What configuration setting is most likely causing this
issue, and where would you configure it?
A. The 'Sync Interval' for LDAP synchronization in Cloudera Manager's LDAP configuration is set too high.
B. The 'User Base DN' in Cloudera Manager's LDAP configuration is incorrect, preventing the discovery of
new user entries.
C. The 'Group Membership Attributes' in the LDAP configuration are not correctly mapped to the
corresponding LDAP attributes.
D. Kerberos authentication is enabled and conflicting with the LDAP configuration.
E. The 'External Account Mapping' section is misconfigured, so new accounts are unable to be created.
Answer: A,B
Explanation:
The 'Sync Interval' (A) determines how frequently Cloudera Manager synchronizes with the LDAP server.
If set too high, new users won't be recognized promptly. The 'User Base DN' (B) specifies the starting
point for searching for user entries in the LDAP directory. An incorrect Base DN will prevent the
discovery of new users. Group membership (C) impacts group synchronization but not necessarily user
discovery. Kerberos and LDAP can coexist (D). External account mapping isn't related to syncing(E).
Question: 221
You are configuring Ranger policies for a new data lake in CDP on-premises. You want to implement a
row-level filtering policy on a Hive table 'customer_data' based on the 'country' column. Only users in
the 'eu_analysts' group should be able to see data where 'country' is 'Germany', 'France', or 'Spain'. How
would you define the Ranger policy condition?
A. country IN ('Germany', 'France', 'Spain') AND usergroup='eu_analysts'
B. country IN ('Germany', 'France', 'Spain')
C. ROW FILTER = 'country IN ('Germany', 'France', 'Spain') AND inGroup("eu_analysts")'
D. ROW FILTER = 'country IN ("Germany", "France", "Spain")' && usergroup("eu_analysts")'
E. ROW FILTER = 'country IN ("Germany", "France", "Spain") AND in_group("eu_analysts")'
Answer: E
Explanation:
The correct syntax for row-level filtering in Ranger uses the property, and incorporates the function to
check group membership. Also, the strings need to be quoted correctly with double quotes (E). Option A
is missing the ROW FILTER syntax. Options B do not take the group consideration. The other Options has
incorrect syntaxes and the quotations are incorrect as well.
Question: 222
A user is complaining that they cannot access HDFS, even though they belong to a group that has been
granted access via Ranger policies. You have confirmed that the user is indeed a member of the correct
group and that the Ranger policy is active. What is the most likely reason for this access denial?
A. The user's Kerberos ticket has expired.
B. The HDFS NameNode is in safemode.
C. The user's group membership information has not been synchronized with the HDFS NameNode.
D. The Ranger plugin for HDFS is disabled or misconfigured.
E. The user's account is locked out in the KDC
Answer: C
Explanation:
Even with correct Ranger policies, HDFS needs to be aware of the user's group memberships. If this
information is out of sync, access will be denied. Kerberos ticket expiration (A) would prevent any
Kerberos-authenticated access, not just HDFS. Safemode (B) would affect all users. A disabled Ranger
plugin (D) would likely prevent any Ranger-controlled access. KDC lockout (E) prevent any Kerberos-
authenticated access, not just HDFS
Question: 223
You have a requirement to limit the resources (CPU, memory) that a specific group of users, 'etl_users',
can consume when submitting Spark jobs on YARN. How would you configure this resource limitation
within Cloudera CDP on-premises?
A. Configure YARN queues with resource limits and assign the 'etl_users' group to a specific queue with
restricted resources.
B. Configure Linux cgroups to limit the CPU and memory usage for processes owned by members of the
'etl_users' group.
C. Implement Ranger policies to restrict the maximum CPU and memory allocated to Spark applications
submitted by 'etl_users'.
D. Modify the Spark configuration files (e.g., spark-defaults.conf) to set default resource limits for jobs
submitted by 'etl_users'.
E. Implement resource quotas for HDFS directories owned by the 'etl_users' group.
Answer: A
Explanation:
YARN queues (A) are the standard mechanism for resource management in Hadoop. By configuring
queues with limits and assigning users to those queues, you can control their resource consumption.
Linux cgroups (B) are a lower-level system and not directly integrated with YARN. Ranger (C) is primarily
for authorization, not resource management. Spark configuration (D) can set defaults, but this is not
group-specific resource limitation. HDFS quotas (E) limit storage space, not CPU or memory.
Question: 224
You are setting up a new CDP on-premises cluster with Kerberos enabled. You need to create a service
principal and keytab for the HTTP endpoint of the YARN Timeline Service v2. What is the correct
command to create this principal using 'kadmin.locar , assuming the hostname of the node is
'yarn.example.com' and the Kerberos realm is 'EXAMPLE.COM'?
A. addprinc -randkey HTTP/yarn.example.com@EXAMPLE.COM
B. addprinc HTTP/yarn.example.com@EXAMPLE.COM
C. createprinc -randkey HTTP@yarn.example.com
D. add_principal HTTP/yarn.example.com
E. create -principal HTTP/yarn.example.com@EXAMPLE.COM -random_key
Answer: A
Explanation:
The correct command is ‘addprinc -randkey HTTP/yarn.example.com@EXAMPLE.COMS. The ‘addprinc'
command creates a new Kerberos principal. '-randkey' generates a random key for the principal. The
principal name follows the format ‘service/hostname@REALM' Option B is missing the -randkey. All
other options used wrong commands
Question: 225
You need to configure access control for Kafka topics using Ranger. Specifically, you want to allow users
in the 'kafka_producers' group to produce messages to topics prefixed with 'events-', and users in the
'kafka_consumers' group to consume messages from those same topics. Which Ranger policies would
you create?
A. Option A
B. Option B
C. Option C
D. Option D
E. Option E
Answer: A
Explanation:
Ranger for Kafka uses specific access types: 'produce' for writing messages and 'consume' for reading
messages. You need separate policies (A) to grant these specific permissions to the respective groups.
'write' and 'read' (B) are not the correct access types. 'publish' and 'subscribe' (C) are commonly related
to messaging but are not the specific terms Ranger uses for Kafka. Granting broad access and then
revoking (D) is less secure and harder to manage. Combining permissions in a single policy (E) is
generally not recommended for separation of concems.
Question: 226
You are trying to troubleshoot why a specific user, 'analystl', cannot access a table in Impala even
though you've granted them SELECT privileges via Ranger. You suspect that the user's privileges are not
being properly propagated to Impala’. Which steps can you take to verify the Ranger-Impala integration
and refresh the user's privileges?
A. Restart the Impala catalog server to force a refresh of Ranger policies.
B. Run the 'INVALIDATE METADATA' command in Impala to refresh the metadata cache.
C. Run the 'REFRESH AUTHORIZATION' command in Impala to reload Ranger policies.
D. Verify that the Ranger Impala plugin is enabled and configured correctly in Cloudera Manager.
E. Ensure that the Impala user is synced in the Ranger plugin by running 'SYNC USER analystl command
in Impala shell.
Answer: C,D
Explanation:
'REFRESH AUTHORIZATION' (C) in Impala specifically reloads Ranger policies, ensuring Impala has the
latest privileges. Verifying the Ranger plugin configuration (D) ensures the integration is functioning
correctly. Restarting the catalog server (A) might help in some cases, but it's not the most direct
solution. 'INVALIDATE METADATA (B) refreshes metadata, not authorization. Impala does not have a
‘SYNC USER command (E).
Question: 227
After implementing Kerberos authentication, you notice that some legacy applications, which are not
Kerberos-aware, need to access HDFS. How can you provide access to these applications without
compromising the overall security of the cluster?
A. Disable Kerberos authentication temporarily to allow the legacy applications to access HDFS.
B. Create a dedicated user account with simplified authentication (e.g., password-based) for the legacy
applications to use.
C. Use Knox to provide a secure gateway for the legacy applications to access HDFS via REST APIs with
authentication.
D. Configure the legacy applications to directly access HDFS using the HDFS client libraries without
Kerberos.
E. Implement Delegation Tokens. The token can be acquired through Kerberos and then used to
authenticate non-kerberized applications.
Answer: C,E
Explanation:
Knox (C) provides a secure gateway, allowing authentication and authorization before accessing HDFS
resources via REST. Using delegation tokens (E) offers a secure alternative by allowing non-Kerberos
applications to authenticate with a token obtained via Kerberos. Disabling Kerberos (A) is a major
security risk. Simplified authentication (B) is also less secure. Direct access without Kerberos (D)
circumvents the security setup.
Question: 228
You have a situation where several users are inadvertently locking Hive tables, causing performance
issues. You want to implement a mechanism to automatically kill Hive queries that have been holding
locks for an excessive period. How can you achieve this using configuration settings?
A. Set the ‘hive.lock.expiration’ property in 'hive-site.xmr to a short duration (e.g., 60 seconds) to
automatically release locks after that time.
B. Configure the Hive Metastore to use an external database that supports lock timeouts, and then
configure the database's lock timeout settings.
C. Write a custom script that periodically queries the Hive Metastore database to identify long-held
locks and then kills the corresponding Hive queries using the Hive CLI.
D. Set the "hive.txn.timeout property in 'hive-site.xml' to control how long a transaction can remain
open before being automatically aborted.
E. Set the Ranger policies that have time-based restrictions of access to the data, so no user can
accidentally lock Hive tables for a long time.
Answer: B,D
Explanation:
Hive's lock management relies on the underlying Metastore database. Configuring the database to use
lock timeouts (B) is the most effective approach. Set the 'hive.txn.timeout' property in 'hive-site.xmr to
control how long a transaction can remain open before being automatically aborted.(D)
'hive.lock.expiration’ (A) is deprecated. A custom script (C) is complex and unnecessary. Ranger (E) can
control access but doesn't directly manage lock timeouts.
Question: 229
You have enabled TLS for all services in your Cloudera CDP on-premise cluster. After the cluster restart,
the Hue service fails to start, and the logs show SSL handshake errors. What is the most likely cause?
A. The Hue service does not support TLS encryption.
B. The Hue service is configured to use a different truststore or keystore than the other services.
C. The TLS certificates used for the Hue service have expired.
D. The Hue service is configured to use an unsupported TLS protocol version.
E. The TLS encryption was not enabled for all nodes where Hue runs.
Answer: B
Explanation:
If Hue fails with SSL handshake errors after enabling TLS, it often indicates that it's using a different
truststore/keystore, or that these are incorrectly configured. Ensure Hue is using the correct, updated
truststore and keystore with valid certificates. Incorrect TLS protocol version or incomplete TLS
enablement would typically affect other services as well.
Question: 230
You are configuring encryption in transit for your Cloudera Data Platform (CDP) cluster using Cloudera
Manager. Which of the following components are essential for a successful TLS configuration?
A. A valid Certificate Authority (CA) certificate.
B. Correctly configured Keystores and Truststores on all nodes.
C. The correct TLS/SSL protocol version enabled for each service.
D. Firewall rules allowing communication on the encrypted ports.
E. Kerberos authentication configured for all services
Answer: A,B,C,D
Explanation:
All options are crucial for a successful TLS configuration. A CA certificate is needed to sign the service
certificates. Correct Keystore and Truststore setup ensures identity and trust. Proper TLS/SSL protocol
version management (e.g., TLSv1.2 or higher) is crucial for security, and correctly configured firewall
rules allow communication on encrypted ports. Kerberos is for authentication, not encryption.
Question: 231
You have enabled TLS for all services, but now MapReduce jobs are failing. The error logs indicate a
problem with hostname verification. How would you address this issue, assuming the certificates are
valid and correctly configured?
A. Disable hostname verification globally in the cluster.
B. Update the 'hadoop.ssl.hostname.verifier’ property in the ‘core-site.xml' to allow wildcard matching
of hostnames.
C. Ensure the hostname in the certificate matches the hostname used by the MapReduce client when
connecting to the ResourceManager.
D. Add the following to ‘yarn-site.xmr: ‘yarn.nodemanager.ssl.enabled=false’ .
E. Modify '/etc/hosts’ to include all possible names each server might be known by.
Answer: C
Explanation:
Hostname verification failure suggests that the hostname used by the client doesn't match the
hostname in the server's certificate. The correct solution is to ensure consistency between the
hostname in the certificate and the hostname used by the client during connection. Disabling hostname
verification (A) is insecure. Option (B) is a poor practice, though it could technically work if absolutely
necessary. Option (D) would disable SSL on the Nodemanager (not the correct answer). Adding to
'/etc/hosts’ might work, but isn't reliable in all environments.
Question: 232
Which of the following configuration parameters is most directly responsible for enabling TLS encryption
for communication between the DataNodes in HDFS?
A. dfs.http.policy
B. dfs.namenode.https-address
C. dfs.datanode.address
D. dfs.datanode.https.address
E. hadoop.security.authentication
Answer: D
Explanation:
'dfs.datanode.https.address’ is the configuration parameter that specifies the HTTPS address that the
DataNodes use to communicate with each other and the NameNode for secure data transfer.
‘dfs.http.policy’ enforces HTTP or HTTPS policies, 'dfs.namenode.https-address’ defines the NameNode's
HTTPS address, 'dfs.datanode.address’ is for unencrypted communication, and
'hadoop.security.authentication’ is related to Kerberos.
Question: 233
You need to rotate the TLS certificates in your Cloudera CDP cluster without causing any downtime.
What steps should you take?
A. Replace the certificates and restart the entire cluster at once.
B. Replace the certificates and restart each service role one at a time in a rolling fashion, using Cloudera
Manager.
C. Update the certificates in Cloudera Manager, and the changes will automatically propagate without
any restarts.
D. Replace the certificates on each host manually and restart the Cloudera Manager Agent on each host.
E. Place the new certificates in the HDFS and point all services to that shared location, and then restart
the services
Answer: B
Explanation:
To rotate certificates with minimal downtime, you should replace the certificates in Cloudera Manager,
then perform a rolling restart of each service role. This allows services to continue operating while
others are being updated. Restarting the entire cluster would cause downtime. Cloudera Manager
doesn't automatically propagate certificates without service restarts. Manually replacing certificates and
restarting agents is less efficient and error-prone. Placing the certificates in HDFS is not a common or
recommended practice.
Question: 234
You are setting up TLS encryption for a new Kafka cluster in CDR You have generated the necessary
certificates and configured the listeners. However, the Kafka brokers fail to communicate with each
other. Upon inspection, you find that the broker.id is not included as a Subject Alternative Name (SAN)
in the broker certificates. What is the consequence and how should you solve this?
A. The Kafka brokers will use the internal IP addresses for communication instead, which may cause
connectivity issues. Regenerate the certificates including the broker.id as a SAN.
B. This is not an issue as long as the Common Name (CN) matches the hostname. No action is needed.
C. The Kafka brokers will be vulnerable to man-in-the-middle attacks. Disable TLS to solve this problem.
D. This will cause high CPU usage on the brokers due to constant certificate verification failures.
Regenerate the certificates without the broker.id as a SAN.
E. The communication will be unencrypted and vulnerable.
Answer: A
Explanation:
Kafka requires the broker.id to be included as a SAN (Subject Alternative Name) in the broker certificates
to ensure proper TLS communication between brokers. If the broker.id is missing, the brokers might
attempt to use internal IP addresses, leading to connectivity problems, especially in multi-homed
environments. You must regenerate the certificates to include the broker.id as a SAN. Disabling TLS or
ignoring the issue are not viable solutions.
Question: 235
Your organization mandates that all network communication must use TLS 1.3. How do you ensure that
all components in your CDP cluster adhere to this requirement, given that some older components
might support older TLS versions?
A. Configure Cloudera Manager to only allow TLS 1.3 in its settings. Cloudera Manager will then enforce
this setting across all components.
B. Configure the ‘ssl_protocols' parameter in the relevant configuration files (e.g., 'ssl-server.xml', 'core-
site.xml') for each service to only include TLSv1.3. Example: ‘ssl_protocolsTLSv1.3'.
C. Upgrade all components to the latest versions. These should automatically use TLS 1.3.
D. Disable all older TLS versions (TLS 1.0, TLS 1.1, TLS 1.2) at the operating system level by modifying the
system-wide SSL configuration.
E. Configure the Java Cryptography Extension (JCE) policy files to disable older TLS versions.
Answer: B,E
Explanation:
To enforce TLS 1.3, you need to explicitly configure each service to only use that version. This is typically
done by modifying the service-specific configuration files (B). Additionally, you could configure the JCE
policy files to disable older TLS versions, which offers a more systemic approach and ensures no older
versions are used by any Java application. While upgrading components (C) can help, it doesn't
guarantee that older TLS versions are disabled. Disabling at the OS level (D) might impact other
applications. Cloudera Manager (A) doesn't inherently provide a global setting to force TLS versions for
all components.
Question: 236
After enabling TLS, you notice that performance has degraded significantly in your HDFS cluster. What
are the potential causes of this performance degradation related to TLS encryption?
A. Increased CPU overhead due to encryption and decryption.
B. Increased network latency due to larger packet sizes with TLS headers.
C. Insufficient memory allocated for the Java Cryptography Extension (JCE).
D. Inefficient configuration of the cipher suites used for TLS.
E. Lack of hardware acceleration for cryptographic operations.
Answer: A,D,E
Explanation:
TLS encryption introduces CPU overhead for encryption and decryption (A). Inefficiently configured
cipher suites can also contribute to performance problems (D). Lack of hardware acceleration for
cryptographic operations means that the CPU bears the entire burden of encryption, which can
significantly slow down performance (E). While increased packet sizes and memory issues might have a
small impact, the primary reasons are CPU overhead, inefficient cipher suites, and lack of hardware
acceleration.
Question: 237
You are tasked with configuring TLS for your Cloudera CDP cluster. You have a central Certificate
Authority (CA). What is the recommended approach for managing certificates across the cluster?
A. Manually generate and distribute certificates to each service and node.
B. Use Cloudera Manager's built-in certificate generation and management tools to generate and deploy
certificates signed by the central CA.
C. Configure each service to trust the central CA's root certificate and then request certificates directly
from the CA.
D. Create a self-signed certificate for each service, then import each of these into a central truststore.
E. Copy the same certificate and key pair to all services.
Answer: B
Explanation:
Cloudera Manager provides built-in tools for generating and managing certificates, including signing
them with a central CA. This simplifies the process and ensures consistency. Manually managing
certificates (A) is error-prone and difficult to maintain. While configuring each service to trust the CA
and request certificates directly (C) is possible, it's more complex and not directly supported by Cloudera
Manager's tooling. Self-signed certificates (D) are not ideal for production environments. Using the same
certificate for all services (E) is a security risk.
Question: 238
After enabling encryption in transit on a CDP cluster, you are observing connection timeout issues. The
clients are unable to connect to the services. What are the possible causes for these timeouts related to
the new encryption?
A. Firewall rules are blocking the encrypted traffic ports.
B. The TLS handshake process is taking longer than the client's timeout settings.
C. The client and server are using incompatible cipher suites.
D. The client's truststore does not contain the CA certificate used to sign the server's certificate.
E. Kerberos authentication is conflicting with the SSL settings
Answer: A,B,C,D
Explanation:
Several factors can contribute to connection timeout issues after enabling TLS. Firewall rules (A) might
be blocking the new ports used for encrypted traffic. The TLS handshake (B) can take longer than the
client's timeout if resources are limited or the network is slow. Incompatible cipher suites (C) between
the client and server will prevent a successful connection. If the client doesn't trust the CA that signed
the server's certificate (D), the handshake will fail. Option E (Kerberos) is not correct, since timeouts
would not be impacted by Kerberos.
Question: 239
You want to verify that TLS encryption is working correctly for HDFS data transfer. Which of the
following methods can you use?
A. Use ‘tcpdump’ or 'Wireshark’ to capture network traffic and analyze the packets to ensure they are
encrypted.
B. Check the HDFS service logs for messages indicating successful TLS handshake and data transfer.
C. Use the ‘openssl s_client’ command to connect to the DataNodes and NameNode and verify the
certificate information.
D. Monitor the CPU usage on DataNodes, an increase in CPU implies an active encryption process.
E. Review the audit logs in Ranger for encrypted connections.
Answer: A,B,C
Explanation:
You can use 'tcpdump’ or 'Wireshark' (A) to capture and analyze network traffic, confirming that the
packets are encrypted. Checking the HDFS service logs (B) can reveal messages about successful TLS
handshakes and data transfers. ‘openssl s_client’ (C) allows you to connect to the DataNodes and
NameNode and inspect the certificate information, ensuring it is valid. While increased CPU usage (D)
might indicate encryption, it's not a direct verification method, it could be due to other processes. While
Ranger does perform access control for secure services, TLS isn't logged or audited in the same way.
Question: 240
A user reports that they are unable to connect to the HiveServer2 JDBC endpoint using a client
application. You suspect that TLS configuration on the client side may be incorrect. Which of the
following JDBC connection string parameters are most relevant for troubleshooting TLS connectivity?
A. ‘ssl=true’
B. ‘sslTrustStore=‘
C. ‘trustStorePassword=‘
D. ‘principal=‘
E. ‘useSSL=trues
Answer: A,B,C,E
Explanation:
For TLS connectivity in JDBC, ‘ssl=true’ or ‘useSSL=true' (A, E) enables SSL/TLS. 'sslTrustStore=' (B)
specifies the location of the truststore file containing the CA certificate, and ‘trustStorePassword=' (C)
provides the password for the truststore. The ‘principal=‘ parameter is related to Kerberos
authentication, not TLS encryption.
Question: 241
You are tasked with installing Atlas in a Cloudera Data Platform (CDP) on-premises environment. Which
of the following statements regarding the installation process are accurate?
A. Atlas requires a dedicated ZooKeeper ensemble, separate from other CDP services.
B. Atlas metadata is stored within the Hive Metastore by default and cannot be changed during
installation.
C. The Atlas service can be co-located on the same nodes as other services like Hive or Impala, as long as
resource contention is carefully managed.
D. During installation, you must specify a fully qualified domain name (FQDN) for each Atlas server node.
E. Atlas installation requires that a supported database (e.g., PostgreSQL, MySQL) is pre-configured and
accessible.
Answer: C,D,E
Explanation:
Atlas can be co-located with other services if resources are managed properly. FQDNs are necessary for
proper inter-service communication. Atlas requires a backend database to store its metadata.
Question: 242
After installing Atlas, you need to verify its functionality. Which command-line tool is typically used to
interact with the Atlas API and perform basic metadata operations?
A. cmctl
B. atlas-cli
C. curl
D. hdfs dfs
E. beeline
Answer: C
Explanation:
While there isn't a dedicated 'atlas-cli' tool, 'curl' is commonly used to interact with Atlas's REST API,
allowing for metadata retrieval, creation, and updates.
Question: 243
You want to create a new custom classification in Atlas to tag sensitive data’. Which Atlas API endpoint
should you use to define this classification?
A. /api/atlas/v2/types/typedefs
B. /api/atlas/v2/entity
C. /api/atlas/v2/lineage
D. /api/atlas/v2/glossary
E. /api/atlas/v2/types
Answer: A
Explanation:
The 'lapi/atlas/v2/types/typedefs' endpoint is used to create and manage type definitions, including
classifications, entity types, and enums in Atlas.
Question: 244
Given the following JSON payload, which represents a custom classification, what is missing to make it a
valid Atlas classification?
A. The’super Types’ array is missing, specifying inheritance.
B. The ‘attributeDefs’ array is missing, defining attributes for the classification.
C. The ‘name’ field is improperly formatted.
D. The ‘serviceType’ field is missing.
E. The ‘category' field is missing or invalid.
Answer: B
Explanation:
A valid Atlas classification requires the 'attributeDefs' array to define any attributes associated with the
classification. Without this, the classification is not properly defined and cannot be used effectively.
Question: 245
You have defined a classification named 'PII' in Atlas. How can you apply this classification to a Hive
table named 'customers' in the 'sales' database using HiveQL?
A. COMMENT ON TABLE sales.customers IS 'CLASSIFIED: Pill;
B. ALTER TABLE sales.customers SET TBLPROPERTIES ('atlas.classification'
C. ALTER TABLE sales.customers ADD CLASSIFICATION PII;
D. CREATE CLASSIFICATION PII ON TABLE sales.customers;
E. ALTER TABLE sales.customers SET SERDEPROPERTIES ('atlas.classification' = 'Pill);
Answer: B
Explanation:
The correct HiveQL command to apply an Atlas classification is using ALTER TABLE SET TBLPROPERTIES
('atlas.classification' = 'classification_name');’ This sets a table property that Atlas recognizes and uses to
associate the classification with the table.
Question: 246
Which of the following configurations directly impacts Atlas's ability to ingest metadata from Hive?
A. hive.hook.proto.validate.fully.materialized=true
B. hive.metastore.uris
C. atlas.authentication.method=kerberos
D. atlas.jaas.KafkaClient.option.serviceName
E. atlas.enable.hive.hook=true
Answer: B,E
Explanation:
'hive.metastore.uriS specifies the location of the Hive Metastore, which Atlas needs to connect to.
‘atlas.enable.hive.hook=true' enables the Hive hook, allowing Hive to send metadata updates to Atlas.
Question: 247
You have enabled Atlas integration with Ranger. What is the primary benefit of this integration in a CDP
on-premises environment?
A. Automatically encrypting data at rest in HDFS.
B. Synchronizing Ranger policies with Atlas classifications, enabling policy enforcement based on
metadata tags.
C. Automatically backing up Atlas metadata to a remote cloud storage service.
D. Creating audit logs for all data access events directly within Atlas.
E. Allowing Atlas to directly manage user authentication and authorization.
Answer: B
Explanation:
The key benefit of Ranger and Atlas integration is the ability to create Ranger policies based on Atlas
classifications. This allows for dynamic policy enforcement based on metadata.
Question: 248
After a recent upgrade, the Atlas IJI is not accessible. You suspect a problem with the Atlas server. How
would you check the status of the Atlas service using Cloudera Manager?
A. Use the ‘atlas-stop' and ‘atlas-start’ commands to restart the service.
B. Navigate to the Atlas service in Cloudera Manager, check the service status, and review any recent
alerts or logs.
C. Execute ‘yarn application -list to see if the Atlas application is running.
D. Check the ZooKeeper nodes for the Atlas service to verify its availability.
E. Review the 'atlas. log' file in the /var/log/atlas directory for error messages.
Answer: B,E
Explanation:
Cloudera Manager provides a centralized view of service status, alerts, and logs. Checking the specific
Atlas log files can provide granular details on any errors or issues.
Question: 249
You need to configure Atlas to ingest metadata from a custom application that is not natively supported.
What is the recommended approach?
A. Directly modify the Atlas codebase to add support for the custom application.
B. Develop a custom Atlas bridge that uses the Atlas API to push metadata from the application.
C. Configure the application to write metadata directly to the Atlas database.
D. Use Flume to collect metadata from the application and send it to Atlas.
E. Utilize the GenericJson hook to send metadata as a JSON to Kafka topic which Atlas can consume.
Answer: B,E
Explanation:
Developing a custom bridge using the Atlas API is the recommended and supported way to integrate
with custom applications. GenericJson hook provides a standardized method for sending custom
metadata to Atlas via Kafka.
Question: 250
An analyst reports that lineage information is missing for a newly created Impala table in Atlas. What
steps should you take to troubleshoot this issue?
A. Verify that the Impala audit logs are being correctly ingested by Atlas.
B. Ensure that the 'impala.atlas.hook.enabled' property is set to 'true' in the Impala configuration.
C. Restart the Atlas service to refresh its metadata cache.
D. Manually create the lineage information in Atlas using the API.
E. Check if Impala is configured to use the same Hive Metastore instance as Atlas.
Answer: A,B,E
Explanation:
Impala lineage depends on audit logs being ingested and the Impala Atlas hook being enabled. Both
Atlas and Impala using the same Hive Metastore is essential for metadata consistency and proper
lineage tracking. Verify property also.
Question: 251
You are planning to implement data masking based on Atlas classifications. Which component(s) within
the CDP ecosystem would you primarily use to enforce these masking policies?
A. Atlas
B. Ranger
C. Sentry
D. HDFS ACLs
E. Cloudera Navigator
Answer: B
Explanation:
Ranger is the primary component for enforcing data masking policies in CDP. By integrating with Atlas
classifications, you can create dynamic masking policies based on metadata.
Question: 252
Consider you have installed Atlas on your CDP on-premise cluster and want to classify all tables in Hive
databases that contain credit card information. Which of the following steps would be most effective to
accomplish this using automated discovery and classification?
A. Manually inspect each Hive table and apply the 'CreditCardlnfo' classification in Atlas.
B. Write a custom script that connects to the Hive Metastore, analyzes column names and data samples,
and then uses the Atlas API to apply the 'CreditCardlnfo' classification based on pattern matching.
C. Create a Ranger policy that automatically applies the 'CreditCardlnfo' classification to all Hive tables.
D. Configure the Atlas Hive hook to automatically classify tables based on column names that match
specific patterns (e.g., 'credit_card', 'cc_number').
E. Use the Apache Griffin project to profile the data quality and automatically classify tables based on
predefined data quality rules for credit card information.
Answer: B,D
Explanation:
While manual inspection is an option, it's not scalable. Writing a custom script allows for automated
pattern matching and classification via the Atlas API. Configuring the Atlas Hive hook with column name
pattern matching also provides automated classification. Although other options might seem viable, the
custom script provides the most flexibility and direct control over the classification process. Option D is
also valid, but not as precise.
Question: 253
You are configuring Ranger to authorize access to Hive. After enabling Ranger plugin in Hive, you notice
that users are still able to access Hive tables without Ranger policies being applied. Which of the
following is the most likely cause?
A. The Hive service in Ranger Admin is not properly configured with the correct JDBC connection string.
B. The Ranger plugin for Hive is not enabled in the Hive Metastore configuration.
C. The Ranger Admin service is down.
D. The Hive user does not have the appropriate Kerberos principal associated.
E. The HiveServer2 advanced configuration property ‘hive.security.authorization.manager’ is not set to
‘org.apache.ranger.authorization.hive.authorizer.RangerHiveAuthorizer’.
Answer: E
Explanation:
Ranger authorization for Hive relies on the ‘hive.security.authorization.manager' property being
correctly set to the Ranger authorizer. If this is missing or incorrect, Hive will bypass Ranger policies.
Question: 254
Which Ranger component is responsible for enforcing access policies at the resource level for various
Hadoop components?
A. Ranger Admin
B. Ranger UserSync
C. Ranger Plugins
D. Ranger KMS
E. Ranger TagSync
Answer: C
Explanation:
Ranger Plugins are embedded within the Hadoop components (e.g., Hive, HDFS, YARN) and intercept
access requests to enforce the policies defined in Ranger Admin.
Question: 255
You want to grant read access to a specific column in a Hive table to a group of users using Ranger.
Which of the following policy structures would you use?
A. Database -> Table -> Column
B. Table -> Database -> Column
C. Column -> Table -> Database
D. Database -> Column-> Table
E. Namespace-> Database -> Table
Answer: A
Explanation:
Ranger policies for Hive follow a hierarchical structure: Database -> Table -> Column. This allows you to
define granular access controls at the column level within a specific table in a database.
Question: 256
Which of the following is the primary purpose of Ranger UserSync?
A. Synchronizing Ranger policies across multiple Hadoop clusters.
B. Synchronizing user and group information from external identity providers (e.g., LDAP, Active
Directory) into Ranger.
C. Synchronizing data masking policies across different Hadoop components.
D. Synchronizing audit logs from Hadoop components to Ranger Admin.
E. Synchronizing Ranger roles and permissions.
Answer: B
Explanation:
Ranger UserSync is responsible for fetching user and group information from external identity providers
and keeping Ranger's internal user and group database synchronized. This ensures that Ranger can
properly authenticate and authorize users based on their existing identities.
Question: 257
You are tasked with auditing all access to HDFS files through Ranger. What configuration setting needs
to be enabled on HDFS for successful auditing?
A. Enable HDFS encryption.
B. Enable HDFS quota management.
C. Enable Ranger audit logging in HDFS configuration.
D. Enable HDFS ACLs.
E. Enable HDFS delegation tokens.
Answer: C
Explanation:
To audit access to HDFS files through Ranger, you need to enable Ranger audit logging in the HDFS
configuration. This ensures that all access attempts are logged and sent to Ranger for auditing purposes.
Question: 258
You are configuring Ranger to provide centralized auditing for Hive, HDFS, and YARN. However, you
notice that the audit logs are not appearing in Ranger Admin for YARN. Which of the following steps is
most likely to resolve this issue?
A. Restart the YARN Resource Manager.
B. Verify that the Ranger plugin is enabled for YARN in the YARN configuration and that the yarn-site.xml
is correctly configured to point to the Ranger Admin server.
C. Restart the Ranger Admin server.
D. Enable audit logging in the YARN application master configuration.
E. Disable and re-enable the YARN service in Cloudera Manager.
Answer: B
Explanation:
For Ranger to audit YARN access, the Ranger plugin must be correctly enabled in the YARN configuration
(yarn-site.xml). This includes specifying the Ranger Admin server details so that YARN can send audit
logs to Ranger.
Question: 259
What is the correct order of steps to install and configure Ranger for Hive access control in a Cloudera
Data Platform (CDP) on-premises cluster?
A. Install Ranger Admin -> Configure Ranger Usersync -> Enable Ranger Plugin for Hive -> Define Ranger
Policies.
B. Enable Ranger Plugin for Hive -> Install Ranger Admin -> Configure Ranger Usersync -> Define Ranger
Policies.
C. Install Ranger Admin Define Ranger Policies -> Configure Ranger Usersync Enable Ranger Plugin for
Hive.
D. Configure Ranger Usersync -> Install Ranger Admin -> Enable Ranger Plugin for Hive -> Define Ranger
Policies.
E. Install Ranger Admin -> Configure Ranger Usersync Define Ranger Policies -> Enable Ranger Plugin for
Hive.
Answer: A
Explanation:
The correct order is: Install Ranger Admin, configure IJserSync to populate users/groups, enable the
Ranger plugin for Hive so that Hive requests are intercepted and checked against Ranger, and finally,
define Ranger policies to grant or deny access.
Question: 260
You need to implement row-level filtering in Hive using Ranger. Which of the following conditions must
be met to achieve this functionality?
A. The Hive table must be stored in ORC format.
B. The Ranger plugin for Hive must be configured with a row-level filtering policy that includes a SQL
WHERE clause condition.
C. The user accessing the Hive table must have the 'SELECT' privilege on the table.
D. The Hive table must be ACID-compliant.
E. All of the above.
Answer: E
Explanation:
All the conditions must be met. The Hive table needs to be in ORC format, the Ranger plugin needs to be
configured with a SQL WHERE clause to filter rows, the user needs SELECT privilege, and the Hive table
needs to be ACID-compliant (although strictly speaking Ranger masking/filtering can be applied to non-
ACID tables, ACID is needed for update/delete policies).
Question: 261
You are troubleshooting an issue where Ranger policies are not being enforced for a specific HDFS path.
You have confirmed that the Ranger plugin is enabled for HDFS and that the policies are correctly
defined in Ranger Admin. However, users can still access the path without the policies being applied.
Which of the following are potential causes for this issue? (Select all that apply)
A. The HDFS user's Kerberos principal is not properly configured.
B. The HDFS namenode is not properly configured to communicate with Ranger Admin.
C. The HDFS path is not included in any Ranger policies.
D. The Ranger Admin service is experiencing high latency, causing delays in policy retrieval.
E. HDFS ACLs are conflicting with the Ranger policies.
Answer: B,E
Explanation:
If policies are not being enforced despite the plugin being enabled and policies defined, the most likely
causes are: HDFS namenode cannot properly communicate with Ranger Admin to retrieve policies, and
there is a potential conflict with pre-existing HDFS ACLs which might override Ranger policies.
Question: 262
You want to configure Ranger to mask a specific column in a Hive table (e.g., email address) so that only
authorized users can see the actual data’. Other users should see a masked version (e.g., 'XXX'). Which
of the following masking options available in Ranger would be most suitable for this scenario?
A. Partial mask: show first 4, last 3
B. Redact
C. Hash
D. Custom
E. Date: show only year
Answer: D
Explanation:
The 'Custom' masking option is the most suitable because it allows you to define a specific masking
format (e.g., replacing the entire email address with IXXX') for unauthorized users. This provides the
required level of data protection while allowing authorized users to see the actual email address.
Question: 263
Consider the following Ranger policy for HDFS: Resource: /data/sensitive User: analytics_team
Permissions: read, write, execute Conditions: Time-based restriction (9 AM - 5 PM) If a user from the
'analytics_team' attempts to access the '/data/sensitive' directory at 7 PM, what will happen?
A. The user will be granted read, write, and execute access because they belong to the ‘analytics_team’ .
B. The user will be denied access because the access attempt is outside the allowed time window (9 AM
- 5 PM).
C. The user will be prompted to enter a password to verify their identity.
D. The user will be granted read access only.
E. The behavior is unpredictable; it depends on the load on the Ranger Admin server.
Answer: B
Explanation:
Ranger policies are strictly enforced based on the defined conditions. In this case, the time-based
restriction will deny access to the 'Idata/sensitive" directory outside the 9 AM - 5 PM window, regardless
of the user's group membership.
Question: 264
You are configuring Ranger auditing to store audit events in HDFS. However, the Ranger Admin server is
generating a large number of audit events, which are filling up the HDFS storage. Which of the following
strategies can you implement to manage the audit event storage effectively? (Select all that apply)
A. Reduce the audit log level to only capture critical events.
B. Implement a data lifecycle management policy in HDFS to archive or delete older audit events.
C. Increase the replication factor for the audit event directory in HDFS.
D. Configure Ranger to store audit events in a different storage system (e.g., Kafka).
E. Disable auditing for less critical services.
Answer: A,B,D,E
Explanation:
To manage audit event storage effectively, you can: reduce the audit log level to capture only critical
events, implement data lifecycle management policies to archive/delete old events, configure Ranger to
use a different storage system (like Kafka which allows for easier scaling), and disable auditing for less
critical services. Increasing the replication factor will increase storage use.
Question: 265
You are tasked with implementing column-level masking on the 'employees’ table in Hive. The
requirement is to mask the ‘salary' column for users in the 'analytics' group while allowing full access to
the 'hr' group. Which of the following Apache Ranger policies would best achieve this?
A. Create a Ranger policy with the 'analytics' group, 'salary' column, and 'mask' permission, selecting the
'MASKED' mask type. Create another policy with 'hr' group, 'salary' column, and 'select' permission,
allowing full access.
B. Create a Ranger policy with the 'analytics' group, 'salary' column, and 'select' permission, selecting the
'NULLIFY' mask type. Create another policy with 'hr' group, 'salary' column, and 'select' permission,
allowing full access.
C. Create a single Ranger policy with the 'analytics' group, 'salary' column, and 'mask' permission,
selecting the 'PARTIAL' mask type. Add an exclusion condition for the 'hr' group with 'select' permission.
D. Create a Ranger policy with the 'analytics' group, 'salary' column, and 'select' permission, selecting
the 'MASKED' mask type. No separate policy is needed for the 'hr' group.
E. Create a single Ranger policy with the 'analytics' group, 'salary' column, and 'mask' permission,
selecting the 'MASKED' mask type. Create another policy with 'hr' group, 'salary' column, and 'select'
permission, allowing full access. Ensure the 'hr' policy is ordered higher in the policy list.
Answer: E
Explanation:
The correct approach is to create two separate policies: one for masking the salary for the 'analytics'
group and another explicitly granting select access to the 'hr' group. The 'hr' policy needs to be higher in
order because Ranger uses a first-match approach. If a single policy with exclusions is used, it can
become complex to manage and might not be as clear in its intent.
Question: 266
You have configured Ranger to audit access to Hive tables. You notice a significant increase in audit logs,
making it difficult to analyze relevant events. Which of the following actions could you take to reduce
the volume of audit logs while still maintaining adequate security monitoring?
A. Disable auditing for all Hive policies except those related to sensitive data, such as columns containing
Personally Identifiable Information (PII).
B. Reduce the audit frequency by increasing the ‘ranger.audit.async.max.queue.size’ property in
Ranger's configuration.
C. Implement a Ranger policy to exclude certain users or groups from audit logging while still auditing
their data access.
D. Decrease the ‘ranger.audit.max.flusher.threads’ property to reduce the number of threads writing
audit logs.
E. Configure Ranger to only audit 'SELECT' statements and disable auditing for 'INSERT', 'UPDATE', and
'DELETE' statements.
Answer: A
Explanation:
Disabling auditing for policies not related to sensitive data will significantly reduce the volume of audit
logs without compromising security monitoring for critical resources. Increasing queue size (B) only
postpones the writing. Excluding users or groups (C) could miss important audit events. Decreasing
flusher threads (D) could cause performance issues or dropped audits. Auditing only SELECT statements
(E) would miss data modification activities.
Question: 267
You are configuring Apache Ranger to manage access to data in Hive. You need to define a policy that
grants access to a specific set of columns in a table only when the user is accessing the data through a
specific application. How can you achieve this?
A. Use the 'Context Enforcer' feature in Ranger to define a policy that restricts access based on the
application's name.
B. Use the 'Resource-based Authorization' feature in Hive to define access control lists (ACLs) based on
the application's user ID.
C. Use the 'Application ID' condition in Ranger policies to restrict access to specific columns based on the
application's identifier. This needs Ranger version 2.0+.
D. Use the 'Custom Conditions' feature in Ranger to write a Java plugin that verifies the application's
identity and grants access accordingly.
E. It is not possible to restrict access based on the application accessing the data using Ranger's built-in
features. Custom code or external authentication mechanisms are required.
Answer: C
Explanation:
Ranger policies can use the 'Application ID' condition (available in Ranger 2.0 and later) to restrict access
based on the application accessing the data. This allows you to define policies that grant access to
specific columns only when the user is accessing the data through the specified application. Context
Enforcer is not a valid Ranger feature, and resource-based authorization in Hive does not directly
integrate with application identity in this way.
Question: 268
Which of the following are valid methods for ensuring data masking policies are consistently enforced
across multiple Cloudera Data Platform (CDP) environments (e.g., development, staging, production)?
(Select TWO)
A. Manually re-creating the Ranger policies in each environment, ensuring identical configurations.
B. Using the Ranger Import/Export functionality to export policies from one environment and import
them into another.
C. Configuring Ranger replication across environments using a shared metastore.
D. Using Cloudera Manager to automatically synchronize Ranger policies across all configured
environments.
E. Employing infrastructure-as-code (laC) tools, such as Terraform or Ansible, to automate the
deployment and configuration of Ranger policies.
Answer: B,E
Explanation:
Ranger's import/export functionality provides a structured way to move policies between environments.
Infrastructure-as-code (laC) allows for automated and repeatable deployment of Ranger configurations,
including policies, ensuring consistency across environments. Manually re-creating policies is error-
prone and not scalable. While CM manages the Ranger service itself, it does not directly manage Ranger
policies. Replication using a shared metastore would only replicate metadata, not Ranger policies.
Question: 269
You've configured Apache Ranger with Atlas integration. After making some changes to Ranger policies,
you notice that the Atlas metadata is not being updated with the new access control information. Which
of the following steps would you take to troubleshoot this issue?
A. Restart the Ranger Admin service to force a synchronization with Atlas.
B. Verify that the ‘atlas.hook.ranger.sync' property is set to ‘true' in the Ranger Admin's configuration.
C. Check the Ranger audit logs for any errors related to Atlas integration.
D. Manually trigger a full import of Ranger policies into Atlas using the Atlas API.
E. Ensure the user running the Ranger to Atlas synchronization has 'atlas_admin' role.
Answer: B,C
Explanation:
The ‘atlas.hook.ranger.sync' property must be set to 'true' to enable automatic synchronization between
Ranger and Atlas. Checking the Ranger audit logs can reveal errors or warnings related to the Atlas
integration, providing clues about the cause of the synchronization failure. Restarting the Ranger Admin
service might help if the process has stalled, but doesn't address the root cause if synchronization is
disabled or failing. There is no specific 'manual import' from Ranger to Atlas using the Atlas API. The
Atlas admin role is important to check as well.
Question: 270
A data analyst reports that they are unable to see the data in a specific column, even though they
believe they have the necessary permissions. After investigating, you discover that a masking policy is
applied to the column, but the analyst is supposed to be exempt. Assuming Ranger is configured
correctly, what is the MOST likely cause of this issue?
A. The analyst's user account is not properly synchronized between Ranger and the underlying data
source (e.g., Hive, Impala).
B. The Ranger policy granting the analyst access is not high enough in the policy order compared to the
masking policy.
C. The masking policy is configured with a higher priority than the grant access policy and there is a
conflict.
D. The masking policy has not been properly replicated to the data source nodes, and a restart is
required.
E. The Ranger audit log is full and masking policies are not being applied correctly due to write errors.
Answer: B
Explanation:
Ranger policies are evaluated in order, with the first matching policy taking effect. If a masking policy is
applied before a policy granting access to the analyst, the masking will override the access, even if the
analyst is supposed to be exempt. Account synchronization (A) is important, but a separate access policy
would still be needed. Replication of policies (D) and audit log issues (E) would likely affect all users, not
just a specific analyst. Policy conflict and priority (C) is similar to the correct answer, but policies are
evaluated based on order, not priority.
Question: 271
You are responsible for implementing data lineage tracking in your Cloudera CDP environment. You
want to use Apache Atlas to capture the lineage of data transformations performed by Spark jobs.
Which of the following configurations are necessary to enable Spark-Atlas integration for data lineage?
A. Install the Atlas client libraries on all Spark driver nodes and configure the 'spark.extraListenerS
property to point to the Atlas listener class.
B. Configure the Spark thrift server to publish metadata events to Atlas via the Atlas REST API.
C. Enable the ‘spark.sql.queryExecutionListenerS property and specify the Atlas lineage listener class.
Ensure the Atlas client libraries are included in the Spark classpath.
D. No specific configuration is needed. Spark automatically integrates with Atlas to capture data lineage
information.
E. You must write a custom Spark listener that extracts data lineage information from Spark events and
publishes it to Atlas using the Atlas API.
Answer: C
Explanation:
To enable Spark-Atlas integration, you need to configure the 'spark.sql.queryExecutionListenerS
property to point to the Atlas lineage listener class. This listener captures Spark SQL query execution
events and publishes them to Atlas, creating data lineage relationships. The Atlas client libraries must
also be included in the Spark classpath for the listener to function correctly. spark.extraListeners is used
for general spark listeners, but spark.sql.queryExecutionListeners is specifically used to track SQL query
execution. There is no built in thrift server publish to Atlas function. While you could write a custom
listener, the default configuration is the intended practice.
Question: 272
A security auditor has reported that sensitive data is being stored in a Hive table without any masking or
access controls. As a Cloudera CDP administrator, what steps would you take to remediate this issue
using Apache Ranger? Select all that apply
A. Implement column-level masking on the sensitive columns in the Hive table to prevent unauthorized
access.
B. Configure Ranger policies to restrict access to the Hive table based on user roles and groups.
C. Encrypt the entire Hive table using Hive's built-in encryption features to protect the sensitive data at
rest.
D. Implement row-level filtering in Ranger to restrict access to specific rows based on user attributes or
data values.
E. Purge the sensitive data from the Hive table and store it in a separate, more secure location.
Answer: A,B,D
Explanation:
Implementing column-level masking (A) protects sensitive data by replacing it with masked values for
unauthorized users. Configuring Ranger policies (B) restricts access to the table based on roles and
groups. Implementing row-level filtering (D) allows access to rows based on some condition.
Encryption(C) is generally recommended, but is more focused on data at rest and doesn't protect access
in the same way that Ranger policies and masking do. Purging the data(E) might be a valid option in
some cases, but is not a general solution for managing sensitive data.
Question: 273
You are using Apache Atlas to manage the metadata of your data assets in Cloudera CDP. You want to
create a custom entity type in Atlas to represent a specific type of data transformation process in your
organization. What steps would you take to define this custom entity type?
A. Create a new Java class that extends the ‘org.apache.atlas.model.instance.AtlasEntity’ class and
define the custom attributes and relationships for the entity type.
B. Define a new entity definition in a JSON file and import it into Atlas using the Atlas REST API.
C. Create a new table in the Atlas metadata repository and define the columns for the custom entity
type.
D. Use the Atlas IJI to create a new entity type and define its attributes and relationships.
E. Define a new entity type in a YAML file and use the 'atlas-cli' command-line tool to import it into
Atlas.
Answer: B
Explanation:
Custom entity types in Atlas are defined using JSON files that specify the attributes, relationships, and
other properties of the entity type. These JSON files are then imported into Atlas using the Atlas REST
API. Creating a Java class (A) is not necessary for defining custom entity types. Creating a table in the
Atlas metadata repository (C) is incorrect, as Atlas manages its metadata internally. While the Atlas IJI
(D) can be used to browse and manage existing entities, it does not support creating custom entity
types. YAML isn't native to Atlas.
Question: 274
You are configuring Ranger to manage access to Impala data’. A user reports that they are able to see
data in a table via Impala-shell but are unable to see the same data when accessing the table through a
Bl tool connected via JDBC. What is the MOST likely reason for this discrepancy?
A. The user's Kerberos principal is different when connecting via Impala-shell versus JDBC.
B. The JDBC connection is using a different Ranger plugin configuration than Impala-shell.
C. The Impala service is configured to bypass Ranger authorization for JDBC connections.
D. The Ranger policy evaluation is case-sensitive, and the table name is being passed differently via
JDBC.
E. The BI tool is caching the Impala metadata, and the Ranger policies have not been updated in the
cache.
Answer: A
Explanation:
When connecting via Impala-shell, the user's Kerberos principal is typically used for authentication and
authorization. However, when connecting via JDBC, the BI tool might be using a different Kerberos
principal or a different authentication mechanism altogether. If the Ranger policies are configured based
on the Kerberos principal, a mismatch can cause access discrepancies. The JDBC connection typically
uses the same Ranger plugin as other Impala access methods. Impala is not configured to bypass Ranger,
and policy evaluation is not case-sensitive.
Question: 275
You need to implement a data retention policy for audit logs generated by Apache Rangen The policy
requires that audit logs be retained for one year and then automatically archived. What steps should
you take to configure this data retention policy?
A. Configure the ‘ranger.audit.max.age’ property in Ranger's configuration file to 365 days.
B. Implement a custom script that periodically moves audit logs older than one year to an archive
directory.
C. Configure a log rotation policy in the operating system to rotate audit logs on a daily basis and retain
logs for one year.
D. Use the Ranger API to query audit logs older than one year and delete them from the audit log
repository.
E. Configure Ranger's archiver feature to automatically move audit logs to a separate storage location
after one year.
Answer: B
Explanation:
Currently, Cloudera CDP does not have built in features for automatic archival. The best solution
involves implementing custom scripts. The ‘ranger.audit.max.age' (A) only affects how long the logs are
stored in the current system, not archival. Configuring a log rotation policy (C) is a good practice for
managing log file size, but doesn't provide a mechanism for archiving. The Ranger API could be used, but
the logs would be deleted, not archived.
Question: 276
You are using Ranger to manage access to Hive tables, and you want to create a policy that grants access
to a table only during specific business hours (e.g., 9 AM to 5 PM on weekdays). How can you achieve
this?
A. Use the 'Time-based Access Control' feature in Ranger to define a schedule for the policy to be active.
B. Use the 'Custom Condition' feature in Ranger to write a Java plugin that checks the current time and
grants access accordingly.
C. Schedule a script to enable and disable the Ranger policy based on the business hours.
D. Time-based access control is not directly supported in Ranger. You must implement it using external
authentication mechanisms or custom code within the application accessing the data.
E. Use the 'REST API' feature in Ranger to enable and disable the Ranger policy based on the business
hours.
Answer: B
Explanation:
Ranger does not have a built-in 'Time-based Access Control' feature. The most straightforward way to
implement time-based access control is to use the 'Custom Condition' feature in Ranger and write a Java
plugin that checks the current time and grants access accordingly. The Ranger REST API can be used to
enable and disable policies, but would require a separate scheduling mechanism. Building it directly into
a Java plugin allows for a more complete solution.
Question: 277
Your Cloudera CDP on-premises cluster is experiencing frequent HDFS block replication delays,
impacting write performance. Which of the following actions would MOST directly address this issue
from a capacity management perspective?
A. Increase the value of ‘dfs.namenode.handler.count’ in the NameNode's ‘hdfs-site.xmr configuration
file.
B. Reduce the 'dfs.replication’ factor for newly created HDFS files.
C. Add more DataNodes to the cluster to increase overall storage capacity and network bandwidth.
D. Increase the ‘dfs.datanode.max.locked.memory' on all DataNodes.
E. Enable HDFS Federation with another existing cluster to balance the load.
Answer: C
Explanation:
Adding more DataNodes directly addresses the capacity issue by increasing storage capacity and,
critically, network bandwidth for replication. While A might help the NameNode, it doesn't solve the
underlying resource shortage. B reduces data durability. D focuses on memory usage, not bandwidth or
storage. E is a complex solution and not the most direct.
Question: 278
You notice that your YARN ResourceManager is consistently operating near its memory limit, causing
application failures. You want to increase the ResourceManager's heap size. Which Cloudera Manager
property should you modify to achieve this goal?
A. resourcemanager_java_heapsize
B. yarn.resourcemanager.resource-tracker.expiry-interval
C. yarn_resourcemanager_heapsize
D. rm.java.opts
E. resourcemanager_java_opts
Answer: E
Explanation:
is the correct Cloudera Manager property to modify Java options for the ResourceManager, including
heap size. You would set this property to a value like '-Xmx8g’ to allocate 8GB of heap memory.
"yarn.resourcemanager.resource-tracker.expiry- interval' is related to node manager heartbeats. The
other options are incorrect.
Question: 279
A Cloudera CDP cluster is experiencing a high volume of small file writes to HDFS. This is causing
excessive NameNode load. What are TWO effective strategies to mitigate this issue from a capacity
management perspective, assuming you cannot change the application writing the files?
A. Increase the 'dfs.blocksize’ configuration parameter in 'hdfs-site.xmr.
B. Implement HDFS Federation with a dedicated namespace for small files.
C. Enable HDFS caching for the directories containing the small files.
D. Implement a small file archival strategy using tools like Apache NiFi or Flume to periodically
consolidate small files into larger ones.
E. Reduce the number of DataNodes in the cluster to consolidate storage.
Answer: B,D
Explanation:
HDFS Federation allows you to isolate the small files to a dedicated namespace, reducing the load on the
primary NameNode. Archiving small files into larger ones reduces the overall number of files, thereby
reducing NameNode metadata load. Increasing the block size doesn't directly address the number of
files. Caching can improve read performance, but doesn't reduce NameNode load from writes. Reducing
the number of DataNodes decreases overall capacity.
Question: 280
You are tasked with monitoring the disk utilization of your HDFS DataNodes. Which command provides
the MOST comprehensive information about disk space usage across all DataNodes in the cluster?
A. hdfs dfsadmin -report
B. hdfs fsck /
C. df -h
D. yarn application -list
E. hdfs storagepolicies -getStoragePolicy I
Answer: A
Explanation:
'hdfs dfsadmin -report' provides a detailed report on the overall HDFS status, including disk space
utilization for each DataNode. 'hdfs fsck P checks file system integrity. 'df -h' shows local disk usage on
the server it's executed on. 'yarn application -list lists YARN applications. ‘hdfs storagepolicies -
getStoragePolicy P lists the storage policy applied to the root directory.
Question: 281
A critical MapReduce job is consistently failing due to insufficient container memory. You want to
increase the maximum memory allocation for containers in a specific YARN queue named 'production'.
Which configuration property and location should you modify?
A. Modify yarn.scheduler.maximum-allocation-mb' in ‘yarn-site.xmr for the entire cluster.
B. Modify ‘yarn.scheduler.maximum-allocation-mb' specifically for the 'production' queue in the
Capacity Scheduler configuration within Cloudera Manager.
C. Modify mapreduce.map.memory.mb' in ‘mapred-site.xmr for the entire cluster.
D. Modify ‘yarn.nodemanager.resource.memory-mb' on each NodeManager.
E. Modify yarn.scheduler.maximum-allocation-mb' in the mapred-site.xml file for the entire cluster.
Answer: B
Explanation:
The correct approach is to modify 'yarn.scheduler.maximum-allocation-mb' specifically for the
'production' queue within the Capacity Scheduler configuration in Cloudera Manager. This allows you to
increase the maximum memory allocation for containers within that specific queue without impacting
other queues. Modifying it in ‘yarn-site.xmr affects the entire cluster. controls memory requested by
map tasks, not the overall maximum. "yarn.nodemanager.resource.memory-mb' defines the total
memory available on each NodeManager.
Question: 282
You need to determine the total available memory resources in your YARN cluster. Which of the
following commands or Uls would provide you with the MOST accurate and up-to-date information?
A. Summing the ‘yarn.nodemanager.resource.memory-mb' values from the ‘yarn-site.xml' files on each
NodeManager.
B. Using the Cloudera Manager YARN service IJI to view the 'Total Available Memory' metric.
C. Running 'free -m' on the ResourceManager node.
D. Parsing the output of 'yarn node -list' and summing the 'memory' attribute for each node.
E. Consulting the hardware specifications of the servers in the cluster.
Answer: B
Explanation:
The Cloudera Manager YARN service IJI provides the most accurate and up-to-date view of the total
available memory in the YARN cluster. It dynamically reflects the actual resources available, taking into
account node health and configuration changes. Summing values from ‘yarn-site.xml' is static and
doesn't reflect the current state. Tree -m' shows memory on a single server. ‘yarn node -list' can be
used, but parsing is cumbersome. Hardware specs don't reflect what's allocated to YARN.
Question: 283
Which statement about the impact of HDFS block size ('dfs.blocksize') on cluster capacity and
performance is MOST accurate ?
A. Increasing the block size always improves write performance for all types of workloads.
B. Decreasing the block size reduces the memory footprint of the NameNode but can lead to increased
disk I/O for large file reads.
C. Increasing the block size reduces the number of metadata objects the NameNode must manage,
potentially improving NameNode performance, but may lead to wasted space if many files are
significantly smaller than the block size.
D. The block size has no impact on the overall storage capacity of the cluster.
E. Decreasing the block size always increases write throughput and reduces latency.
Answer: C
Explanation:
Increasing the block size reduces the number of metadata objects (blocks) that the NameNode needs to
track, which can improve its performance. However, it can also lead to wasted space if many files are
smaller than the block size, as each file will occupy at least one full block. The other options are
inaccurate or oversimplified.
Question: 284
You have a Cloudera CDP cluster where several applications are competing for resources in the default
YARN queue. You want to implement fair sharing of resources among these applications. What is the
MOST appropriate YARN scheduler to use and how would you configure it within Cloudera Manager?
A. Use the FIFO scheduler and configure the ‘yarn.scheduler.fifo.max-capacity’ property.
B. Use the Capacity scheduler and configure the ‘yarn.scheduler.capacity.maximum-am-resource-
percent property for the default queue.
C. Use the Fair scheduler and create allocation policies in the 'fair-scheduler.xmr file, managed through
the Fair Scheduler Advanced Configuration Snippet (Safety Valve) for 'Fair Scheduler Configuration
Directory' in Cloudera Manager.
D. Use the Capacity scheduler and enable preemption.
E. Use the FIFO scheduler and configure ‘yarn.scheduler.fair.preemption’ to True.
Answer: C
Explanation:
The Fair scheduler is designed for fair resource sharing among applications. It's configured using
allocation policies defined in ‘fair- scheduler.xmr. Cloudera Manager provides a Safety Valve for
managing this file. The FIFO scheduler is not designed for fair sharing. The Capacity scheduler can be
configured for capacity guarantees, but Fair scheduler is better for fair sharing. While the Capacity
scheduler with preemption can improve faimess, it's not as direct as using the Fair scheduler with
appropriate policies.
Question: 285
Your company is implementing a data lake using Cloudera CDP on-premises. You anticipate a significant
increase in data volume over the next year. What is the MOST proactive approach to capacity planning
for HDFS?
A. Wait until HDFS utilization reaches 80% before adding more DataNodes.
B. Regularly monitor HDFS utilization metrics (e.g., remaining capacity, disk 1/0) and extrapolate future
growth based on historical trends, factoring in planned data ingestion and retention policies. Establish
thresholds for triggering capacity expansion.
C. Periodically run 'hdfs fsck r to identify under-replicated blocks and allocate more space accordingly.
D. Assume a linear growth rate of 10% per month and add DataNodes accordingly, regardless of actual
utilization.
E. Add as many datanodes as the budget allows.
Answer: B
Explanation:
Proactive capacity planning involves monitoring key metrics, analyzing historical trends, and considering
future data ingestion plans and retention policies. Establishing thresholds allows for timely capacity
expansion before performance is significantly impacted. Waiting until 80% utilization is reactive. 'hdfs
fsck' checks file system integrity, not capacity. Assuming a fixed growth rate is often inaccurate. Adding
nodes without any planning is not cost-effective.
Question: 286
Which of the following are valid reasons for using HDFS Storage Policies (e.g., COLD, WARM, HOT) in a
Cloudera CDP on-premises environment?
A. To automatically tier data based on access frequency, moving less frequently accessed data to lower-
cost storage.
B. To improve the reliability of data stored in HDFS by replicating data across multiple data centers.
C. To optimize disk I/O by placing frequently accessed data on faster storage devices (e.g., SSDs).
D. To reduce the overall storage capacity required for HDFS by compressing data more aggressively.
E. To isolate data based on security classifications, storing sensitive data on encrypted storage volumes.
Answer: A,C
Explanation:
HDFS Storage Policies are primarily used for data tiering based on access frequency (A) and for
optimizing disk I/O by placing data on appropriate storage devices (C). They don't inherently improve
reliability through cross-DC replication, reduce storage capacity through compression, or provide
security isolation (although those capabilities can be combined with storage policies).
Question: 287
You are investigating slow query performance in Impala, and suspect that insufficient memory is a
contributing factor. You want to increase the memory available to Impala queries. Which configuration
setting should you adjust, and where can you find it in Cloudera Manager?
A. Increase ‘mem_limit under the Impala Daemon Advanced Configuration Snippet (Safety Valve) for
'impala-site.xml'.
B. Increase ‘yarn.scheduler.maximum-allocation-mb' in the YARN service configuration in Cloudera
Manager.
C. Increase in the Impala Catalog Server Advanced Configuration Snippet (Safety Valve) for 'impala-
site.xml'.
D. Increase under the Impala Daemon Service-Wide Advanced Configuration Snippet (Safety Valve).
E. Increase the in Cloudera Manager.
Answer: A
Explanation:
The ‘mem_limit setting in the 'impala-site.xmr file controls the maximum memory that an Impala query
can use. You can adjust this through the Impala Daemon Advanced Configuration Snippet (Safety Valve).
‘yarn.scheduler.maximum-allocation-mb’ affects YARN container memory. controls the Catalog Server
memory. configures default query options but is not the primary setting for memory limits.
impala_server_memory_percent is a setting that governs the proportion of server memory to be used
but not the maximum limit.
Question: 288
Your organization uses Ranger for fine-grained access control in Cloudera CDP A new business unit
needs access to specific tables in Hive. However, you are running out of available memory on Ranger
Admin server. What are the best ways to address Ranger Admin memory capacity and performance?
Choose two options.
A. Increase Ranger Admin heap size via Ranger Admin Java Heap Size property in Cloudera Manager.
B. Reduce the number of Ranger policies defined within Ranger IJI.
C. Reduce the value of ‘ranger.audit.db.batch.size' in ranger-admin-site.xml
D. Scale out the number of Ranger Admin servers and load balance the traffic.
E. Tune the Ranger policy retrieval queries and add appropriate indexes on Ranger database.
Answer: A,E
Explanation:
Increasing the Ranger Admin heap size is the first line of defense in increasing the Ranger Admin
memory capacity. Ranger policies are stored in database and tuning the database and indexes can
improve performance. Number of Ranger policies should be reduced only if there are overlapping
policies. Ranger audit database batch size change is a configuration level tuning and is only relevant if
audit logging is causing issue. Currently Ranger Admin does not support scale out and so load balancing
is not an option.
Question: 289
You are tasked with decommissioning a host node in a CDP on-premises cluster. After stopping all
services and roles on the node, you attempt to remove it from the cluster using Cloudera Manager.
However, the removal process fails with the error 'Insufficient replicas found for HDFS blocks on the
node'. What is the MOST appropriate action to take?
A. Immediately force the node removal from Cloudera Manager, ignoring the replication warning.
B. Run an HDFS balancer to redistribute blocks from the node to other nodes in the cluster before
attempting the removal again.
C. Increase the replication factor for all HDFS files to ensure sufficient replicas before removing the
node.
D. Manually move all HDFS blocks from the node to other nodes using the 'hdfs dfs -mv’ command.
E. Wait for the HDFS replication to complete automatically; no further action is required.
Answer: B
Explanation:
Running the HDFS balancer is the most appropriate action. It intelligently redistributes the data from the
decommissioning node to other nodes in the cluster, ensuring data availability and preventing data loss.
Forcing the removal ignores the data replication issue and could lead to data loss. Increasing replication
factor increases storage utilization and doesn't solve the immediate problem. Manually moving blocks is
inefficient and prone to errors. Waiting for replication to complete might not happen if the node is
already considered dead by the cluster.
Question: 290
You are observing high CPU utilization on a NameNode in your CDP cluster. Which of the following
actions can help alleviate the CPU load on the NameNode and improve cluster performance? (Select
TWO)
A. Increase the number of DataNodes in the cluster.
B. Enable HDFS caching for frequently accessed metadata.
C. Reduce the number of files and directories in the HDFS namespace.
D. Increase the block size of HDFS files.
E. Increase the heap size allocated to the NameNode.
Answer: B,C
Explanation:
Enabling HDFS caching stores frequently accessed metadata in memory, reducing disk I/O and CPU load.
Reducing the number of files and directories lessens the metadata the NameNode needs to manage,
directly reducing CPU utilization. Increasing DataNodes does not directly alleviate NameNode CPU load.
Increasing block size can improve I/O performance but doesn't directly affect CPU load significantly.
Increasing heap size can help if the NameNode is running out of memory, but if CPU is the bottleneck, it
won't be as effective as caching or reducing the namespace size.
Question: 291
A DataNode in your CDP cluster is consistently reporting disk failures. You want to gracefully
decommission the node and prevent future data loss. What is the correct sequence of steps to achieve
this?
A. 1. Stop the DataNode. 2. Remove the DataNode from Cloudera Manager.
B. 1. In Cloudera Manager, initiate the decommission process for the DataNode. 2. Monitor the
decommissioning progress until completion. 3. Stop the DataNode. 4. Remove the DataNode from
Cloudera Manager.
C. 1. Physically remove the disk drives from the DataNode. 2. Stop the DataNode. 3. Remove the
DataNode from Cloudera Manager.
D. 1. Stop the DataNode. 2. In Cloudera Manager, initiate the decommission process for the DataNode.
3. Monitor the decommissioning progress until completion. 4. Remove the DataNode from Cloudera
Manager.
E. 1. Remove the DataNode from Cloudera Manager. 2. Stop the DataNode.
Answer: B
Explanation:
The correct sequence is to first initiate the decommissioning process from Cloudera Manager, which
allows HDFS to replicate the data to other nodes. Then, monitor the progress until the decommissioning
is complete. After that, the DataNode can be stopped and removed from Cloudera Manager.
Question: 292
You want to add a new DataNode to your existing CDP on-premises cluster. Which of the following
configuration changes are REQUIRED on the new DataNode before starting the DataNode process?
A. Install the Cloudera Manager Agent and configure it to point to the Cloudera Manager server.
B. Create the HDFS data directories as specified in the DataNode's configuration.
C. Configure the DataNode's hostname in the /etc/hosts file of all other nodes in the cluster.
D. Install the Hadoop client libraries.
E. Install the Hive client libraries.
Answer: A, B
Explanation:
The Cloudera Manager Agent is essential for managing the DataNode. Creating the HDFS data
directories ensures the DataNode has storage space. While hostname resolution is important, updating
/etc/hosts on every node isn't scalable; DNS is preferred. Hadoop client libraries are typically installed as
part of the CDP installation. Hive libraries are not directly required for a DataNode.
Question: 293
A new application writes a large number of small files to HDFS, causing significant performance
degradation. What configuration change can you implement to mitigate this issue and improve write
performance? Assume no code changes are possible in the application itself. (Single choice)
A. Increase the number of DataNodes in the cluster.
B. Enable HDFS Federation.
C. Enable HDFS caching.
D. Use the HDFS Archive (HAR) tool to combine the small files into larger archive files.
E. Decrease the block size of HDFS.
Answer: D
Explanation:
The HDFS Archive (HAR) tool is designed specifically for addressing the small files problem. It combines
small files into larger archive files, reducing the load on the NameNode and improving performance.
Increasing DataNodes won't directly address the small file issue. HDFS Federation is for scaling the
namespace, not directly for small files. HDFS caching helps with read performance. Decreasing block size
would exacerbate the problem. Since application change is not permitted, HAR is the only solution.
Question: 294
You are troubleshooting a DataNode that is failing to start. The DataNode logs contain the following
error: 'java.io.IOException: Incompatible clusterlD. What is the MOST likely cause of this error?
A. The DataNode's clock is not synchronized with the NameNode's clock.
B. The DataNode is using an incorrect core-site.xml configuration file.
C. The DataNode's disk space is full.
D. The DataNode is attempting to join a different HDFS cluster.
E. The DataNode's Java version is incompatible with the NameNode's Java version.
Answer: D
Explanation:
The ‘ Incompatible clusterlD error indicates that the DataNode is configured to connect to a different
HDFS cluster than the NameNode. The ‘clusterlD’ is a unique identifier for an HDFS cluster. The
DataNode's 'dfs.cluster.id' property in ‘hdfs-site.xmr must match the NameNode's 'dfs.cluster.id'.
Question: 295
You have configured automatic decommissioning of unhealthy DataNodes in your CDP cluster. However,
you notice that DataNodes are being decommissioned too aggressively, even when the issues are
transient (e.g., network blips). Which configuration parameter can you adjust to make the
decommissioning process less sensitive?
A. 'dfs.namenode.decommission.intervar
B. 'dfs.namenode.decommission.nodes.per.interval'
C. 'dfs.health.check.intervar
D. 'dfs.datanode.failed.volumes.tolerated'
E. 'dfs.namenode.decommission.replicas.required'
Answer: D
Explanation:
'dfs.datanode.failed.volumes.tolerated' determines how many disk failures a DataNode can tolerate
before being considered unhealthy. Increasing this value will make the decommissioning process less
sensitive, as DataNodes will be allowed to have more failed disks before being automatically
decommissioned. The other parameters control the decommissioning interval, the number of nodes
decommissioned per interval, the health check interval and replica availability respectively but don't
directly address the sensitivity to transient issues.
Question: 296
You need to perform a rolling restart of your DataNodes to apply a security patch. While doing so, you
want to ensure minimal impact on the running applications. Which of the following approaches is the
MOST suitable and ensures high availability during the process?
A. Stop all DataNodes simultaneously, apply the patch, and then start all DataNodes.
B. Decommission each DataNode one at a time, apply the patch, and then recommission them.
C. Use Cloudera Manager's rolling restart feature, ensuring the minimum number of replicas are
available throughout the process.
D. Restart each DataNode without any prior action, assuming HDFS will handle the temporary
unavailability.
E. Manually move all the data from each DataNode to other nodes, patch the node, and then move the
data back.
Answer: C
Explanation:
Using Cloudera Manager's rolling restart feature is the best approach. It handles the restart process
gracefully, ensuring that the minimum number of replicas are available throughout the process,
minimizing impact on running applications and maintaining high availability. Stopping all DataNodes
simultaneously will cause a cluster outage. Decommissioning and recommissioning each node is time-
consuming and unnecessary. Restarting without any prior action can lead to data unavailability.
Manually moving data is extremely time consuming and prone to errors.
Question: 297
You are monitoring your CDP cluster and notice that one of your DataNodes has a consistently high disk
I/O wait time. The disk is almost full. What are the MOST effective actions you can take to address this
issue and improve performance without adding new hardware immediately? (Select TWO)
A. Increase the HDFS block size.
B. Run the HDFS balancer to redistribute data to other DataNodes with more free space.
C. Delete unused or unnecessary data from the DataNode's disks.
D. Reduce the replication factor of the HDFS files.
E. Increase the number of NameNodes in the cluster.
Answer: B,C
Explanation:
Running the HDFS balancer redistributes data to other DataNodes, alleviating the I/O bottleneck on the
full disk. Deleting unused data frees up space, directly reducing disk I/O wait time. Increasing block size
might help in some cases, but it's not a guaranteed solution and might not be feasible without data
migration. Reducing the replication factor reduces data redundancy. Increasing the number of
NameNodes does not address the DataNode disk I/O issue.
Question: 298
A MapReduce job is failing consistently with ‘OutOfMemoryError: Java heap space' on the TaskTrackers
(running on DataNodes). You suspect the problem is related to the amount of memory available to the
TaskTrackers. Which configuration parameters should you adjust to increase the memory allocated to
MapReduce tasks without affecting other services running on the DataNode? (Select TWO)
A. 'mapreduce.map.java.opts
B. ‘mapreduce.reduce.java.opts’
C. yarn.nodemanager.resource.memory-mb'
D. ‘mapreduce.task.io.sort.mb'
E. mapreduce.map.memory.mb’ and ‘mapreduce.reduce.memory.mb'
Answer: A,B
Explanation:
‘mapreduce.map.java.opts’ and ‘mapreduce.reduce.java.opts’ are the correct parameters to adjust.
These allow you to increase heap size for map and reduce tasks respectively without affecting other
services. Although ‘yarn.nodemanager.resource.memory-mb’ controls the total memory available to
YARN containers on the NodeManager, it impacts all containers, not just MapReduce tasks.
‘mapreduce.task.io.sort.mb' controls the amount of memory used for sorting intermediate data, not the
overall heap size. ‘mapreduce.map.memory.mb’ and ‘mapreduce.reduce.memory.mb' define the
memory requested by the containers, not the java heap options. Java heap options are controlled via
the java opts parameter.
Question: 299
You are setting up a new CDP cluster and want to optimize DataNode disk usage. You have a mix of fast
SSDs and slower HDDs. How can you configure HDFS to leverage the faster SSDs for frequently accessed
data and the slower HDDs for less frequently accessed data? Show Configuration as code snippet. (Single
Answer)
A. Configure HDFS caching using the 'hdfs dfsadmin -storagePolicies’ command, creating a policy that
moves data to SSDs based on access frequency.
B.
C. Manually move frequently accessed files to the SSDs using the ‘hdfs dfs -mv’ command.
D. Create separate HDFS clusters for SSDs and HDDs.
E. Configure Hadoop Distributed Cache to use SSDs.
Answer: B
Explanation:
The correct answer is to configure 'dfs.datanode.data.diff to specify different storage types (SSD and
HDD) and enable storage policies. The configuration allows HDFS to automatically manage data
placement based on storage policies. Option A is close, but it doesn't show configuration and it's more
of a management command. Manually moving files is not scalable or efficient. Creating separate clusters
is unnecessary complexity. Hadoop Distributed Cache is for distributing application-specific files, not
general HDFS data.
Question: 300
You've configured HDFS to utilize erasure coding to reduce storage overhead. However, you observe
that read performance for erasure- coded files is significantly slower than for replicated files. What are
the PRIMARY factors contributing to this performance difference and what steps can you take to
mitigate this issue? (Select TWO)
A. Erasure coding requires reconstruction of data blocks during reads, which involves computation and
network I/O.
B. Erasure coded files are stored in a compressed format, requiring decompression during reads.
C. Increase the replication factor for erasure-coded files.
D. Enable short-circuit reads on the DataNodes.
E. Ensure sufficient network bandwidth and CPU resources on the DataNodes to handle the
reconstruction process.
Answer: A,E
Explanation:
Erasure coding inherently introduces overhead due to the need to reconstruct data blocks from parity
blocks during reads. This reconstruction involves computation (decoding) and network 1/0 to retrieve
the necessary blocks. Ensuring sufficient network bandwidth and CPU resources on the DataNodes is
crucial for efficiently handling the reconstruction process. While compression can be used with erasure
coding, it's not the primary factor contributing to the performance difference. Increasing the replication
factor defeats the purpose of using erasure coding. Short-circuit reads can improve read performance in
general but doesn't specifically address the overhead of erasure coding reconstruction.
Question: 301
You are managing a CDP cluster with a large number of DataNodes. You need to automate the process
of updating the 'hdfs-site.xml' configuration file on all DataNodes whenever there's a change. What is
the BEST approach to achieve this in a scalable and maintainable manner within the Cloudera Manager
environment?
A. Manually edit the 'hdfs-site.xmr file on each DataNode.
B. Use a script to copy the 'hdfs-site.xmr file to all DataNodes.
C. Use Cloudera Manager's configuration management features to modify the *hdfs-site.xmr template
and redeploy the DataNode configurations.
D. Create a symbolic link from each DataNode's 'hdfs-site.xmr file to a central location.
E. Use Ansible to push the configuration changes to all DataNodes.
Answer: C
Explanation:
Cloudera Manager's configuration management features are designed for managing configuration files
across the cluster. Modifying the 'hdfs-site.xmr template in Cloudera Manager and redeploying the
configurations ensures that the changes are propagated to all DataNodes in a controlled and auditable
manner. It also handles the restart of the DataNodes if necessary. Manual editing and scripting are not
scalable or maintainable. Symbolic links can lead to inconsistencies. While Ansible can be used, Cloudera
Manager provides a more integrated and manageable solution within the CDP environment.
Question: 302
You are performing a rolling upgrade of a CDP Private Cloud Base cluster from 7.1.7 to 7.2.17. During the
upgrade process, the NameNode service fails to start after being restarted. You check the NameNode
logs and find the following error: 'java.lang.lllegalArgumentException: Required attribute
'dfs.namenode.rpc-address' not defined'. What is the most likely cause of this issue?
A. The HDFS site configuration file was not properly propagated to the NameNode host.
B. The CM agent on the NameNode host is not running or is unable to communicate with the CM server.
C. The upgrade process did not properly migrate the NameNode metadata to the new version.
D. A custom property defined in a safety valve for the NameNode overwrites the required configuration.
E. The Kerberos principal for the NameNode is not properly configured.
Answer: D
Explanation:
The error message ‘java.lang.lllegalArgumentException: Required attribute 'dfs.namenode.rpc-address'
not defined' indicates that a required configuration property is missing or has been unintentionally
overridden. A likely cause is a custom property defined in a safety valve that is inadvertently overwriting
the necessary configuration. While other options might seem plausible, the error points directly to a
configuration problem.
Question: 303
After upgrading your CDP cluster's Data Hub runtime, you notice that Spark applications are failing with
'java.lang.NoSuchMethodError: org.apache.spark.sql.SparkSession.builder()'. What are the potential
reasons for this error? (Select TWO)
A. The Spark client libraries used by the applications are incompatible with the new Spark version in the
Data Hub runtime.
B. The YARN client libraries used by the applications are incompatible with the new YARN version.
C. The Spark application is using an older version of the Java Development Kit (JDK).
D. The Spark configuration properties in ‘spark-defaults.conf are incorrect.
E. The PATH environment variable on the client machine does not point to the correct Spark installation.
Answer: A,E
Explanation:
The ‘java.lang.NoSuchMethodError’ suggests that the Spark client libraries used by the application are
referencing a method ('builder()') that is either missing or has a different signature in the Spark version
available on the cluster (A). Inconsistent client environment (E). The PATH envirnoment variable on the
client machine does not point to the correct Spark installation can lead to client using old libraries.
Question: 304
You've initiated an upgrade rollback procedure for the Hive service in your CDP cluster. During the
rollback, you encounter the following error in the Hive metastore logs:
'javax.jdo.JDODataStoreException: Error executing SQL query'. Which actions should you consider to
troubleshoot this issue?
A. Verify that the Hive metastore database is accessible and running.
B. Check for any database schema changes or inconsistencies that might have occurred during the
upgrade.
C. Review the Hive metastore logs for more detailed error messages and stack traces.
D. Restart the Hive metastore service to clear any transient errors.
E. All of the above.
Answer: E
Explanation:
The error ‘javax.jdo.JDODataStoreException: Error executing SQL query' during a Hive rollback indicates
a problem with the Hive metastore database. All the options provided are valid troubleshooting steps.
Verifying database accessibility (A), checking for schema inconsistencies (B), reviewing logs for more
details (C), and restarting the service (D) are all important to diagnose and resolve the issue.
Question: 305
You are using Cloudera Manager to upgrade your CDP cluster You are in the pre-upgrade tasks phase.
Which of the following actions is MOST critical to perform before proceeding with the actual upgrade?
A. Back up all configuration files for each service in the cluster.
B. Run the Cloudera Manager pre-upgrade health checks and resolve all critical alerts.
C. Stop all client applications that are accessing the cluster.
D. Disable all scheduled jobs and workflows.
E. Clear all temporary directories on the cluster nodes.
Answer: B
Explanation:
Running the Cloudera Manager pre-upgrade health checks (B) is the most critical step. These checks
identify potential issues that could cause the upgrade to fail or lead to data loss. Resolving the critical
alerts ensures that the cluster is in a stable state before the upgrade begins. While other actions are
important, resolving the pre-upgrade health checks is the most crucial.
Question: 306
During an upgrade of your Cloudera DataFlow (CDF) cluster using Cloudera Manager, the NiFi services
fail to start after the upgrade is complete. The NiFi logs show a persistent error related to
'javax.net.ssl.SSLHandshakeException'. What is the most probable cause of this issue, and how would
you address it?
A. The Keystore/Truststore configuration in NiFi is corrupted or misconfigured. Regenerate the
Keystore/Truststore using the NiFi toolkit, and update the NiFi properties.
B. The NiFi Zookeeper quorum is unstable. Restart the Zookeeper services and then try starting the NiFi
services.
C. The NiFi database is corrupted. Restore the NiFi database from a backup taken before the upgrade.
D. The Java version used by NiFi is incompatible with the upgraded cluster. Update the JAVA_HOME
environment variable to point to a compatible JDK.
E. There are conflicting dependencies in NiFi's lib directory. Clean the lib directory and redeploy the
required dependencies.
Answer: A
Explanation:
An 'SSLHandshakeException’ strongly suggests a problem with the SSL/TLS configuration, specifically
related to Keystores and Truststores. During an upgrade, these configurations can become invalidated
due to changes in security protocols or certificate updates. Regenerating and updating these stores (A)
ensures that NiFi can establish secure communication within the cluster.
Question: 307
You are upgrading a CDP cluster and using the Express Upgrade option. After the upgrade completes,
you notice that the Hue service is not functioning correctly and shows '502 Bad Gateway' errors. What is
the MOST likely reason for this and how would you quickly verify the cause?
A. Hue is not compatible with the new version of the cluster. Revert the upgrade and upgrade Hue
separately after the core cluster upgrade.
B. The Hue service did not restart successfully. Check the Hue service logs for errors and restart the
service via Cloudera Manager.
C. The Hue load balancer configuration is incorrect. Verify the load balancer settings and update them if
necessary.
D. The Hue metastore connection is broken. Check the Hue configuration and ensure that the metastore
is accessible.
E. The Hue service depends on other services that did not upgrade correctly. Check all dependent
service logs for errors.
Answer: B
Explanation:
A '502 Bad Gateway' error commonly indicates that the upstream server (in this case, the Hue service) is
not responding. Checking the Hue service logs is the quickest way to determine if the service failed to
start or is experiencing errors preventing it from serving requests. Restarting the service might resolve
temporary issues.
Question: 308
You are preparing to upgrade a CDP 7.1 .x cluster to CDP 7.2.x. Your current cluster heavily utilizes
custom YARN queue configurations.
What considerations must you make regarding these custom YARN queue configurations during the
upgrade process?
A. Custom YARN queue configurations are automatically migrated during the upgrade. No special action
is required.
B. You must manually back up the ‘yarn-site.xmr configuration file before the upgrade, and restore it
after the upgrade is complete.
C. You should review the YARN queue configurations after the upgrade to ensure they are compatible
with the new YARN version and make any necessary adjustments using the YARN Resource Manager U
or API.
D. You need to migrate the existing YARN queue configuration to use the Capacity Scheduler's
configuration API before upgrading.
E. All custom YARN queues will be deleted during the upgrade. You must recreate them after the
upgrade.
Answer: C
Explanation:
While upgrades aim to preserve configurations, compatibility issues can arise with custom YARN queues.
It's important to review the configurations after the upgrade (C) to ensure they are still valid and
functioning correctly with the newer YARN version. This often involves checking queue capacities, access
controls, and other parameters.
Question: 309
You have a CDP Private Cloud Base cluster running on-premise. You are planning to upgrade to a newer
version of CDP. Your organization mandates a zero-downtime upgrade. Which upgrade strategy can you
use?
A. Express Upgrade
B. Rolling Upgrade
C. Offline Upgrade
D. Blue/Green Deployment Upgrade
E. Upgrade via Lift and Shift
Answer: D
Explanation:
A Blue/Green Deployment Upgrade (D) is the best strategy for zero-downtime upgrades. It involves
creating a new, upgraded cluster (Green) alongside the existing cluster (Blue). Once the new cluster is
validated, traffic is switched over to it, providing a seamless transition without downtime. Rolling
upgrades minimize downtime but may not fully eliminate it, and other methods involve significant
downtime.
Question: 310
You are upgrading your CDP cluster's Data Hub runtime. During the upgrade, the upgrade process fails
with a message indicating a conflict in the versions of Hadoop client libraries. What steps can you take to
resolve this?
A. Manually remove the older Hadoop client libraries from the nodes and restart the upgrade.
B. Update the 'HADOOP_CLASSPATH' environment variable to point to the new Hadoop client libraries
and retry the upgrade.
C. Use Cloudera Manager to identify and resolve the version conflicts automatically. Cloudera Manager
has built-in conflict resolution mechanisms.
D. Rollback to the previous version and perform a full cluster restart before attempting the upgrade
again.
E. Ignore the warning and continue the upgrade. Hadoop client library conflicts are usually benign.
Answer: C
Explanation:
Cloudera Manager is designed to manage and resolve dependency conflicts during upgrades. Option C
leverages Cloudera Manager's built-in capabilities to handle these types of issues automatically, which is
the best and safest approach. The other approaches are not recommended.
Question: 311
You've upgraded your CDP cluster to a new version. After the upgrade, you observe that the
performance of Hive queries has significantly degraded. What are the potential causes of this
performance degradation? (Select TWO)
A. The Hive metastore schema was not upgraded properly.
B. The Hive configuration properties are not optimized for the new version of Hive.
C. The underlying storage format of the Hive tables is incompatible with the new Hive version.
D. The YARN queue configurations are not properly configured for the Hive workload.
E. The hardware resources allocated to the Hive service are insufficient.
Answer: B,D
Explanation:
Upgrading to a new version of Hive can introduce changes in configuration requirements and YARN
resource allocation. The default settings may not be optimized for your specific workload in the new
version (B), and changes to YARN configurations can affect the resources available to Hive queries (D).
The hardware may also become a bottleneck, but that wouldn't be directly related to the upgrade itself.
Question: 312
After upgrading your CDP cluster using a rolling upgrade, some of your Spark applications are failing
intermittently with a 'Container killed by YARN for exceeding memory limits' error. These applications
were running successfully before the upgrade. What are the possible root causes? (Select TWO)
A. The default memory allocation for Spark containers has decreased after the upgrade.
B. YARN node labels are not correctly configured after the upgrade, causing applications to be scheduled
on nodes with insufficient resources.
C. The Spark application code contains a memory leak that was not previously triggered but is now
exposed by changes in the upgraded environment.
D. The YARN queue used by the Spark applications has a lower memory limit than before the upgrade.
E. The Spark dynamic allocation feature is disabled.
Answer: C,D
Explanation:
The error indicates that containers are exceeding memory limits set by YARN. While various factors can
contribute, a memory leak in the Spark application code (C) would cause it to consume more memory
over time, leading to the container being killed. Changes in YARN queue configurations could also
reduce the memory available to applications (D), causing them to exceed the new limits. Incorrect node
label configuration is less likely to suddenly cause 00M errors after an upgrade.
Question: 313
During an upgrade of your CDP cluster, you encounter an issue where the HBase upgrade process gets
stuck in the 'Pre-Upgrade Tasks' state within Cloudera Manager. The logs show errors related to HBase
region server connectivity. What steps should you take to diagnose and resolve this?
A. Force the upgrade to proceed to the next step. Connectivity issues are often transient and resolve
themselves.
B. Check the network connectivity between the Cloudera Manager server and the HBase region servers.
Verify DNS resolution and firewall rules.
C. Restart all HBase region servers simultaneously to resolve any potential temporary connectivity
issues.
D. Increase the timeout values for HBase region server connectivity checks in the Cloudera Manager
configuration.
E. Check if the HBase root directory in HDFS is accessible and has the correct permissions.
Answer: B
Explanation:
The 'Pre-Upgrade Tasks' state getting stuck with connectivity errors strongly suggests a network issue.
Option B addresses the most fundamental issue: ensuring the Cloudera Manager server can
communicate with the HBase region servers. Verifying network connectivity, DNS resolution, and
firewall rules are essential steps to resolve this problem before proceeding with the upgrade.
Question: 314
You are tasked with patching a Cloudera Data Platform (CDP) on-premises cluster using Cloudera
Manager. Before proceeding, what is the most crucial pre-patching task you should perform to ensure
minimal downtime and data integrity?
A. Immediately halt all cluster services to ensure no data corruption during the patch application.
B. Download the latest Cloudera Manager agent packages from the Cloudera website.
C. Perform a full backup of the Cloudera Manager Server and all service metadata databases.
D. Ensure all users are logged out of the cluster to prevent any concurrent modifications.
E. Disable automatic failover for all highly available services.
Answer: C
Explanation:
Backing up the Cloudera Manager Server and service metadata databases is crucial. This allows for a
complete rollback in case the patch application fails or introduces unexpected issues. While other
options might seem relevant in specific scenarios, a full backup provides the most comprehensive
protection against data loss and cluster instability.
Question: 315
While applying a patch via Cloudera Manager, you encounter the following error message during the
'Deploy Client Configuration' step for the Hive service: 'java.net.ConnectException: Connection refused'.
Which of the following is the most likely cause of this error?
A. The Hive Metastore service is unavailable or not responding.
B. There is a firewall blocking communication between Cloudera Manager Server and the Hive client
host.
C. The Hive service user does not have sufficient permissions to write to the Hive configuration
directory.
D. The Hadoop Distributed File System (HDFS) is in safe mode.
E. The Cloudera Manager Agent on the Hive client host is not running.
Answer: B
Explanation:
A ‘java.net.ConnectException: Connection refused' typically indicates a network connectivity issue. In
this context, it suggests that Cloudera Manager Server is unable to connect to the Hive client host, likely
due to a firewall rule preventing the communication.
Question: 316
After successfully applying a patch to your CDP on-premises cluster, you observe that a custom IJDF
(User Defined Function) in Hive is no longer working. Which of the following steps are necessary to
troubleshoot and resolve this issue? (Select all that apply)
A. Verify that the IJDF JAR file is still present in the HDFS location specified by ‘hive.aux.jars.patW.
B. Restart the Hive Metastore service to reload the IJDF definitions.
C. Recompile the IJDF code against the updated Hive libraries after the patch.
D. Update the Hive catalog to reflect the new patch version using 'ALTER DATABASE SET DBPROPERTIES
('cdp.version'=");’
E. Rollback the entire cluster to the previous version.
Answer: A,B,C
Explanation:
A, B and C are the most appropriate actions. Patches can sometimes affect dependencies or classpaths,
so these steps help ensure the UDF is correctly loaded and compatible. The UDF JAR needs to exist in
the specified location (A), the metastore needs to be refreshed (B), and recompilation might be required
if the underlying Hive libraries have changed (C). D isn’t directly related to UDF functionality, and E is an
extreme measure if targeted troubleshooting can fix the problem.
Question: 317
Which of the following Cloudera Manager roles is required to initiate and execute a patch upgrade
process within a CDP on-premises cluster?
A. Read-Only User
B. Operator
C. Limited Operator
D. Cluster Administrator
E. Navigator Administrator
Answer: D
Explanation:
The Cluster Administrator role has the necessary privileges to manage all aspects of the cluster,
including initiating and executing patch upgrades, which involve significant system-level changes.
Question: 318
You are applying a patch to your Cloudera Data Engineering (CDE) service using Cloudera Manager. After
the patch is applied, some Spark applications are failing with 'java.lang.ClassNotFoundException'. What
could be the possible reason and how can you address this? (Select all that apply)
A. The patch introduced a breaking change in the Spark API, requiring you to update the Spark
application code.
B. The Spark driver and executors are not picking up the updated classpath, requiring a restart of the
CDE service.
C. The dependent JAR files for the Spark application are missing from the executor nodes, requiring you
to redeploy the application with the necessary dependencies.
D. The application requires a different version of Java than that on the CDE nodes after the patch.
E. Disable the CDE service, manually download the missing class files, and copy them to the appropriate
lib directory.
Answer: A,B,C
Explanation:
A, B, and C are valid reasons. A patch might introduce incompatible API changes (A), the updated
classpath might not be propagating correctly (B), or dependencies might be missing on the executor
nodes (C). D is also possible but less frequent than A, B, and C. E is not a recommended approach as it
involves manual changes outside the scope of Cloudera Manager and can cause instability.
Question: 319
During a patch application process, Cloudera Manager displays a warning: 'Rolling restart is required for
Impala to fully apply the patch'.
What does this warning signify, and what steps should you take to ensure a smooth restart?
A. A rolling restart means stopping and starting all Impala daemons simultaneously, which can cause
significant downtime. You should schedule a full cluster outage.
B. A rolling restart involves restarting Impala daemons one at a time (or in small groups) to minimize
service interruption. Ensure sufficient resources are available and monitor query performance during
the restart.
C. The warning is benign and can be ignored, as Impala will automatically update itself without requiring
any manual intervention.
D. A rolling restart means that only the Impala catalog server needs to be restarted.
E. You need to manually update the Impala metadata in the Hive metastore before restarting Impala.
Answer: B
Explanation:
A rolling restart is designed to minimize service interruption. Restarting daemons one at a time (or in
small groups) allows the service to remain available while the update is applied. Monitoring query
performance is critical to ensure the restart doesn't negatively impact users.
Question: 320
You are preparing to patch a Cloudera Manager deployment that is running on RHEL 7. However, you
noticed that the Cloudera Manager agent on one of the host nodes is showing as 'unresponsive' in the
Cloudera Manager UI. What are the most likely causes and the steps to resolve this before initiating the
patching process? (Select all that apply)
A. The Cloudera Manager agent process is not running on the host. Use 'systemctl status cloudera-scm-
agent' and ‘systemctl start cloudera-scm-agent’ to verify and restart the agent.
B. There is a network connectivity issue preventing the agent from communicating with the Cloudera
Manager server. Verify firewall rules and DNS resolution.
C. The host's clock is significantly out of sync with the Cloudera Manager server. Use ‘ntpdate’ or
‘chrony’ to synchronize the clocks.
D. The host's Cloudera Manager agent is using an outdated version of the agent package. Manually
upgrade the agent package using ‘yum update cloudera-manager-agent.
E. The Cloudera Manager server has run out of disk space. Clean up the Cloudera Manager server's logs
and database files.
Answer: A,B,C
Explanation:
A, B, and C are the most likely causes. An unresponsive agent can be due to the agent not running (A),
network connectivity issues (B), or clock synchronization problems (C). An outdated agent could be a
cause, but it's less likely if the cluster has been running smoothly before. A full Cloudera Manager server
could impact agent responsiveness indirectly, but the primary focus should be on the agent's health and
connectivity.
Question: 321
You have scheduled a patch upgrade for your CDP on-premises cluster using Cloudera Manager. During
the patch process, you want to monitor the progress of the upgrade, including the status of each service
and host. Which of the following methods or tools can provide the most detailed and real-time insights
into the patch upgrade process?
A. The Cloudera Manager Activity Monitor, which provides a visual representation of the upgrade
process and the status of each step.
B. The ‘cm_api' command-line interface, which allows you to query the Cloudera Manager API for
detailed information about the upgrade process.
C. The system logs on each host in the cluster, which contain detailed information about the upgrade
process for each service.
D. The Hadoop YARN ResourceManager U, which provides information about the resources being used
by the upgrade process.
E. Nagios or other external monitoring tool using metrics from the Cloudera Manager Metrics API.
Answer: A
Explanation:
The Cloudera Manager Activity Monitor provides the most comprehensive and real-time view of the
patch upgrade process. It shows the status of each step, any errors encountered, and the overall
progress of the upgrade. The can also provide detailed information, but the Activity Monitor is generally
more user-friendly for monitoring progress. Other options provide some insight, but not as directly
related to the CM managed process of the patching activity itself.
Question: 322
After patching your CDP cluster, you notice that the Hue service is unable to connect to the Hive
Metastore. After reviewing the Hue logs, you see a message indicating a Kerberos authentication failure.
What steps are most appropriate to resolve this Kerberos-related connectivity issue? (Select all that
apply)
A. Ensure the Hue service principal (e.g., 'hue/hostname@REALM') is properly created and the keytab
file is correctly configured and accessible by the Hue service account.
B. Verify that the 'hive.metastore.kerberos.principal' property in the Hive Metastore configuration is
correctly set to the Hive Metastore service principal.
C. Renew the Kerberos ticket for the Hue service account using 'kinit -kt Ipath/to/hue.keytab
hue/hostname@REALM' and then restart the Hue service.
D. Disable Kerberos authentication for the Hive Metastore and rely on simple authentication.
E. Restart the entire Kerberos Key Distribution Center (KDC).
Answer: A,B,C
Explanation:
A, B, and C are the correct approaches to troubleshoot Kerberos authentication issues. Ensure the Hue
service principal and keytab are correctly configured (A), verify the Hive Metastore principal is set
correctly (B), and renew the Kerberos ticket for the Hue service (C). Disabling Kerberos (D) is not a
recommended solution in a secure environment. Restarting the entire KDC (E) is overkill and should only
be considered as a last resort if other troubleshooting steps fail.
Question: 323
You are using Cloudera Manager to upgrade a cluster from CDP 7.1.7 to CDP 7.2.16. The upgrade
process fails during the ‘Pre- Upgrade Taskse step with the following error message: 'Incompatible
schema version detected for table.. Expected version: , Actual version: What are the most effective
strategies to address this issue and continue the upgrade process?
A. Manually upgrade the schema version for the affected table using the SALTER TABLE . COMPACT
'major" command in Hive.
B. Revert the cluster back to CDP 7.1.7, drop the affected table, and then recreate it before attempting
the upgrade again.
C. Use the 'hive —service metastore -hiveconf "datanucleus.autoCreateSchema=true" -hiveconf
"datanucleus.fixedDatastore=false" –hiveconf "datanucleus.autoCreateTables=true" -updateSchema’
command to automatically update the metastore schema.
D. Run the 'Upgrade Hive Metastore Schema' command from Cloudera Manager for the Hive service,
specifically targeting the database containing the incompatible table.
E. Ignore the error message and proceed with the upgrade. The schema version will be automatically
updated during the post-upgrade tasks.
Answer: D
Explanation:
Running the 'Upgrade Hive Metastore Schema' command from Cloudera Manager is the recommended
approach. This command is designed to handle schema upgrades in a managed and controlled manner.
Option A might work but is a manual workaround and not the intended upgrade path. Option B is
destructive. Option C is closer, but the Cloudera Manager command is preferred and option E is not
advisable as that can lead to failures.
Question: 324
A security vulnerability has been identified in a specific version of Apache Kafka running within your CDP
cluster. Cloudera has released a patch to address this vulnerability. You have downloaded the patch and
are preparing to apply it. However, your Kafka cluster is configured with custom interceptors for data
auditing. What are the most important considerations and steps to take before applying the Kafka patch
to ensure the custom interceptors continue to function correctly?
A. Back up the Kafka broker configuration files (e.g., ‘server.properties') before applying the patch. After
the patch, manually re-apply any custom configurations related to the interceptors.
B. Test the custom interceptors in a non-production environment (e.g., a staging cluster) after applying
the patch to verify their functionality.
C. Recompile the custom interceptor code against the updated Kafka libraries included in the patch to
ensure compatibility.
D. Disable the custom interceptors temporarily during the patching process and re-enable them after
the patch is complete.
E. Download the newer version of interceptor jar files.
Answer: B,C
Explanation:
B and C are the most critical steps. Thorough testing in a non-production environment (B) is crucial to
identify any issues with the custom interceptors. Recompiling the interceptor code against the updated
Kafka libraries (C) is necessary to ensure compatibility with the new Kafka version. A is helpful, but not
required. D and E don't guarantee they continue to function as expected after patching.
Question: 325
You are planning to perform a rolling restart of the HDFS DataNodes after applying a patch. Before
initiating the rolling restart, you want to ensure that there is sufficient disk space available on each
DataNode to handle the data replication that occurs during the restart process. Which of the following
methods provides the most accurate and reliable way to assess the available disk space on each
DataNode before the restart?
A. Use the command on each DataNode host to check the available disk space. This provides a quick
overview of the disk usage.
B. Use the HDFS Disk Balancer tool to redistribute data evenly across the DataNodes and check the disk
utilization after the balancing process.
C. Use the ‘hdfs dfsadmin -report’ command to generate a report showing the disk space utilization for
each DataNode in the cluster.
D. Monitor the 'HDFS Disk Free' metric in Cloudera Manager for each DataNode role instance. This
provides a real-time view of the available disk space.
E. Consult the NameNode logs for information about free disk space on the DataNodes.
Answer: D
Explanation:
Cloudera Manager’s built-in monitoring provides the most accurate and reliable real-time view of the
free disk space on each HDFS DataNode. While options A and C offer insights into disk utilization,
Cloudera Manager's metrics provide continuous monitoring and historical data that's essential for
proactive management during operations such as rolling restarts.
Question: 326
You are administering an HDFS cluster with several DataNodes reporting high disk utilization. You need
to add more storage capacity.
Which of the following is the MOST efficient and non-disruptive way to accomplish this?
A. Add more disks to the existing DataNodes and rebalance the cluster.
B. Format and add new DataNodes to the cluster.
C. Decommission existing DataNodes, add disks, then recommission them.
D. Add a new service role instance of NameNode to balance storage.
E. Increase the replication factor of existing data.
Answer: A
Explanation:
Adding disks to existing DataNodes and then rebalancing the cluster is the most efficient because it
avoids downtime associated with formatting new nodes or decommissioning existing ones. Rebalancing
redistributes data evenly across all DataNodes, including the newly added storage.
Question: 327
You are troubleshooting an HDFS issue where certain files are consistently unavailable. You suspect a
problem with block placement. Which of the following HDFS commands would be MOST useful to
identify the location of the blocks for a specific file?
A. hdfs fsck /path/to/file
B. hdfs dfs -du -h /path/to/file
C. hdfs dfs -Is /path/to/file
D. hdfs getconf -confKey fs.defaultFS
E. hdfs balancer -threshold 5
Answer: A
Explanation:
The 'hdfs fsck’ command with the path to the file provides detailed information about the health and
location of all blocks associated with that file. It allows you to identify if blocks are missing, corrupted, or
under-replicated.
Question: 328
Which of the following parameters in the 'hdfs-site.xmr file directly impacts the replication factor of
newly created files in HDFS?
A. dfs.replication
B. dfs.blocksize
C. dfs.namenode.name.dir
D. dfs.datanode.data.dir
E. dfs.namenode.rpc-address
Answer: A
Explanation:
The 'dfs.replication’ parameter in ‘hdfs-site.xmr specifies the default replication factor for newly created
files in HDFS. This parameter determines how many copies of each block will be stored across the
cluster.
Question: 329
You observe frequent 'Safe Mode' entries and exits in your HDFS cluster logs. Which of the following
actions would be the MOST effective first step to diagnose the root cause?
A. Increase the heap size for the NameNode.
B. Check the DataNode logs for any errors or connectivity issues.
C. Run a full HDFS balance operation.
D. Format the NameNode.
E. Decommission and recommission all DataNodes.
Answer: B
Explanation:
Frequent Safe Mode entries typically indicate that the NameNode is not receiving enough block reports
from the DataNodes. Checking the DataNode logs for errors is the most logical first step to identify
connectivity problems, disk issues, or other factors preventing block reports.
Question: 330
Which of the following statements are true regarding HDFS Federation? (Select TWO)
A. It allows for horizontal scalability of the NameNode namespace.
B. It eliminates the need for DataNodes.
C. It isolates failures to specific namespaces.
D. It automatically balances data across all DataNodes.
E. It combines all metadata into a single NameNode for faster access.
Answer: A,C
Explanation:
HDFS Federation allows for horizontal scalability of the NameNode namespace by using multiple
independent NameNodes (A). It also isolates failures to specific namespaces because each NameNode
manages its own namespace independently (C).
Question: 331
You have configured HDFS access control lists (ACLs) for a directory '/data/sensitive’. A user 'alice' needs
read and execute permissions. How would you grant these permissions using the command line?
A. hdfs dfs -setfacl -m /data/sensitive
B. hdfs dfs -setfacl -m user:alice:r-x /data/sensitive
C. hdfs dfs -chown alice /data/sensitive
D. hdfs dfs -chmod 755 /data/sensitive
E. hdfs dfs -setfacl -rwx user:alice /data/sensitive
Answer: B
Explanation:
The correct command to grant read and execute permissions to user 'alice' is 'hdfs dfs -setfacl -m
user:alice:r-x Idata/sensitive’ . The -m' option modifies the ACL, specifies the user, ‘r-x’ defines the
permissions (read and execute), and '/data/sensitive’ is the target directory.
Question: 332
A MapReduce job is failing with 'OutOfMemoryError’. You suspect that the amount of data being
processed by the mappers is too large for the default configuration. Which HDFS setting could you
adjust to reduce the amount of data read by each mapper?
A. Increase 'dfs.blocksize’
B. Decrease 'dfs.replication
C. Decrease 'dfs.blocksize’
D. Increase 'dfs.namenode.handler.count'
E. Decrease 'dfs.datanode.max.locked.memory’
Answer: C
Explanation:
Decreasing 'dfs.blocksize’ will result in smaller blocks, and potentially more, smaller input splits for the
mappers to process. This reduces the amount of data each mapper handles at one time, which may
alleviate ‘OutOfMemoryError’ issues.
Question: 333
You want to configure HDFS to use a custom trash retention policy where files in the trash are
automatically deleted after 48 hours. How would you configure this?
A. Set 'fs.trash.intervar to 2 in ‘core-site.xmr.
B. Set 'fs.trash.max.age' to 172800 in 'core-site.xml'.
C. Set ‘fs.trash.classname’ to a custom class implementing the retention policy.
D. Set 'hadoop.security.impersonation.provider.class’ to a custom class implementing trash policy
E. Set 'fs.trash.intervar to 48 and 'fs.trash.purge.interval' to 1 in ‘core-site.xml'.
Answer: A
Explanation:
To set the trash retention policy to 48 hours, set 'fs.trash.interval' to 2 in ‘core-site.xmr. The unit of
fs.trash.interval is in minutes. 48 hours 60 minutes/hour = 2880 minutes. Then, setting
fs.trash.interval=2 will purge the trash files every 2 minutes, for files older than 'fs.trash.interval
checklntervar. 'fs.trash.interval checklnterval = 2 minutes 1440 checklnterval = 2880 minutes = 48 hours.
Question: 334
You are tasked with setting up encryption at rest for HDFS. Which component is responsible for
managing the encryption keys?
A. DataNode
B. NameNode
C. Key Management Server (KMS)
D. ResourceManager
E. Zookeeper
Answer: C
Explanation:
The Key Management Server (KMS) is responsible for managing the encryption keys used for HDFS
encryption at rest. The KMS provides an interface for generating, storing, and retrieving encryption keys.
Question: 335
A critical HDFS directory '/data/important' was accidentally deleted. You have snapshots enabled. What
steps are required to restore the directory to its previous state? Assume the snapshot name is
'pre_delete'.
A. hdfs dfs -cp /data/important/.snapshot/pre_delete/ Idata/important/
B. hdfs dfs -restoreSnapshot /data/important pre_delete
C. hdfs dfs -mv /data/important/.snapshot/pre_delete Idata/important
D. hdfs dfs -createSnapshot /data/important pre_delete
E. hdfs dfs -rmdir /data/important
Answer: A
Explanation:
To restore the directory, copy the contents of the snapshot to the original location using: ‘hdfs dfs –cp
/data/important/.snapshot/pre_delete/ /data/importantr. You need to first recreate the
/data/important directory using 'hdfs dfs –mkdir /data/important'.
Question: 336
You have a scenario where a user is complaining that his HDFS writes are extremely slow. After some
investigation, you determine that the disk I/O on one of the DataNodes is saturated. What is the BEST
approach to immediately alleviate this issue?
A. Decommission the slow DataNode, allowing the data to be replicated to other nodes.
B. Increase the replication factor of the files being written.
C. Rebalance the cluster.
D. Stop the NameNode and restart it.
E. Reduce the 'dfs.datanode.max.locked.memory’ on all datanodes
Answer: A
Explanation:
Decommissioning the slow DataNode is the most immediate solution. This forces the data to be
replicated to other DataNodes with available resources, relieving the pressure on the saturated node
and improving write performance for the user. A rebalance will also help eventually, but decommission
is faster as the data will migrate faster. A rebalance will try to make the data in the cluster balanced,
meaning it will attempt to put some data on the node again
Question: 337
Assume that you have an HDFS cluster where NameNode HA is enabled. You observe that the Active
NameNode is constantly switching to Standby and vice-vers
a. Which of the following could be the root causes for this frequent failover? (Select THREE)
A. ZooKeeper quorum issues.
B. Network connectivity problems between the Active and Standby NameNodes.
C. A bug in the operating system kernel on the NameNode servers.
D. Incorrect configuration of the Failover Controller.
E. High disk I/O on the DataNodes.
Answer: A,B,D
Explanation:
Frequent NameNode failover can be caused by issues with the ZooKeeper quorum (A), network
connectivity problems (B), and incorrect configuration of the Failover Controller (D). The DataNode 1/0
and OS are unlikely.
Question: 338
You are tasked with optimizing YARN resource allocation for a cluster running diverse workloads,
including batch processing and interactive queries. You notice that interactive queries are frequently
delayed due to resource contention. Which of the following YARN features would be MOST effective in
mitigating this issue?
A. Using only a single FIFO scheduler.
B. Implementing Capacity Scheduler with dedicated queues and resource guarantees for interactive
queries.
C. Disabling preemption in the Fair Scheduler.
D. Increasing the yarn.scheduler.minimum-allocation-mb to a very high value.
E. Relying solely on the default resource settings for all applications.
Answer: B
Explanation:
The Capacity Scheduler allows you to create dedicated queues with guaranteed resources. This ensures
that interactive queries always have access to the necessary resources, reducing delays. The FIFO
scheduler doesn't provide prioritization. Disabling preemption can exacerbate contention. Increasing
minimum allocation can waste resources. Relying on default settings rarely optimizes diverse workloads.
Question: 339
A YARN application is consistently failing with an 'OutOfMemoryError'. After inspecting the application
logs, you suspect that the memory allocated to the YARN container is insufficient. Which YARN
configuration parameter should you adjust to increase the maximum memory allocated to the
containers managed by a specific queue in the Capacity Scheduler?
A. yarn.scheduler.capacity..maximum-allocation-mb
B. yarn.nodemanager.resource.memory-mb
C. yarn.scheduler.maximum-allocation-mb
D. yarn.scheduler.capacity..maximum-am-resource-percent
E. yarn.scheduler.capacity..capacity
Answer: D
Explanation:
The ‘yarn.scheduler.capacity..maximum-allocation-mb' parameter controls the maximum memory that
can be allocated to containers within a specific queue. "yarn.nodemanager.resource.memory-mb'
defines the total memory available on a NodeManager. yarn.scheduler.maximum-allocation-mb&
represents the overall maximum for the entire scheduler, not per queue. ‘maximum-am-resource-
percent’ relates to Application Master resources. Queue capacity deals with proportional resource
allocation, not maximum memory.
Question: 340
You are using the Fair Scheduler with preemption enabled. A large application starts and consumes most
of the cluster resources. Later, a smaller, high-priority application is submitted. What will happen?
A. The high-priority application will be placed in a pending state indefinitely.
B. The large application will be immediately killed to free up resources for the high-priority application.
C. The Fair Scheduler will preempt containers from the large application to make resources available for
the high-priority application, after a certain delay.
D. The Fair Scheduler will evenly distribute remaining resources among all running applications,
regardless of priority.
E. The high-priority application will be assigned to a different cluster if one is available.
Answer: C
Explanation:
With preemption enabled, the Fair Scheduler will preempt (i.e., reclaim) containers from lower-priority
applications to satisfy the resource demands of higher-priority applications. This happens after a
configurable delay to avoid excessive preemption. The large application will not be immediately killed,
but rather have containers taken away gradually. The Scheduler will allocate resources based on priority,
not distribute them evenly.
Question: 341
You need to configure YARN node labels for a specific set of nodes in your cluster. You want to ensure
that applications requesting a particular label only run on those nodes. Which of the following steps are
necessary to achieve this? (Select TWO)
A. Enable node labels globally in YARN configuration ('yarn.node-labels.enabled=true').
B. Configure the application to explicitly request the desired node label using ‘yarn.application.node-
label-expression’ .
C. Disable all other queues in the Capacity Scheduler.
D. Assign the node labels to the desired nodes using the YARN CLI or REST API.
E. Remove the ‘yarn-site.xml' file from all NodeManagers.
Answer: A,D
Explanation:
To use node labels, you must first enable them globally ('yarn.node-labels.enabled=true'). Then, you
must assign the labels to specific nodes using the YARN CLI or REST API. The application must then
request the label. Disabling queues or removing ‘yarn-site.xmr will break YARN functionality.
Question: 342
A MapReduce job is consistently slow, and you suspect that data locality is a contributing factor. Which
YARN parameter can you tune to improve data locality by influencing how YARN allocates containers to
nodes?
A. yarn.nodemanager.resource.cpu-vcores
B. yarn.nodemanager.resource.memory-mb
C. yarn.resourcemanager.scheduler.locality.delay
D. yarn.nodemanager.aux-services
E. yarn.nodemanager.disk-health-checker.max-disk-utilization-per-disk-percentage
Answer: C
Explanation:
'yarn.resourcemanager.scheduler.locality.delay’ controls how long the Resource Manager will wait for a
node-local or rack-local container assignment before relaxing the locality constraint. Tuning this
parameter can improve data locality by giving YARN more time to find a node with the data. The other
options affect CPU cores, memory, auxiliary services, and disk utilization, respectively, but not data
locality directly.
Question: 343
Which of the following commands is used to submit a YARN application with specific resource
requirements using the command line?
A. yarn application -jar myapp.jar -memory 4G -vcores 2
B. yarn jar myapp.jar —resource memory=4096,vcores=2
C. hadoop jar myapp.jar —resource memory=4096,vcores=2
D. hadoop jar myapp.jar -D mapreduce.map.memory.mb=4096 -D mapreduce.map.cpu.vcores=2
E. yarn application -submit -jar myapp.jar -am memory:4096,vcores:2
Answer: C
Explanation:
The 'hadoop jar’ command is used to submit applications to YARN. The '—resource' option allows
specifying resource requirements like memory and vcores. The other options are incorrect because they
use incorrect syntax or command names. Option D configures settings that are specific to MapReduce
applications only, not general YARN applications.
Question: 344
You have enabled the Fair Scheduler and configured a placement policy to automatically place
applications into queues based on their user and group. However, some applications are not being
placed into the correct queues. What is the MOST likely cause?
A. The application is specifying a queue name explicitly, overriding the placement policy.
B. The Fair Scheduler is not compatible with user and group-based placement policies.
C. The Capacity Scheduler is also enabled and interfering with the Fair Scheduler.
D. The ‘yarn-site.xml' file is corrupted.
E. The placement policy is configured to use the FIFO scheduler.
Answer: A
Explanation:
If an application explicitly specifies a queue name (e.g., using ‘-D mapreduce.job.queuename’ or similar),
it will override the Fair Scheduler's placement policy. You need to ensure that applications are not
explicitly specifying queue names if you want the placement policy to take effect. The Fair Scheduler
does support user and group-based placement. Having both schedulers active will cause problems, but
the manual queue specification is more common. A corrupted ‘yarn-site.xmr file would likely cause more
widespread issues.
Question: 345
You are monitoring your YARN cluster and notice that some nodes are consistently underutilized, while
others are heavily loaded. You suspect that node labels might not be configured correctly. How can you
verify the assigned node labels and resource usage on each node?
A. Using the 'hdfs dfsadmin -report' command.
B. Using the YARN Resource Manager web U and navigating to the Nodes section.
C. Inspecting the Vvar/log/hadoop-yarn/nodemanager/yarn-nodemanager- .log’ files on each node.
D. Using the "yarn node -list' command and analyzing the output.
E. By logging into each NodeManager and running 'free -m' to check memory utilization.
Answer: B
Explanation:
The YARN Resource Manager web U provides a comprehensive view of the cluster, including node
labels, resource usage, and other relevant information. The 'hdfs dfsadmin -report' command is for
HDFS information. While the NodeManager logs might contain some information, they are not the most
efficient way to get a cluster-wide view. ‘yarn node -list' is valid, but the web U provides more detailed
information in a more accessible format. ‘free -m’ only shows memory utilization on a single node, not
node labels or a cluster view.
Question: 346
A YARN application requires a specific version of a library that is not available on all nodes in the cluster.
How can you ensure that this library is available to the application's containers running on any node?
A. Install the library globally on all nodes in the cluster.
B. Package the library along with the application's JAR file and configure YARN to distribute it to the
containers.
C. Create a symbolic link to the library on the HDFS filesystem.
D. Configure YARN to download the library from a remote repository at runtime.
E. Use the ‘Idconfig’ command on all nodes to update the shared library cache.
Answer: B
Explanation:
The best practice is to package the required libraries along with the application's JAR file. YARN can then
distribute these files to the containers' working directories. This ensures that the application has access
to the specific version of the library it needs, without requiring global installation or relying on symbolic
links. Global installation can lead to version conflicts. Downloading from a remote repository at runtime
can be unreliable. Mdconfig’ updates the system's shared library cache, but it doesn't guarantee
availability within YARN containers.
Question: 347
You have a critical YARN application that needs to be restarted automatically if it fails. Which YARN
feature should you configure to achieve this?
A. YARN Federation.
B. ApplicationMaster Restart.
C. Container Preemption.
D. Resource Localization.
E. NodeManager Heartbeat.
Answer: B
Explanation:
ApplicationMaster Restart is specifically designed to automatically restart the ApplicationMaster (the
main process coordinating the application) if it fails. This provides fault tolerance and ensures that
critical applications can recover from failures. YARN Federation is for scaling across multiple clusters.
Container preemption is for reclaiming resources. Resource localization is about making resources
available to containers. NodeManager heartbeat is about monitoring node health.
Question: 348
You're encountering 'Container killed by YARN for exceeding memory limits.' errors. You've already
increased 'yarn.scheduler.maximum- allocation-mb’ and 'yarn.nodemanager.resource.memory-mb'.
However, the problem persists. Which of the following is the MOST likely reason for this continued
failure? (Select TWO)
A. The application is not requesting enough memory for its containers.
B. The application is exceeding its requested memory limits, and YARN is correctly killing the container.
C. The YARN queue's smaximum-allocation-mb' is set lower than the application's request.
D. There is insufficient disk space on the NodeManager's local directories.
E. The NodeManager's 'yarn-site.xml' is out of sync with the ResourceManager's configuration.
Answer: B,C
Explanation:
If containers are being killed for exceeding memory limits even after increasing global memory settings,
it's most likely because the application is exceeding its requested memory. YARN enforces these limits.
Also, the queue's ‘maximum-allocation-mb' could be lower than the application's request, effectively
capping the memory available to containers in that queue. Insufficient disk space might cause other
errors, but memory limits trigger the container kill message. NodeManager configuration being out of
sync would also cause problems, but its less likely. Option A is invalid since its already exceeding the
memory limit.
Question: 349
You want to use YARN Resource Profiles to simplify resource requests for your applications. How can
you define and manage these profiles within your YARN cluster?
A. By editing the ‘core-site.xmr file and adding custom properties.
B. By using the YARN CLI and creating a properties file with the profile definitions.
C. Resource profiles are not supported in Cloudera CDP On-premise.
D. By using the YARN REST API to create, update, and manage the profiles.
E. By defining the profiles directly within the application code.
Answer: D
Explanation:
YARN Resource Profiles are typically managed using the YARN REST API. This allows you to define and
manage different profiles with specific resource configurations (memory, CPU, etc.). The application can
then refer to these profiles when submitting jobs, simplifying resource requests. Editing ‘core-site.xmr is
not the correct approach. While some YARN configurations are done through XML files, resource
profiles are dynamic and managed through the REST API. Profiles cannot be created using the YARN CLI
or defined within the application code.



